{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYmZWfVa3czi"
   },
   "source": [
    "<h1 align=\"center\"> SANTANDER CHALLENGE PROJECT </h1>\n",
    "<br>\n",
    "<center>University of Rome “Tor Vergata” - Master Big Data in Business 2019</center>\n",
    "<br>\n",
    "<div style='text-align: right'>27/06/2019<br> Federico Francone</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "colab_type": "code",
    "id": "DNTSpiub3ge4",
    "outputId": "ccfe3a6c-517a-4377-b5d1-30ef198567da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vh0cEzRq3czl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn  import preprocessing, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Lasso\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, make_scorer, roc_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pAD1NtO3czs"
   },
   "outputs": [],
   "source": [
    "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd:goldenrod', 'xkcd:cadet blue', \n",
    "          'xkcd:scarlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NcRnjDWB3czx",
    "outputId": "d7182fde-defd-4668-e240-fa889b084df4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset.csv\", delimiter=';')\n",
    "data.set_index('ID', inplace=True)\n",
    "# last column is target, store in array t\n",
    "t = data.as_matrix(['TARGET'])\n",
    "\n",
    "\"\"\"# count number of positive and negative items\n",
    "c, counts = np.unique(t, return_counts=True)\n",
    "negative = counts[0]\n",
    "positive = counts[1]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "5O6w1hD23cz5",
    "outputId": "395209bf-89b8-4b9a-d983-98f9a1293a0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>imp_op_var40_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21638</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99968.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119266</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99946.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108190</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.35</td>\n",
       "      <td>103.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99921.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105047</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99909.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51076</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99863.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "ID                                                                 \n",
       "21638      2     40                 0.0                     0.00   \n",
       "119266     2     46                 0.0                     0.00   \n",
       "108190     2     23                 0.0                   103.35   \n",
       "105047     2     23                 0.0                     0.00   \n",
       "51076      2     28                 0.0                     0.00   \n",
       "\n",
       "        imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  \\\n",
       "ID                                                         \n",
       "21638                      0.00                      0.0   \n",
       "119266                     0.00                      0.0   \n",
       "108190                   103.35                      0.0   \n",
       "105047                     0.00                      0.0   \n",
       "51076                      0.00                      0.0   \n",
       "\n",
       "        imp_op_var40_comer_ult3  imp_op_var40_efect_ult1  \\\n",
       "ID                                                         \n",
       "21638                       0.0                      0.0   \n",
       "119266                      0.0                      0.0   \n",
       "108190                      0.0                      0.0   \n",
       "105047                      0.0                      0.0   \n",
       "51076                       0.0                      0.0   \n",
       "\n",
       "        imp_op_var40_efect_ult3  imp_op_var40_ult1   ...    \\\n",
       "ID                                                   ...     \n",
       "21638                       0.0                0.0   ...     \n",
       "119266                      0.0                0.0   ...     \n",
       "108190                      0.0                0.0   ...     \n",
       "105047                      0.0                0.0   ...     \n",
       "51076                       0.0                0.0   ...     \n",
       "\n",
       "        saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "ID                                                         \n",
       "21638                       0.0                      0.0   \n",
       "119266                      0.0                      0.0   \n",
       "108190                      0.0                      0.0   \n",
       "105047                      0.0                      0.0   \n",
       "51076                       0.0                      0.0   \n",
       "\n",
       "        saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "ID                                                       \n",
       "21638                      0.0                     0.0   \n",
       "119266                     0.0                     0.0   \n",
       "108190                     0.0                     0.0   \n",
       "105047                     0.0                     0.0   \n",
       "51076                      0.0                     0.0   \n",
       "\n",
       "        saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "ID                                                         \n",
       "21638                       0.0                      0.0   \n",
       "119266                      0.0                      0.0   \n",
       "108190                      0.0                      0.0   \n",
       "105047                      0.0                      0.0   \n",
       "51076                       0.0                      0.0   \n",
       "\n",
       "        saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
       "ID                                                                        \n",
       "21638                      0.0                     0.0  99968.40       1  \n",
       "119266                     0.0                     0.0  99946.59       0  \n",
       "108190                     0.0                     0.0  99921.81       0  \n",
       "105047                     0.0                     0.0  99909.60       1  \n",
       "51076                      0.0                     0.0  99863.61       0  \n",
       "\n",
       "[5 rows x 370 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "LcB5fP793cz-",
    "outputId": "e0581d9a-a216-43c0-c40d-22384e5f0f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6016 entries, 21638 to 151804\n",
      "Columns: 370 entries, var3 to TARGET\n",
      "dtypes: float64(111), int64(259)\n",
      "memory usage: 17.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7Qd2Tfj3c0C"
   },
   "source": [
    "Evaluate degree of sparsity of original matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "1XPWIAGP3c0F",
    "outputId": "ed1ecb97-babf-4118-9417-cdacd5162949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEGREE OF SPARSITY OF MATRIX:  91.5 %\n"
     ]
    }
   ],
   "source": [
    "from numpy import count_nonzero\n",
    "print(\"DEGREE OF SPARSITY OF MATRIX: \", (1.0 - count_nonzero(data) / data.size).round(3) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WH9qKof3c0z"
   },
   "source": [
    "## TRAIN TEST SPLIT AND SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "BRdzWITM3c01",
    "outputId": "7dabd7e1-a8fa-4c82-f205-b54cc0eb104c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>imp_op_var40_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var17_ult3</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033290</td>\n",
       "      <td>-0.960583</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>0.521308</td>\n",
       "      <td>0.710403</td>\n",
       "      <td>-0.048883</td>\n",
       "      <td>-0.059525</td>\n",
       "      <td>-0.040587</td>\n",
       "      <td>-0.041514</td>\n",
       "      <td>-0.053949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021393</td>\n",
       "      <td>-0.021033</td>\n",
       "      <td>-0.014889</td>\n",
       "      <td>-0.01874</td>\n",
       "      <td>-0.019391</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.014889</td>\n",
       "      <td>-0.021619</td>\n",
       "      <td>-0.021363</td>\n",
       "      <td>-0.156780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036476</td>\n",
       "      <td>1.747971</td>\n",
       "      <td>-0.063103</td>\n",
       "      <td>-0.223953</td>\n",
       "      <td>-0.241413</td>\n",
       "      <td>-0.048883</td>\n",
       "      <td>-0.059525</td>\n",
       "      <td>-0.040587</td>\n",
       "      <td>-0.041514</td>\n",
       "      <td>-0.053949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021393</td>\n",
       "      <td>-0.021033</td>\n",
       "      <td>-0.014889</td>\n",
       "      <td>-0.01874</td>\n",
       "      <td>-0.019391</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.014889</td>\n",
       "      <td>-0.021619</td>\n",
       "      <td>-0.021363</td>\n",
       "      <td>0.658623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var3     var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0  0.033290 -0.960583            0.173667                 0.521308   \n",
       "1  0.036476  1.747971           -0.063103                -0.223953   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                 0.710403                -0.048883                -0.059525   \n",
       "1                -0.241413                -0.048883                -0.059525   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  imp_op_var40_ult1  \\\n",
       "0                -0.040587                -0.041514          -0.053949   \n",
       "1                -0.040587                -0.041514          -0.053949   \n",
       "\n",
       "     ...     saldo_medio_var17_ult3  saldo_medio_var33_hace2  \\\n",
       "0    ...                  -0.021393                -0.021033   \n",
       "1    ...                  -0.021393                -0.021033   \n",
       "\n",
       "   saldo_medio_var33_hace3  saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "0                -0.014889                -0.01874               -0.019391   \n",
       "1                -0.014889                -0.01874               -0.019391   \n",
       "\n",
       "   saldo_medio_var44_hace2  saldo_medio_var44_hace3  saldo_medio_var44_ult1  \\\n",
       "0                -0.017793                -0.014889               -0.021619   \n",
       "1                -0.017793                -0.014889               -0.021619   \n",
       "\n",
       "   saldo_medio_var44_ult3     var38  \n",
       "0               -0.021363 -0.156780  \n",
       "1               -0.021363  0.658623  \n",
       "\n",
       "[2 rows x 266 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data.drop(labels=['TARGET'], axis=1), data['TARGET'],test_size=0.25,random_state=0)\n",
    "#standardizer = Pipeline([('standardizzo', StandardScaler().fit_transform())])\n",
    "\n",
    "#standardizzo train\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=  pd.DataFrame(scaler.fit_transform(X_train), columns= X_train.columns)\n",
    "\n",
    "#standardizzo test. Uso mean e standard deviation che ho visto nel training, quindi faccio solo .transform() \n",
    "#(non avrebbe senso fare .fit per il test, barerei!)\n",
    "\n",
    "scaled_test= scaler.transform(X_test)\n",
    "scaled_test= pd.DataFrame(scaled_test, columns= X_test.columns)\n",
    "\n",
    "X_train_scaled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See PART 1 for The ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fp_5H7Jn3c3o"
   },
   "source": [
    "# NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "yuomOd_h3c3o",
    "outputId": "695657a0-c33e-4509-8702-92b243156be6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import keras\n",
    "\n",
    "lr= 0.001\n",
    "adam= optimizers.Adam(lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "2u0fb3S_3c3p",
    "outputId": "505bcc1c-2c7f-4f89-a244-3c5e8cb4778c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3609, True)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create validation set \n",
    "train_setNN, val_setNN, y_trainNN, y_valNN = train_test_split(scaled_train, y_train, test_size=0.2,random_state=0)\n",
    "len(train_setNN), len(train_setNN)+ len(val_setNN)== len(scaled_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Creating a baseline model to practice using KerasClassifier wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ls62D2hO3c3q"
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=len(X_train_scaled.columns), activation='relu'))\n",
    "    model.add(Dense(10, input_dim=len(X_train_scaled.columns), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))   #anche softmax va bene, ma sigmoid dà molta più accuratezza. Penso perchè sigmoid è caso binario, softmax più classi\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  #se piu categorie metto categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "V2Rr59GE3c3r",
    "outputId": "967ccdaf-cb0a-45c7-f32d-2ed609b1f63e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0628 18:18:10.155143 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0628 18:18:10.162055 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0628 18:18:10.166748 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0628 18:18:10.209528 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0628 18:18:10.231691 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0628 18:18:10.238101 140059264472960 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0628 18:18:10.433779 140059264472960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3609 samples, validate on 903 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6078 - acc: 0.6922 - val_loss: 0.5692 - val_acc: 0.7065\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.5395 - acc: 0.7398 - val_loss: 0.5475 - val_acc: 0.7231\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.5216 - acc: 0.7553 - val_loss: 0.5366 - val_acc: 0.7309\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.5077 - acc: 0.7587 - val_loss: 0.5394 - val_acc: 0.7375\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.5040 - acc: 0.7600 - val_loss: 0.5367 - val_acc: 0.7320\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.4944 - acc: 0.7670 - val_loss: 0.5385 - val_acc: 0.7342\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.4869 - acc: 0.7697 - val_loss: 0.5342 - val_acc: 0.7276\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.4820 - acc: 0.7697 - val_loss: 0.5521 - val_acc: 0.7331\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.4791 - acc: 0.7750 - val_loss: 0.5412 - val_acc: 0.7364\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.4709 - acc: 0.7756 - val_loss: 0.5544 - val_acc: 0.7209\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=10, verbose=2)\n",
    "history= estimator.fit(train_setNN, y_trainNN, validation_data=(val_setNN, y_valNN))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OizrMBTY3c3v"
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "QyB3HPrJ3c3w",
    "outputId": "9aa02375-6439-4227-9a68-2d9a61086803"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAFNCAYAAABVKNEpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd81dX9x/HXySIECEkIJGzCJmwJ\nSwUVJKIWtVoFJ25bt6222trWWlut/mydrYM6i9uqaFGDCooyAwJC2DMBEjIIEMjO+f1xbiBERoAk\n35ub9/PxuI/c+x33fi4j5/v5nnM+x1hrERERERERkcYjyOsAREREREREpH4pERQREREREWlklAiK\niIiIiIg0MkoERUREREREGhklgiIiIiIiIo2MEkEREREREZFGRomgSB0xxnQxxlhjTEgNjr3aGPNt\nfcQlIiLSUKltFak9SgRFAGPMJmNMiTEmttr2730NThdvIjsolubGmAJjzKdexyIiInI0/ty2HktC\nKRKolAiKHLARuLTyhTGmPxDhXTg/chFQDIwzxsTX5weroRQRkePk722rSKOlRFDkgNeBq6q8ngy8\nVvUAY0xLY8xrxphsY8xmY8z9xpgg375gY8z/GWNyjDEbgHMPce6/jTHbjTFbjTEPGWOCjyG+ycBz\nwDLgimrv3dEY819fXLnGmGeq7LvBGLPSGLPHGJNmjDnJt90aY7pXOe4VY8xDvuenG2MyjDG/McZk\nAi8bY6KNMZ/4PmOn73mHKufHGGNeNsZs8+3/0Ld9uTFmQpXjQn1/RoOP4buLiEjD5O9t648YY5oY\nY57wtWfbfM+b+PbF+tq/fGNMnjFmdpVYf+OLYY8xZrUxZuyJxCFS15QIihwwD4g0xvTxNSKTgP9U\nO+ZpoCXQFTgN17hd49t3A/ATYDCQBPys2rmvAGVAd98xycD1NQnMGNMZOB2Y6ntcVWVfMPAJsBno\nArQH3vLtuxh4wHd8JHAekFuTzwTigRigM3Aj7vfFy77XnYBC4Jkqx7+Ou8vbF2gD/MO3/TUOTlzP\nAbZba7+vYRwiItJw+W3begS/A0YAg4CBwDDgft++XwEZQGsgDvgtYI0xvYBbgaHW2hbAWcCmE4xD\npE4pERQ5WOWdy3HASmBr5Y4qDdh91to91tpNwOPAlb5DLgGesNamW2vzgIernBuHS4DutNbutdbu\nwCVKk2oY15XAMmttGi7J61ulR20Y0A64x/feRdbaysnx1wOPWmsXWmedtXZzDT+zAvijtbbYWlto\nrc211r5vrd1nrd0D/AXXYGOMaQucDfzcWrvTWltqrf3a9z7/Ac4xxkRW+S6v1zAGERFp+Py1bT2c\ny4EHrbU7rLXZwJ+qxFMKtAU6+9q62dZaC5QDTYBEY0yotXaTtXb9CcYhUqc070fkYK8D3wAJVBu6\nAsQCobiet0qbcT1w4JKx9Gr7KnX2nbvdGFO5Laja8UdyFfAigLV2qzHma9zwmu+BjsBma23ZIc7r\nCBxvQ5RtrS2qfGGMicA1sOOBaN/mFr5GvCOQZ63dWf1NrLXbjDHfARcZYz7AJYx3HGdMIiLS8Phr\n23o47Q4RTzvf88dwI21SfJ/5grX2EWvtOmPMnb59fY0xnwO/tNZuO8FYROqMegRFqvD1lm3E3WH8\nb7XdObg7gZ2rbOvEgTub23EJUdV9ldJxhV5irbVRvkektbbv0WIyxpwM9ADuM8Zk+ubsDQcu8xVx\nSQc6HaagSzrQ7TBvvY+DJ+xXL0Bjq73+FdALGG6tjQRGV4bo+5wYY0zUYT7rVdzw0IuBudbarYc5\nTkREAow/tq1Hse0Q8WzzfZc91tpfWWu74qZb/LJyLqC19g1r7am+cy3wtxOMQ6ROKREU+bHrgDHW\n2r1VN1pry4F3gL8YY1r45u39kgNzHd4BbjfGdDDGRAP3Vjl3O5ACPG6MiTTGBBljuhljTqtBPJOB\nGUAibr7CIKAf0BTXu7YA11A+YoxpZowJN8ac4jt3CnC3MWaIcbr74gZYgksmg40x4/EN8zyCFrh5\ngfnGmBjgj9W+36fAP31FZUKNMaOrnPshcBKuJ7D63WAREQl8/ta2VmriazcrH0HAm8D9xpjWxi19\n8YfKeIwxP/G1pQbYhRsSWmGM6WWMGeMrKlOEay8rjvHPSKReKREUqcZau95am3qY3bcBe4ENwLfA\nG8BLvn0vAp8DS4HF/Piu51VAGJAG7ATew80zOCxjTDhufsTT1trMKo+NuKE2k32N6ATcRPktuEns\nE33f5V3cXL43gD24hCzG9/Z3+M7Lx82H+PBIsQBP4JLPHNzk/8+q7b8Sd1d3FbADuLNyh7W2EHgf\nNyyo+p+LiIgEOH9qW6spwCVtlY8xwENAKq5K9w++z33Id3wP4AvfeXOBf1prZ+LmBz6CayMzcUXT\n7juGOETqnXHzW0VE6pYx5g9AT2vtFUc9WERERETqlIrFiEid8w0lvY4DVddERERExEMaGioidcoY\ncwNuQv+n1tpvvI5HRERERDQ0VEREREREpNFRj6CIiIiIiEgjo0RQRERERESkkQmYYjGxsbG2S5cu\nXochIiL1YNGiRTnW2tZex9FQqI0UEWkcjqV9DJhEsEuXLqSmHm55GhERCSTGmM1ex9CQqI0UEWkc\njqV91NBQERERERGRRkaJoIiIiIiISCOjRFBERERERKSRCZg5godSWlpKRkYGRUVFXodS58LDw+nQ\noQOhoaFehyIiIiIi4onGcv1fG9f+AZ0IZmRk0KJFC7p06YIxxutw6oy1ltzcXDIyMkhISPA6HBER\nERERTzSG6//auvYP6KGhRUVFtGrVKmD/EVQyxtCqVauAv/MhIiIiInIkjeH6v7au/QM6EQQC+h9B\nVY3le4qIiIiIHEljuC6uje8Y8Img1/Lz8/nnP/95zOedc8455Ofn10FEIiIiIiJSFxrStb8SwTp2\nuH8MZWVlRzxv+vTpREVF1VVYIiIiIiJSyxrStX9AF4vxB/feey/r169n0KBBhIaGEh4eTnR0NKtW\nrWLNmjVccMEFpKenU1RUxB133MGNN94IQJcuXUhNTaWgoICzzz6bU089lTlz5tC+fXs++ugjmjZt\n6vE3ExE5WHmFZV9JGYWl5RSWlLPP93DP3fZ9JeWM6NqKhNhmXocrx2DdjgLmrM/hqpFdvA5FRMSv\nNaRrfyWCdeyRRx5h+fLlLFmyhFmzZnHuueeyfPny/RV+XnrpJWJiYigsLGTo0KFcdNFFtGrV6qD3\nWLt2LW+++SYvvvgil1xyCe+//z5XXHGFF19HRBoway0l5RWHTNL2VUneCkvK9u8vKq1ybGnZj5O7\nknL2+Y4pKauoURyPXzxQiWAD8+XKLB7+dBVjerehQ3SE1+GIiPithnTt32gSwT99vIK0bbtr9T0T\n20Xyxwl9j+mcYcOGHVTm9amnnuKDDz4AID09nbVr1/7oH0NCQgKDBg0CYMiQIWzatOnEAheRgLUp\nZy9vLthC6uad7C0+0AtXWFJOYWk55RX2mN4vPDSIiLAQmoYG0zQsmIiwYJqGBhPbPIyIsIgD28KC\niQgNOfB8/3Z3btXt0RFhdfTtpa4k943n4U9XMSMti2tO0TJFItIw+MP1vz9f+zeaRNBfNGt24C74\nrFmz+OKLL5g7dy4RERGcfvrphywD26RJk/3Pg4ODKSwsrJdYRaRhKC2v4Iu0LN5YsIXZa3MIDjIM\n6RxNx5iI/QlZeGhlclY9MauWvIWGHJTwBQUFfuU1ObqE2Gb0aNOclBVKBEVEjoU/X/s3mkTwWHvu\nakuLFi3Ys2fPIfft2rWL6OhoIiIiWLVqFfPmzavn6ESkIduaX8hbC7bw1sJ0svcU065lOL8a15NL\nhnYkLjLc6/AkwCT3jeO5rzewc28J0c3Uqysi/s+L6/+GdO3faBJBr7Rq1YpTTjmFfv360bRpU+Li\n4vbvGz9+PM899xx9+vShV69ejBgxwsNIRaQhKK+wfL1mB1PnbWHm6h1Y4Ixebbh8eCdO79WGYPXg\nSR1JTozn2Znr+WrVDi4a0sHrcERE/FJDuvY31h7bfBF/lZSUZFNTUw/atnLlSvr06eNRRPWvsX1f\nkcZkx+4i3klN580F6WzNLyS2eRMmDe3IpGEdG2XxDmPMImttktdxNBSHaiOPVUWFZeQjXzK4YzTP\nXTmkliITEaldjel6+FDf9VjaR/UIioj4qYoKy5z1ubyxYDMpK7Ioq7Cc2j2W353bh3GJcYQGaylY\nqT9BQYZxiXG8v2grRaXlhIcGex2SiIicACWCIiJ+Jm9vCe8tSueN+VvYlLuP6IhQrj01gUuHddKy\nC+Kp5MR4/jNvC9+uzeHMxLijnyAiIn5LiaCIiB+w1pK6eSdT521m+g+ZlJRXMLRLNHee2ZPx/eLV\n+yJ+YUTXVrRoEkJKWqYSQRGRBk6JoIiIh3YXlfLB4q1Mnb+ZNVkFtGgSwqXDOnLZ8M70im/hdXgi\nBwkLCeKM3m34cuUOyiusihOJiDRgSgRFRDywLCOfqfO2MG3pNgpLyxnYoSV/u6g/Ewa2IyJMv5rF\nfyX3jWPa0m0s3rKToV1ivA5HRESOk642RETqyd7iMqYt3cbU+ZtZvnU3TUODuWBwOy4b1pn+HVp6\nHZ7UI2PMeOBJIBiYYq195BDHXAI8AFhgqbX2Mt/2ycD9vsMesta+Wi9B+5zWszWhwYaUFZlKBEVE\nGjAlgn6mefPmFBQUeB2GiNSiVZm7mTpvCx98v5WC4jJ6xbXgz+f35fzB7YkMD/U6PKlnxphg4Flg\nHJABLDTGTLPWplU5pgdwH3CKtXanMaaNb3sM8EcgCZcgLvKdu7O+4m8RHsrJ3WJJScvit+f0wRgN\nDxUROV5eXvsrERQRqQNFpeVM/2E7U+dvYdHmnYSFBPGT/m25fEQnTuoUrYvnxm0YsM5auwHAGPMW\ncD6QVuWYG4BnKxM8a+0O3/azgBnW2jzfuTOA8cCb9RQ74IaH/u6D5azJKtBcVhGRBkqJYB279957\n6dixI7fccgsADzzwACEhIcycOZOdO3dSWlrKQw89xPnnn+9xpCJSG9ZnF/Dm/C28tziD/H2lJMQ2\n4/5z+3DRSR2IbhbmdXjiH9oD6VVeZwDDqx3TE8AY8x1u+OgD1trPDnNu+0N9iDHmRuBGgE6dOtVK\n4JXG9XGJYMqKTCWCIiJVNKRrfyWCdWzixInceeed+/8xvPPOO3z++efcfvvtREZGkpOTw4gRIzjv\nvPPUQyDSQJWUVTAjLYup8zczZ30uIUGGs/rGc/nwTozs1kr/t+V4hAA9gNOBDsA3xpj+x/IG1toX\ngBcAkpKSbG0G1yYynMGdokhJy+K2sT1q861FRBq0hnTt33gSwU/vhcwfavc94/vD2T+a33+QwYMH\ns2PHDrZt20Z2djbR0dHEx8dz11138c033xAUFMTWrVvJysoiPj6+duMTCQDFZeVk7CzE1uplbO2o\nHP75TmoGOQXFtI9qyj1n9eLipA60aRHudXjiv7YCHau87uDbVlUGMN9aWwpsNMaswSWGW3HJYdVz\nZ9VZpEcwLjGORz9bzbb8QtpFNfUiBBGRI/Pg+r8hXfs3nkTQQxdffDHvvfcemZmZTJw4kalTp5Kd\nnc2iRYsIDQ2lS5cuFBUVeR2miOestWzM2cuS9HyWpuezJD2ftO27KS33wyzQJ8jAmN5xXD6iE6N7\ntNa6alITC4EexpgEXGI3Cbis2jEfApcCLxtjYnFDRTcA64G/GmOifccl44rK1LvkxHge/Ww1X6zM\n4qqRXbwIQUTELzWUa//GkwgepeeuLk2cOJEbbriBnJwcvv76a9555x3atGlDaGgoM2fOZPPmzZ7F\nJuKl3IJilmbks2RLPksydrE0PZ9dhaUARIQF0799S649NYFecS0ICQ7yONofM8CQztHqDZFjYq0t\nM8bcCnyOm//3krV2hTHmQSDVWjvNty/ZGJMGlAP3WGtzAYwxf8YlkwAPVhaOqW/d2zSna+tmpKxQ\nIigifsqj6/+Gcu3feBJBD/Xt25c9e/bQvn172rZty+WXX86ECRPo378/SUlJ9O7d2+sQRepcUWk5\nK7btZomvp29J+k7S8woB16vWM64FZ/eLZ1DHKAZ1iqJHmxbqXZOAZa2dDkyvtu0PVZ5b4Je+R/Vz\nXwJequsYayI5MZ4pszewa18pLSO0FIqICDSca38lgvXkhx8OjE+OjY1l7ty5hzxOawhKIKiosGyo\nNsRz5fbdlFW4IZ5tW4YzqGMUVwzvzMCOUfRv35JmTfTrSKShSe4bx3Nfr2fm6h1cMPiQxUtFRBql\nhnDtrysvETlh2XuK9yd8S9LzWZqRz56iMgCaNwlhQIeW3DC6q+vt6xhFXKQKqYgEgkEdomjdogkp\naZlKBEVEGhglgiJyTApLylm+bZdvXp+b37c13w3xDA4y9IprwYSB7RjUwQ3x7Na6uYZ4igSooCDD\nmX3imLZkK0Wl5YSHBnsdkoiI1JASQRE5rIoKy/rsAr6v7OlLz2dV5h7KfUM820c1ZVDHKK4+uQuD\nOkXRr11LmobpQlCkMUnuG8ebC7Ywd30uZ/Ru43U4IiJSQwGfCFprPV+ssT5Yf1xkTRqcHbuLqhRz\nyWdZxi4Kit0QzxZNQhjYMYqfn9aVQR2jGdixpdbKExFO7taKZmHBpKRlKhEUEb/QGK7/a+PaP6AT\nwfDwcHJzc2nVqlVA/2Ow1pKbm0t4uC7K5dhZa5mRlsXfZ6xhVeYeAEKCDL3btuCCwe0Y1DGaQR1b\n0jW2OUEa4iki1TQJCeb03m2YkZbFQxdYDQUXEU81huv/2rr2D+hEsEOHDmRkZJCdne11KHUuPDyc\nDh06eB2GNDDzN+Tyt89WsXhLPl1jm3H/uX0Y3CmKvu1aaq6PiNRYcmIc/1u2nSXpOxnSOcbrcESk\nEWss1/+1ce0f0IlgaGgoCQkJXoch4ndWbt/No5+tYubqbOIim/Dwhf25eEgHv1y0XUT83xm92xAa\nbEhJy1IiKCKe0vV/zQV0IigiB0vP28ffZ6zhwyVbadEkhN+M783VJ3dRgRcROSGR4aGM6NqKlBVZ\n3Du+d8AOxxIRCSRKBEUagZyCYp75ah1T528myBhuGt2NX5zWjZYRoV6HJiIBIjkxjt9/tIL12QV0\nb9PC63BEROQolAiKBLA9RaW8OHsjU2ZvoLisgkuSOnDH2J7Et1RhIRGpXWf6EsHPV2QpERQRaQCU\nCIoEoOKycqbO28IzM9eRt7eEc/rH86vkXnRr3dzr0EQkQLVt2ZSBHVqSkpbFLWd09zocERE5CiWC\nIgGkvMLy4fdb+fuMNWzNL+Tkbq34zfjeDOwY5XVoItIIJPeN57HPV5O1u4i4SI08EBHxZyoRKBIA\nrLV8uTKLc56cza/eXUp0s1Bev24YU68friRQROpNcmIcADPSsjyOREREjkY9giINXOqmPB75dBWp\nm3fSpVUET186mHP7t9Xi7yJS77q3aU6XVhGkpGVxxYjOXocjIiJHoERQpIFanbmHxz5fxRcrd9C6\nRRMeuqAfE4d2JFRrAYqIR4wxJPeN5+XvNrK7qJTIcFUmFhHxV0oERRqYjJ1uLcAPvt9K87AQ7jmr\nF9ec0oWIMP13FhHvJSfG8cI3G5i1OpvzBrbzOhwRETkMXTmKNBC5BcU8M3MdU+dtAQM3jOrKL07r\nRnSzMK9DExHZb3CnaGKbh5GyIlOJoIiIH1MiKOLnCorL+Pfsjbw4ewP7Ssr42ZAO3HlmT9pFNfU6\nNBGRHwkOMpzZJ45Plm2nuKycJiHBXockIiKHoERQxE+VlFXwxvzNPP3VOnL3lnBW3zjuOauXFmoW\nEb83LjGOtxamM29DHqf1bO11OCIicghKBEX8TEWFZdrSbTw+YzXpeYWM6BrDlPG9Gdwp2uvQRERq\n5JTusUSEBZOyIlOJoIiIn1IiKOInrLXMWp3N3z5bxarMPfRpG8kr1/TjtJ6tMUZLQYhIwxEeGsxp\nPVszIy2LP5/fT8vZiIj4ISWCIn5g0ead/O2zVSzYmEenmAienDSICQPa6eJJRBqs5L5xfLo8k6UZ\n+RrRICLih+o0ETTGjAeeBIKBKdbaR6rt/wdwhu9lBNDGWhvl29cJmAJ0BCxwjrV2U13GK1Lf1mbt\n4dHPVzMjLYvY5mE8eH5fJg3tRFiI1gIUkYZtTK84goMMKWlZSgRFRPxQnSWCxphg4FlgHJABLDTG\nTLPWplUeY629q8rxtwGDq7zFa8BfrLUzjDHNgYq6ilWkvm3NL+SJGWt4f3EGEWEh/GpcT649NYFm\nTdRJLyKBoWVEKCO6xjAjLYvfjO/tdTgiIlJNXV51DgPWWWs3ABhj3gLOB9IOc/ylwB99xyYCIdba\nGQDW2oI6jFOkTpWWV7BjTzGZu4rI3FVE6uY8ps7fAhauOSWBW87oTozWAhSRADSuTxwPfJzG+uwC\nurVu7nU4IiJSRV0mgu2B9CqvM4DhhzrQGNMZSAC+8m3qCeQbY/7r2/4FcK+1trzaeTcCNwJ06tSp\nVoMXqYmC4jIydxWRtbuI7b6fmbuqPN9dRE5BMdYeOMcYuOikDtw1rifttRagiASwcX3jeeDjNGak\nZdHtNCWCIiL+xF/GoU0C3quS6IUAo3BDRbcAbwNXA/+uepK19gXgBYCkpCSLSC2pqLDk7SvZ34uX\nufvAz/1J364i9hSX/ejclk1DadsynLjIcBLbRhLfMtw9It3PdlFNadk01INvJSL+ogZz6K8GHgO2\n+jY9Y62d4tv3KHAuEATMAO6w1vplG9g+qin92keSsiKTn5/WzetwRESkirpMBLfiCr1U6sCBBq26\nScAtVV5nAEuqDCv9EBhBtURQ5HiUlFXs762r2puXudsld9t3FbFjTxGl5QdfVwUZaNPCJXPdWzfn\n1O6x+xO8uMjw/clf07Bgj76ZiDQENZlD7/O2tfbWaueeDJwCDPBt+hY4DZhVp0GfgOTEeP7xxRp2\n7C6iTWS41+GIiIhPXSaCC4EexpgEXAI4Cbis+kHGmN5ANDC32rlRxpjW1tpsYAyQWoexSgApK69g\n+bbdrMnaQ1a13rzMXUXk7i350TlNQ4P3J3XDEmIOSuziW7rnsc2bEKzlHETkxB3rHPqqLBAOhAEG\nCAWy6ijOWpHcN46/z1jDFyt3cNlwTeMQEfEXdZYIWmvLjDG3Ap/jhr68ZK1dYYx5EEi11k7zHToJ\neKvqsBZrbbkx5m7gS+NW0l4EvFhXsUrDVlFhWZm5m7nrc5m7PpcFG/MOGrIZ0yxsf2I3oEMU8ZVJ\nXpXhmpHhIVq0XUTqS03n0F9kjBkNrAHustamW2vnGmNmAttxieAz1tqVdR7xCegV14JOMRHMSMtU\nIigi4kfqdI6gtXY6ML3atj9Ue/3AYc6dwYGhLyL7WWtZn13AHF/iN29DLjv3lQKQENuMCYPaMbJr\nKwZ0aElcZDjhoRqqKSINzsfAm9baYmPMTcCrwBhjTHegD266BcAMY8woa+3s6m/gLwXVjDGMS4zj\n9bmbKSguo7mWyRER8Qv6bSx+z1rLlrx9zF2f65K/Dblk7ykGXCGCsX3iOLlbK0Z2a0XblqrCKSJ+\n76hz6K21uVVeTgEe9T3/KTCvclklY8ynwEjgR4mgPxVUS06M49/fbuTr1dmcO6Ctl6GIiIiPEkHx\nS9t3FTJnnUv65q7PZWt+IQCtWzTh5G6tXOLXNZaOMU01pFNEGpqjzqE3xrS11m73vTwPqBz+uQW4\nwRjzMG5o6GnAE/US9QkY0jmamGZhpKRlKhEUEfETSgTFL2TvKWbeBtfjN29DLhtz9gIQHRHKyG6t\n+PlpXRnZLZZurZsp8RORBq2Gc+hvN8acB5QBebgllADewxVQ+wFXOOYza+3H9f0djlVIcBBje7fh\nsxWZlJRVEBYS5HVIIiKNnhJB8UT+vhLmbcjzJX85rMkqAKBFkxCGd43hihGdGdm1Fb3jWxCkSp0i\nEmCONofeWnsfcN8hzisHbqrzAOtAct943l2UwfyNuYzq0drrcEREGj0lglIvCorLWLgxjznrc5i7\nIZcV23ZjrVu2YWhCDD8d3IGTu7Wib7tIQoJ1p1hEJNCM6hFL09BgZqRlKREUEfEDSgSlThSWlLNo\n807mbshhzvpclmXsorzCEhYSxEmdorjrzJ6M7NaKgR2iNERIRKQRCA8NZlSPWFJWZPGn8/pqmL+I\niMeUCEqtKCmrYEl6vuvxW5/L91vyKSmvICTIMLBjFL84rRsnd2vFSZ2jtZyDiEgjldw3npS0LH7Y\nuosBHaK8DkdEpFFTIijHray8glfnbmbW6h0s3JRHUWkFxkC/di255pQujOjWiqFdYrRmlIiIADC2\ndxuCDKSsyFIiKCLiMV2hy3H76/RVvPTdRnrFtWDS0E6c3K0VwxNa0TIi1OvQRETED0U3C2NYQgwp\naZncfVYvr8MREWnUlAjKcXlj/hZe+m4j156SwB8mJHodjoiINBDJifE8+Ekam3L20iW2mdfhiIg0\nWqrSIcdszvoc/vDRck7v1ZrfntPb63BERKQBGZcYB8CMtCyPIxERadyUCMox2Zizl1/8ZzEJsc14\n6tLBWupBRESOSceYCPq0jSQlLdPrUEREGjVdxUuN7Sos5bpXFxJk4N+ThxIZrrmAIiJy7JIT40jd\nvJOcgmKvQxERabSUCEqNlJVXcOsbi0nP28dzVwyhU6sIr0MSEZEGKrlvHNbClys1PFRExCtKBKVG\n/vxJGrPX5vCXC/ozvGsrr8MREZEGLLFtJO2jmpKyQomgiIhXlAjKUb0+dxOvzt3MjaO7csnQjl6H\nIyIiDZwxhuS+ccxel8Pe4jKvwxERaZSUCMoRfbs2hwc+TmNs7zb8ZrwqhIqISO0YlxhHSVkFs9dm\nex2KiEijpERQDmt9dgE3T11E99bNefLSwQQHGa9DEhERL6QvgPeuhbLaK+4yrEsMLZuGanioiIhH\nlAjKIeXvK+H6V1MJDQ5iyuQkmjcJ8TokERHxSvZqWP4+vDMZykpq5S1DgoMY26cNX67aQWl5Ra28\np4iI1JwSQfmR0vIKbp66mK07C3n+yiF0jFGFUBGRRu2kK+Hcx2HNp/DeNVBeWitvm5wYz67CUhZu\nzKuV9xMRkZpTIigHsdbyx2m1pGYQAAAgAElEQVQrmLM+l4cv7E9SlxivQxIREX8w9Ho4+1FY9Qm8\nfz2Un3iRl9E9Y2kSEkRKmoaHiojUNyWCcpBX52zijflb+MXp3bhoSAevwxEREX8y/CY466+Q9iF8\ncOMJJ4MRYSGM6tGaGWlZWGtrKUgREakJJYKy36zVO3jwkzSSE+O4J7mX1+GIiIg/GnkLnPknN2fw\no5uhovyE3i45MY6t+YWs2La7lgIUEZGaUAUQAWDdjj3c9sb39IqP5B8TBxGkCqEiInI4p94JFWXw\n1Z8hKATOewaCju/e8tg+bQgykJKWRb/2LWs5UBERORz1CAo795Zw7SupNAkNZsrkJJqpQqiIiBzN\n6Lvh9PtgyVT45A6oOL7Kn62aNyGpcwwpKzJrOUARETkSJYKNXElZBT//zyIydxfxwlVDaB/V1OuQ\nRESkoTjtNzDqblj8Gkz/FRznPL/kvnGsytxDet6+Wg5QREQOR4lgI2at5fcfLmf+xjwe+9kATuoU\n7XVIIiLSkBgDY+6HU+6E1Jfg018fVzI4LjEOQNVDRUTqkRLBRuzf327k7dR0bhvTnfMHtfc6HBER\naYiMgTMfgJG3woIX4PPfHXMy2LlVM3rHt9DwUBGReqTJYI3UV6uy+Ov0lZzdL567zuzpdTgiItKQ\nGQPJD7kCMvOehaBgGPeg215D4xLjeHbmOvL2lhDTLKwOgxUREVCPYKO0OnMPt7+5hMR2kTx+yUBV\nCBURkRNnDIx/xC08P+cpV1H0GHoGkxPjqbDw5UoNDxURqQ9KBBuZ3IJirnt1IRFhwbx4VRIRYeoU\nFhGRWmIMnP0YDLkaZj8Osx6p8an92kfStmW45gmKiNQTJYKNSHFZOT//zyKy9xTz4lVJtG2pCqEi\nIl4wxow3xqw2xqwzxtx7iP1XG2OyjTFLfI/rq+zrZIxJMcasNMakGWO61GfsRxUUBOf+AwZfAV8/\nAl8/VqPTjDEkJ8Yxe202hSUntki9iIgcnRLBRsJay2//u5yFm3by+CUDGdgxyuuQREQaJWNMMPAs\ncDaQCFxqjEk8xKFvW2sH+R5Tqmx/DXjMWtsHGAbsqPOgj1VQEEx4CgZeCjMfgtl/r9FpyX3jKSqt\nYPba7DoOUERElAg2Es9/s4H3F2dw55k9+MmAdl6HIyLSmA0D1llrN1hrS4C3gPNrcqIvYQyx1s4A\nsNYWWGv9c/G9oGA4/1nofzF8+SeY8/RRTxmWEENkeIiGh4qI1AMlgo1AyopM/vbZKn4yoC13jO3h\ndTgiIo1deyC9yusM37bqLjLGLDPGvGeM6ejb1hPIN8b81xjzvTHmMV8Po38KCoYLnoO+P4WU+2He\nv454eGhwEGN6t+HLlVmUlVfUU5AiIo2TEsEAl7ZtN3e+vYQB7VvyfxcPxBxDKW8REfHMx0AXa+0A\nYAbwqm97CDAKuBsYCnQFrj7UGxhjbjTGpBpjUrOzPRxqGRwCF74IfSbAZ/fCghePeHhy33h27isl\ndfPOegpQRKRxUiIYwHbsKeL6VxcSGR7KC1clER7qvzeNRUQaka1AxyqvO/i27WetzbXWFvteTgGG\n+J5nAEt8w0rLgA+Bkw71IdbaF6y1SdbapNatW9fqFzhmwaFw0UvQ6xyYfjekvnzYQ0f3bE1YSBAp\nKzQ8VESkLikRDFBFpeXc9Poi8vaVMGVyEnGR4V6HJCIizkKghzEmwRgTBkwCplU9wBjTtsrL84CV\nVc6NMsZUZnZjgLQ6jrd2hITBxa9Aj7Pgkzth8euHPKx5kxBO7R7LjJWZ2GNYh1BERI6NEsEAZK3l\n3veX8f2WfP5xySD6tW/pdUgiIuLj68m7Ffgcl+C9Y61dYYx50Bhznu+w240xK4wxS4Hb8Q3/tNaW\n44aFfmmM+QEwwJHHWvqTkCZwyWvQbSxMuw2WvHHIw5IT40jPK2RV5p56DlBEpPHQauIB6J+z1vPh\nkm3cndyTs/u3PfoJIiJSr6y104Hp1bb9ocrz+4D7DnPuDGBAnQZYl0LDYdJUeHMSfHgzBIXAgEsO\nOmRsnziM+YGUFVn0aRvpUaAiIoFNPYIB5rPl23ns89WcP6gdt5zR3etwREREfiy0KUx6E7qcCh/c\nBMvfP2h36xZNOKlTNClpmR4FKCIS+JQIBpDlW3dx19tLGdQxir9dNEAVQkVExH+FRcBlb0PHEfD+\nDZD20UG7kxPjWLFtNxk7/XOZRBGRhk6JYIDYsbuI619NJToilBeuGqIKoSIi4v/CmsHl70CHJHjv\nWlj1v/27kvvGAzBDi8uLiNQJJYIBoKi0nBteS2V3USlTJg+lTQtVCBURkQaiSQu4/D1oOwjemQyr\nPwMgIbYZPdo0VyIoIo2DtbA3p14/UolgA2et5e53l7Js6y7+MXEQie00qV5ERBqY8Ei44n2I7wfv\nXAlrvwAguW8c8zfmkb+vxOMARUTqUN5GmPozeOksKCs++vG1RIlgA/fUl+v4ZNl2fn1Wb87yDaMR\nERFpcJpGwZUfQOve8NZlsP4rxiXGU15h+WrVDq+jExGpfWUl8M3/wT9HwJb5MPQGV0m5nigRbMA+\nWbaNf3yxhgtPas/PT+vqdTgiIiInpmk0XPURxPaANy9lQMkS4iKbkLJCw0NFJMBsngvPj4Kv/gw9\nkuHWBTDi5xBUf3U+lAg2UEvT8/nVO0tJ6hzNwxf2V4VQEREJDBExLhmMTiDorUu5odN2vl6TTVFp\nudeRiYicuH158NGt8PJ4KNkHl70DE1+HyHb1HooSwQYoc1cRN7yWSmzzJjx35RCahKhCqIiIBJBm\nsTB5GrTsyDWbfk1iWRrfrq3fIgoicgIWvQIvnwtp01wRFHF/DkvegGeSYOmbcModcMs86HmWZyEp\nEWxgCkvKuf61hewtLuPfVycR27yJ1yGJiIjUvuZtYPI0giLb8WrYo6xO/crriETkaCrK4dN74eM7\nIHOZK/70/GhY83njTgiz18CrE+DDX0BMN7jpGxj3oFtCx0N1mggaY8YbY1YbY9YZY+49xP5/GGOW\n+B5rjDH51fZHGmMyjDHP1GWcDUVFheVX7y5hxbbdPHXpYHrHq0KoiIgEsBbxmKs/pjAsmskbfkl5\n+iKvIxKRwynaDW9Ogvn/ghE3wz3r4IJ/QdEueOMSmHImrJ/ZuBLC0iL46i/w3CkuMf7JE3Dt5xDX\n1+vIgBokgsaY24wx0cf6xsaYYOBZ4GwgEbjUGJNY9Rhr7V3W2kHW2kHA08B/q73Nn4FvjvWzA9UT\nX6xh+g+Z/PbsPoztE+d1OCIiInUvsh1LxvyHvIpm2NcvgG1LvI5IRKrbudktfbDuSzj37zD+YQhp\nAoMug9sWwYQnYU8mvH4BvHIubPrO64jr3vqZ8K+R8M2jkHgB3JoKSddAkP8MyKxJJHHAQmPMO74e\nvppWJRkGrLPWbrDWlgBvAecf4fhLgTcrXxhjhvg+O6WGnxfQPlqylae+WsclSR24flSC1+GIiIjU\nmxGDB3Bl+e8pIAJeOx8yf/A6JBGplL4ApoyFXVvdeqBDrzt4f3AoDLkabl8MZz8GuevglXPgtQsg\nI9WTkOtUwQ54/3qX9GLgyg/hohfdcHc/c9RE0Fp7P9AD+DdwNbDWGPNXY0y3o5zaHkiv8jrDt+1H\njDGdgQTgK9/rIOBx4O6jxdcYLE3P5573ljEsIYaHLlCFUBERaVxahIfSpVsffh78ADasObx6HmSt\n8DosEVn2LrzyEwhrDtd/Ad3OOPyxIU1g+I1w+xJIfsgNlZwyFt6YCNuX1l/MdaWiAlJfcsVg0j6C\n034Dv5hz5D8Tj9Wob9Jaa4FM36MMiAbeM8Y8WktxTALes9ZW1oa+GZhurc040knGmBuNManGmNTs\n7OxaCsX//GvWeiLDQ3juiiGEhfhPd7KIiEh9Se4bx7ydkWz6yZvugvLV82DHKq/DEmmcKirc3Lf/\nXg8dkuCGr6B1z5qdGxYBJ98GdyyDsX+ALfNcQZm3r4CstLqNu65kLndDYz+5C+IHuATwjN9CaLjX\nkR1RTeYI3mGMWQQ8CnwH9LfW/gIYAlx0hFO3Ah2rvO7g23Yok6gyLBQYCdxqjNkE/B9wlTHmkeon\nWWtfsNYmWWuTWrdufbSv0iCVlVcwZ30OY3vHEdMszOtwREREPDHONzf+fxlNYfInbtHlVydAzlqP\nIxNpZEoL4f1r3dy3QVe4oY8RMcf+Pk2aw6hfwZ3L4LR7Yf0s+NfJ8N51kLOu1sOuEyV7IeX3LpHN\nWw8XPAeTP4bYHl5HViM16V6KAS601p5lrX3XWlsKYK2tAH5yhPMWAj2MMQnGmDBcsjet+kHGmN64\nHsa5ldustZdbaztZa7vghoe+Zq39UdXRxmDZ1l3sLirj1B6xXociIiLimTaR4QzuFEVKWhbEdnfJ\nINYNS8tQNVGRerEnyxV7WfEhnPknOP8ZCDnBjorwlnDGfS4hPPVOWD0dnh0KH94MOzfVSth1YvVn\n8OwImPOUK4pzayoMuhQa0BSumiSCnwJ5lS98SzoMB7DWrjzcSdbaMuBW4HNgJfCOtXaFMeZBY8x5\nVQ6dBLzlG34q1cxek4MxcEp3JYIiItK4jUuMY1nGLrblF7phaJM/BixMGQNvXe6GZ4lI3cj8AV4c\nAztWwsT/uKStNpOeiBg48wE3ZHTEzbD8fXh6iFuTcNcRZ4vVr11b3TDWNye6Ya7XfOoS4uPpFfWY\nOVr+ZYz5HjipMlHzFXJJtdaeVA/x1VhSUpJNTQ28ykMXPzeH4rIKpt16qtehiIj4DWPMImttktdx\nNBSB0kau21HAmX//mgfP78tVI7u4jUW7Yf5zMOcZKN4FfS+E0++r+XwlETm61Z+6IZvhLeGyt6Dt\nwLr/zN3bYfbjsOgVl3AOuQZG/RJaxNf9Zx9KeRksfBG+eggqyuC0X8PI2068R7SWHUv7WJMeQVO1\nt843JDTkeIOTmttTVMriLfmcqt5AERERurdpTtfWzUhZkXVgY3ikuyC7cymMuhvWpsA/h8MHP4e8\nDd4FKxIIrIU5T8Obl7qbKzd8VT9JIEBkWzj3/9yyEwMnwcIp8OQgSLkf9ubUTwyVti52Iw8+uxc6\njYCb57n5jX6WBB6rmiSCG4wxtxtjQn2POwD9Zq0Hc9fnUl5hGdUjMAvh1Jnd2+Cz+2Da7e4/roiI\nBIzkxHjmbchlV2HpwTuaRsPY38MdS2HkLW4O09NJri3ITz/0m4nI4ZWVwMe3u8Qr8Ty4erpLzupb\nVCc472m4LRX6XgBzn4UnBsCXD0Lhzrr97KLdMP3XbpmLPZnws5fh8vcgJjDW9K5JIvhz4GRcxc8M\nYDhwY10GJc6363KICAvmpM5RXofSMBTscAngk4NgwQvww3vw4hkwZZx7XlbidYQiInKCkvvGUVZh\nmbV6x6EPaBbr1ii7YwkMvR6WvglPnwT/u9sNNRORo9uXB/+5EBa/5nraf/aKmw/npZiu8NPn4Ob5\n0PMsN2z0iYEw628uYatN1rqbSc8Oc9eUQ6+HWxdCvwsbVDGYoznqEE9r7Q5cQRepZ7PX5jA8IYYm\nIcFeh+Lf9uXBd0/AghehrAgGXgqj73GTdpe84f4Dv38dNI+HpGthyNXQIs7rqEVE5DgM6hBF6xZN\nSFmRxfmD2h/+wBbxcM6jcMrt8M1jsOhl+P51d0F3yp3QXKNtRA4pZx28cQnsSoefPu+GZfqT1j3h\n4pfd0MxZD8Osv8L8f8Epd8CwGyGs2Ym9/85NMP0eN8w8fgBMmgrth9RK6P7mqImgMSYcuA7oC+xf\nFdFae20dxtXopeftY2POXq4c0dnrUPxXYb4bHjDvn24dl/4/c+vQxHY/cMyIX8Cwm2DdF7DgeffL\n4pvHoO9PYfhNbhFUkUCyNxc2zXZrrPWZ4HU0jYIxphuQYa0tNsacDgzALXuU721kgSkoyHBmnzim\nLdlKUWk54aFHuVnasgNMeNIlf18/6tqM1JddG3DybQ2y0p9Indn4Dbx9pWtDJn/s5sP5q/h+Lknb\nuhhm/hW+eMBdF556l7vxH9r02N6vvNTNh/z6UTBBcNbDLrEMDtzSKDUZGvo6EA+cBXyNWxh+T10G\nJW5YKMDonioU8yPFe+Drx+DJAW4x0+5j4ea5cNGUg5PASkFB0DMZrngfbl3kfjms/tSN935xDCx9\nG8qK6/97iNSGwnxYNR0+vRf+dQo81hXenexKWy971+voGov3gXJjTHfgBaAj8Ia3IQW25L5x7C0p\nZ+763JqfFJMAP/0X3LIAeo2Hb/8BTw6EWY/U/rAykYZo0avw+k9db/r1X/p3ElhV+5Pgivfg2hRo\n0wc+/y08Ndg3UqyG13db5sFzo+DLP7nrylsXwMibAzoJhBouH2GtHWyMWWatHWCMCQVmW2v96l9H\noJTGrnTz1EUs3pzP3PvGYAJoLPIJKdnr/lN/9yQU5kGvc1yJ8LYDjv29inbD0rfcsNHctdCsDSRd\n40oTezERWqSmigtcg7XpG3fndvtSsBUQEg4dh0PCaOhyKnz5Z8hYCFf/DzoO9TrqWudPy0cYYxZb\na08yxtwDFFlrn65sO72OrVKgtZHFZeWc9OAMzhvUjocvPI42ACBrhetFWPWJKzRz8u2ul/BEh5WJ\nNDQV5TDjDzD3Geg21g27DG/pdVTHb+NsmPkX2DIXWnZ0VYUHXgrBoT8+dl8efPFHNxcysgOc8xj0\nPqf+Y65Fx9I+1iQRXGCtHWaM+Qa4GcgEFlhru554qLUnkBq58grLSX+eQXJiHI9dXE8lev1ZaRGk\nvuTu3u7d4X5JnfE76FAL47UrKmDDVzD/BTcWPCgYEs+H4T+HDkMDakKwNFClRZCxwCV9G7+BrYvc\n+kVBoe7faMJoSBjlnoc0OXDevjzX412y15X7juro3XeoA36WCM4HngB+B0yw1m40xiy31vbzOLT9\nAqmNrHTLG4uZvyGPBb8dS1DQCfyu3va9SwjXpkCz1sc/rEykISougPevhzWfumGQZz0cGL1g1sL6\nL+Grv8C2xRCdAKffC/0vdtd61sKyt+Hz37nKoyNvdtOLmjT3OvITdiztY03+pl8wxkQD9wPTgObA\n708gPjmKH7buYldhKaN6NvKJ7GUl8P1r8M3jsGcbdBkFE1+v3aEKQUHQ/Uz3yF0PC//tigksfx/a\nDnJ3h/teCKHhR38vkdpQVuIarcrEL30BlBe7+QrtTnK9FgmjoOOII1dwi4iBy96GKWe69Z+u/Swg\nGjg/dQ2uwvZffElgAm5axWEZY8YDTwLBwBRr7SPV9l8NPIar2A3wjLV2SpX9kUAa8KG19tba+iIN\nSXJiHP9btp3v0/MZ0jn6+N+o3WC4/F33f+2rh9ywsjlPu0IUJ1118A0WkUCSn+7ahx1pcM7/wbAb\nvI6o9hjjru26jYU1n7mE8IObYPbf3fIyy99zbWz7JLjqQ4jv73XEnjhij6AxJgj4mbX2nfoL6fgE\n0t3Op79cy9+/WMOi+8cR06xhL1R5XMpLXbnvrx+DXVvcBe+Y37mej/pQXADL3nK9hDmrISLWVRpN\nuhZaHqFCncjxqCiH7UvcUJaN37ihLKX7AOMapoTR7tFppFs4+1it+wKmXgw9x8PEqe7mRwDwpx7B\nqnw3Tjtaa5cd4ZhgYA0wDrcs00LgUmttWpVjrgaSDpfkGWOeBFoDeTVJBAOpjay0q7CUIX+ewXWj\nErjv7D6198YHDSvrBKfdc/hhZSINVUaqSwLLitxQ0O5neh1R3aqogJXTXJXR7FXQpCWc+Uc3JShA\n2sVKtdYjaK2tMMb8GvD7RDCQzF6bQ992kY0vCawohx/edRP3d250vR8T/uHu5tTnEM0mzV158aTr\nYMMsN49w9uNuaGqfCW7YaKcRGjYqx6eiAnasOJD4bZ4Dxbvcvta9YfAVLvHrfErtVDPsfiaMfwQ+\n/bWbBD/uTyf+nnIQY8ws4Dxcm7oI2GGM+c5a+8vDnDIMWGet3eA7/y3gfFwPX00+bwgQB3wG+F0y\nXF9aNg1lZLdWpKzI4t7xvWtvPn3CKOjyKaz/yvUQTrvN/f4/7V5XnTpISzpJA7f8ffjwZmge5yqD\ntuntdUR1LyjILUbfZ4Jrd1v3guZtvI7KczUZGvqFMeZu4G1gb+VGa21enUXViBUUl7F4y05uGO1X\nUzDrVkUFpH3gEsCcNRDXHya9Cb3O9jbZMga6neEeOzfBwiluMnGabwjBsJvcRUFDnEdSVgy562DH\nSjckZMcqd4esdW84/TfQVnNTa421kLMWNn7tlnXYONsVOwK3OG6/n7phz11G1d36lsNudH+/3z3h\nGr9Bl9XN5zReLa21u40x1+OWjfijMeawPYJAeyC9yusMYPghjrvIGDMa13t4l7U23TdS53HgCiDA\nb+EfXXJiHL//aAXrswvo3qZF7b2xMa5yYLcxVYaV3ehuCp5xH/Q5P+B6EaQRsNYtjTDrr26UycT/\nQLNGVp0+KNjd7BGgZongRN/PW6pss0AjylTqz7z1uZRVWEb1aAT/Ma2FVf9z3fRZy10ScvGr0Oc8\n/2tgo7tA8kOuSumyd1wv4bRbXZWtIZNd76E/FuMoL3O9qzvSfEmf75G7Dmy5O8YEQ2wPV3J502x4\n/n/u7+CM37ptcmysdTcONn5zIPEryHT7WnZ0Nzi6jHINUcsO9ROTMXD2o24e7LTb3aT5ziPr57Mb\nhxBjTFvgElzBmNrwMfCmb23Cm4BXgTG4om3TrbUZR+sBM8bcCNwI0KlTp1oKy7+MS4znD9NW8OAn\nK3n2ssG0CK/l4ZvGuP+zPc5yw8pm/hXevdrdsDzjt97fsDyaol3u99H+x2b3c/c2lwBEdYaoThDt\n+xnV2S0doF7PwFNa5K5bfnjXDXWe8KTmv8rRq4Y2FIEy/+GPHy3nndQMlvxxHE1CAvQXsbWwdoab\ng7F9CcR0cwlWvwsbTuNjLWz6FuY/B6unu229z3XDRjufUv8XBhUVbj5l1WRvx0rXw1peuYaOceto\nte7jErzKR6vuBxqDol0w959uQdaSAuh3kauyFdujfr9PQ7Nrqy/p+8Ylfru2uO3N43xJn6+yZ3SC\ntxeNhTtd8ZjCnXDDTHfx10D50xxBY8zFuCJq31lrf2GM6Qo8Zq296DDHjwQesNae5Xt9H4C19uHD\nHB+MmwvY0hgzFRgFVOCKt4UB/7TW3nukGAOljTyUN+Zv4fcfLad76+ZMmZxEx5gjFFE6URXlbljd\nrIchb4ObwjDmd/U/haFSeSnsyjg42cvffOB54c6Dj28a7W5stmgH+3Igfwvs2X7wMUGh7sbmQUli\n5wOvm7fx7+S3tpWVuD+rkn3uz6IhzhUt2AFvXe4qUI/9A5z6y8b1d9jI1PbyEVcdaru19rXjiK3O\nBEojN+bxWXSKieCVa4Z5HUrts9bNuZv5F7e+WVQnN+diwMSGXao4f4urNrr4VdfotukLw2+E/pcc\nuarj8bDWNdpVk73slW5oZ+neA8dFdjg42WvTB2J71TyefXkw5ymY/7ybSD5gkluHJyahdr9PQ5aV\nBotehnVfQt56t61pdJXEbzTE9vS/xjZnHUwZ4y4Er0s5vgI0fsCfEsFjZYwJwQ33HIurCroQuMxa\nu6LKMW2ttdt9z38K/Kb6+r1HKyhTVaC0kYcze202N09dTJOQIJ6/MunEqojWRHmZr6jZo1WKmt1f\n+0POrHXtys6NP+7V27nJJYGVozvAl8R1csneQQ9fItc06sefUVrk3id/k2vPdm52P/M3u+f7cg4+\nPiT8QO9h9d7E6C7u96C//d6rqjKx25tz4Of+59mwN9f93JfjnlfO4Qb33eP6QbtBrtps20FuNJM/\nX8NkrYA3JrrveOHzboksCWi1nQg+XeVlOK7hWmyt/dnxh1j7AqGRy9i5j1P/NpPf/ySR604NsAvu\nTd+5BHDzdxDZHkbfDYOugJAAKohTWuiGXMx/AbJ+gPAoV3p86PXH1/OyN6dKwpfm5njtSHO9dpWa\ntamW8CW6OWC1tRBsQbabV7Zwilu7btDlMPoe/xwGWx/Ky9xaS/Ofdz2AIeHQ9Qx38Zcw2t0E8Ldh\nzYeyYRa8fqGbA3XpWw2nJ74Kf0oEjTEdgKeBU3ybZgN3WGszjnDOObi1B4OBl6y1fzHGPAikWmun\nGWMexhWgKQPygF9Ya1dVe4+rUSK437odBVz36kK27yrisZ8N4PxB9VDlef8yR//nbtIljIYz7odO\nh5ryebj3KHZl/Hducglf1R69nZuhePfBxzdrfYhEr4tLxCLb1f7/55K9vsSwMkmsfPheF+UffHxY\n80MniZWva3uh8rIS2Jd7mETuEAlf1cSuKhPshstGxLqf+5+3hmat3O/7rBWwbYkbzVRS4M4LaQrx\n/Q4khu0Gu5uA/pAcrkmB965xfyeXveVik4BXq4ngId48CnjLWjv+eIKrK4HQyL21YAv3/vcHUu4a\nTc+4Wpz07qX0hTDzIXfh2TzOty7T5MBel89aV3Z8/nOw8hPAQq9zXMGOhNE/vlNatMv16FVN9nas\ndA1ZpfCWLsmrTPba9HFDPJu1qp/vtHs7fPt3WPSKe33SZPd3Gdm2fj7fa/vyXKGghVNgV7qb6zf0\nOvfnUBuVPb2w8N/wv1/CyFvhrL94Hc0x87NEcAbwBgfWDrwCuNxaO867qA4WCG1kTezcW8JN/1nE\ngo153D6mO3ee2fPEFpuvqdJCSH3Z/Z7cmw3dx7kho+0GuzZhb/ahe/R2boLdW3GlF3xCwg/0rh2q\nV8/f1gMt2lUtUazSm5i/+UDCVCk86tBDTiuTxuAwl9jtzT5EIpd98L59OQffHK3KBENEqwOJXLPW\nh0jyfIleRCsXV01v5FVUuJEg274/kBhuX3pwcth2wIHEsN0glxzW1003a901yOe/dT2Yl76l5a8a\nkbpOBEOB5dbaXscTXF0JhEbulqmLSd2cx7z7xtZeGWyvbFviJtWv/dz9gj31LldQpbaHSvq7XRmQ\n+pJLoPbluuRt4ET3fIdcHeMAACAASURBVIdvSOfuKp0Goc1cGef9vXu93c8W8f4x1CY/Hb55DJZM\nhaAQ19t5yp3QvLXXkdWNzB9c798P77ohsl1GwfCboOfZ/nG390RN/zUseB4mPOWKHjUgfpYILrHW\nDjraNi8FQhtZUyVlFfzug/9v777jo6ry/4+/TnpvJEBIo/dOBOkK2BVQVwTXgquy6qqrbtNdV7e4\nq+5vv2tfV7CuBQWUomJBWCUgokF6kQDSkgAJhF5CkvP74w4QkJKEzNyZ5P18PPIgc+fOzCfXmDOf\nOed8PkuZuGAzl3VO5f+u6UJEqI/egJfuc4qJzX3aWdKZ1MKZKTy8//jzYlNPPasX0ygwVhZUxZGl\nrUcTw43Hzybu3AhlB6r2XEcTu1MkctEpxx+vTmJXGyrKnYJcBQudxLBgIRQuObZtIzQKGnc+fllp\ncqvaTw7LDzvtgnJfgbaXw1VjISy6dl9D/FptLw39gGMfVQUB7YEJZ9qY7muBPsiVV1h6PDqDIe0a\n8c9rArh0/9blTgK46kPnj3Dfe5w2C/72CaavHT7oFBj45kXnU8PgcEhpfXyy17CdM9MUCG8Advzg\n7I1Z8o7zyWevMdDnnsCdHausvMz5/f1mrLOUOSTSSd57joFGHdyOrnaVl8HbI5zWFjdMCaiS2n6W\nCM4EXgXGew6NAm621g52L6rjBfoYWV3WWl6cvY4nPllF57R4xt2YTcM4H65EObjbmZEpXPzj2b2E\njMBsO+QNR2ZLd248VuimvOwkSzNdSOxqQ0W50z7oSGJYsAi2LDn2wUBotDNzWHlZaYOWNf85D5TA\nhJucv+n97oNBDwfeNZOzVtuJ4MBKN8uADafb9+CWQB/kFm/aybDn5/L0yK6+2ddQ24pWO1XUlk+G\n8Fg4907ofWft7wUIdNbC3q3O4FYXZpSK85z+j8vec/Yg9L7T+W9/soIE/m5fsTNzm/uKs1QrIRPO\nuc1p8F4XEtxTObgLXroA9m2DW2dCgxZuR1QlfpYIZuHsEeyN88HpV8Dd1tpNp32gDwX6GFlTny7f\nwr3vLCIhKpSXbsqmQxONSeKyinKnonfBomOzh4VLjs2MhsU4vXwrLytNanHmhG77WqcoTMl6pzVE\nt596/UcR/1TbiWAzoNBae9BzOxJoZK1df7aB1qZAH+Sem5XHPz9bTe5DQ0iOCaC+LjvWwRdPwNIJ\nnpmhn0Ofu+v2G2f5sa0rnAa1Kz9wkv8+dzutNMIDYK9rwSJn9m/pJKfVRrOBTuytLwrIIio1smMd\njBvkfPJ+y4yASOT9KRE8GWPMvdbap9yO44hAHyPPxrL8Xdz6ei67Dx7mqWu7cmGHxm6HJHK88jJP\nclhpWemWpc6WBICwWCc5PLKstEk3px3RkeRw/Rx493rn+2vfgqZ9T/46Ui/UdiKYC/Sx1pZ6bofh\n9Eo656wjrUWBPshd++I89h4q46N7AmRp1qE98OUT8PUL9WOvmFRN4WJnafDqT5w9G33vdX43/G1v\naPlhpzn0/LGw6Wtn70aXUc7yz4Zt3Y7OHevnwH+HOQWNrpvo9zPWAZAIbrTW+k0X90AfI8/W1t0H\nue2/uSzN38UDF7dlzIDmgb8XX+q28jKngFzhomOzh1uXHUsOw+Oc5DCpOSx622nvdN27zm2p16oz\nPlZlpA85kgQCWGtLPcmg1JK9h8r4bmMJPwuElhHWOssAP3vI2QDf7XoY9EenmIlIahdnINqc67QL\nmfFHmPec07y2x2j3q8Xu3XZs+eeeQme/zkV/d9piBMAsmFc17QeXPwnT7nYqzV36D7cjCnTKMvxI\no7gI3h3Tm19PXMxjH69ibdFeHh3eibAQ7Z8SPxUc4rSlaNzRea8FzoeYRauOX1a65F1ofh5c/ZLG\nMam2qiSCRcaYodbaaQDGmGFA8RkeI9Uwf912DpdbBrTy89m0rStg+m9gwxznDf+INyDDryaGxV+k\nZ8MNk2HDPJj1KHzyO6dBff9fQbcbfN8/Mn+BM/u3/H0oL4UWg5w9FC0v0Eb6yrrfCEXfO8l7Smtn\nNldqqnolucXrIsOCeXZUN1qkRPPMrDVs2L6f/1zfg8RofbYtASI4FBp3cr663+Acq6jQOCY1VpVE\n8HbgLWPMc57bm4EbvRdS/ZOTV0xEaBA9shLdDuXkDu52CoLM/4+z5+uyfzmzO/Vl/5TUXFZvGP2h\nU8Fs1t+cvnVzn4IBv3WWYnpz+WFZKayY6lRq3fytswG/x2hn+WdyK++9bqC74C9OEaDpv3UKFLQ4\n3+2I/JYxZg8nT/gMoLKQfigoyHD/hW1onhLDbyctYfi/5/LyTefQsmE9r2wtgUtJoJyFKvcRNMbE\nAFhr957pXDcE8v6Hwf/3BWmJUfz3Zz3dDuV41sKSCc7yvr3bnNmCwY/4rom51C3WwprPnSWjBQud\nfQwDH4BOP6ndDxX2bHEaOy941anQmtTCSf66XgcRcbX3OnXZwd3wykVO9dRbZ/pl4uzvewT9TSCP\nkd6yYMMOxvx3AaXlFbzw0x70a5XsdkgiImetOuPjGT9GMMb83RiTYK3da63da4xJNMY8evZhCkDB\nzgOsLdrHAH8bgLYsg1cvhcljIC4NbpsJQ59REig1Zwy0ugBu+x+MfNsp0DJ5DPy7t9N2pKKi5s9t\nLWz6Ft67FZ7sCF8+7jTu/el7cFcunHu7ksDqiIiDUe9AUKhTjnz/DrcjEql1PbKSmPKLvqTGR3DT\nq9/w5tcb3A5JRMSnqjKffIm1dueRG9baEuBS74VUv8zJc7Zb9veX/YEHdsLHv4MXBzgbkq94xpkR\nSOvhdmRSVxgDbS+Dn+fANa85xyaOhhf7w6qPnKSuqsoOweJ3YNz58PIQ+P4TZ1/b3d/B9ZOg1RAt\nm6mpxCy49k3YtQkm3uQUKRCpYzKSonjvjj70b5XMQ1OW8ecPllNeoe2dIlI/VGWDTrAxJtxaewiO\n9hEMoEZ3/m12XhENY8Np3cjl/QkVFbDkHZjxsNNYO/tmpxqo+gGKtwQFQYcrod1QpxLtF4/BO9c5\n/ZHO/wO0HOIkjSezu8Cp/LngNdhXBMmt4dJ/QpeRgdG7MFBk9XaK6ky5wykUdfmTp/5vIhKgYiNC\neenGbP42fSWvzl3P+uJ9PDOqG7ERoW6HJiLiVVVJBN8CZhpjXsXZAD8aeN2bQdUXFRWWuWuKOb9t\nQ3f7GRUugem/hk3zIS0bfjrReTMu4gtBwdB5BHS4ChaPhy//AW/9BDJ6OQlh84HOedY6v6PzX3R6\nAFaUO03fe/0cmp+vBMVbul7nVBKd+xQ0bOdcb5E6JiQ4iEeu6ECLlBgembacn7wwj5duyiYjyc96\noIqI1KIzJoLW2ieMMYuBITjV0T4FsrwdWH2wvGA3JfsPu9c24kCJU8kx92WITIRhz0OX67SUTtwR\nHOKUw+58LSx8A2b/E/47FJr2d5aSLh7vNKwPj4det8M5t6hxrq8MfgS2r4FPHnCK77Qa4nZEIl5x\n/blZNG0QzR1vLWD483MZe2MPemRpZYyI1E1Vfce/FScJvAYYBKz0WkT1yOy8IgD6tvRxoZiKCvju\nDXg220kCs2+Buxc4DUuVBIrbQsKcJO+ehXDxE85s1CcPOPsBL/sX3L8CLvqbkkBfCgqCK1+Ehh1g\n0s2wbZXbEYl4Tb9WyUy+sy8xESGMGjufKQvz3Q5JRMQrTjkjaIxpDYzyfBUD7+K0m1BTqVqSk1dE\nu9Q4UmJ9uOWyYCF89GvIz3WW3l06GVI7++71RaoqNMKp9tn9Rij5ARq21/JPN4XHwKjxMG4QjL8W\nbp2lKsJSZ7VsGMOUO/ty+5sLuPfdRawr2su9Q1oTFKS/QSJSd5xu+mcVzuzf5dbaftbaZ4Fy34RV\n9+0vLWPBhhLftY3YvwM+vA/Gng87N8Dw/8DPPlUSKP4vLAoadVAS6A8SMpzWH7sLYcINUFbqdkQi\nXpMYHcYbt/RiRHY6z8xaw93jF3KgVG+DRKTuOF0ieBVQCPzPGDPOGDMYp1iM1IL563ZwuNx6v21E\nRYVTWfHZHrDgdWdv1V250HWU3liLSPVlnOPsJ94wFz66r3rtPkQCTFhIEE9c3ZnfX9qW6csKuXbs\nPLbtPuh2WCIiteKUiaC1doq1diTQFvgfcC/Q0BjzgjHmQl8FWFfNzisiPCSI7KaJ3nuR/AXw0mD4\n4JeQ0hZ+PhsueRwiE7z3miJS93W+Bgb8Bha+CfOedzsaEa8yxjBmQAvG3pDNmm17Gfb8XJbl73I7\nLBGRs3bGyiDW2n3W2rettVcA6cBC4Hdej6yOm5NXTM9mSUSEBtf+k+/bDtPugXGDYXc+XDUObp4O\njTvW/muJSP103u+h/TD47CH4/hO3oxHxugvaN2Li7b0BuOY/8/h0+RaXIxIROTvVKhFprS2x1o61\n1g72VkD1QeGuA+Rt21v7bSMqyuHbl+G5Hs4n9b1/4SwD7TxCy0BFpHYFBTl7jVO7wHu3wNblbkck\n4nUdmsQz9Rd9ad0ohtvfXMB/vlyL1fJoEQlQ6hXggpy8YsApUV1rNn0L486Hj+6HRh3hjrlOif2I\nuNp7DRGRysKinEqiYTHw9kjYW+R2RCJe1zAugnd/3ptLO6Xy+Mer+O2kJZSWVbgdlohItSkRdMGc\nvGKSY8Jp2zj27J9sXzFM/QW8PAT2boOrX4abPoCG7c7+uUVEziSuiZMM7tsG7/7U6fcoUsdFhAbz\n7Mhu3DO4FRMXbOb6l+ezY5+q6IpIYFEi6GMVFZY5a4oZ0CoZczbLNSvK4Ztx8Gx3WPwO9LkH7voW\nOv1Ey0BFxLfSusOV/4FN853iVFoqJ/VAUJDh/gta89S1XVm0aSdX/nsua7btdTssEZEqUyLoYysK\nd7NjX+nZLQvdOB/GDoTpv4bUrnDHV3DhXyG8FmYYRURqosOVcP4fYPF4mPuU29GI+MzwbmmMv+1c\n9h0q48p/z2WOZ/uHiIi/UyLoY0f3B7asQSK4dxtMvgNeudBpEH/Na3DjVEhpU7tBiojUxIDfQMer\n4fM/w8oP3Y5GxGd6ZCUy5Rd9aRIfyU2vfsObX29wOyQRkTNSIuhjOXlFtG0cS8O4iKo/qLwMvv4P\nPJsNSydCv/vgF984n8BrGaiI+AtjnGbzad3h/dugcLHbEYn4THpiFO/d2YeBrVN4aMoy/jRtOWXl\nKiIjIv5LiaAPHSgtJ3d9Cf2rsyzUWnj/Vvjkd5DeA+6cB0P+BOEx3gpTRKTmQiNh5NsQmQjjR8Ge\nrW5H5JeMMRcbY743xqwxxjxwkvtHG2OKjDGLPF+3eo53NcbMM8YsN8YsMcZc6/vo5VRiwkMYd2M2\nt/Rrxmtfrefm176lYOcBt8MSETkpJYI+NP+H7ZSWV9C/Ov0D5z0HyyfDoD/C9e9DcivvBSgiUhti\nGzuVRA+UwDvXwWG9Ea7MGBMMPA9cArQHRhlj2p/k1HettV09Xy95ju0HbrTWdgAuBp4yxiT4JHCp\nkuAgwx8vb8/jV3Uid30JFz45m7fmb6CiQkWURMS/KBH0oZy8YsJCgujZLKlqD/ghB2Y8Au2ugP6/\n0jJQEQkcqV3gqrGQnwtT71Il0eP1BNZYa9dZa0uBd4BhVXmgtXa1tTbP830BsA2oxqeL4isje2by\n6b0D6Jwezx8mL+O6l75mffE+t8MSETlKiaAP5eQV0bNpEhGhwWc+eXcBTLoZkprDsH8rCRSRwNPu\nChj8CCybBLP/n9vR+JM0YFOl25s9x050tWf55yRjTMaJdxpjegJhwFrvhClnK7NBFG/d2ovHr+rE\n8vzdXPz0bMbNXke5ZgdFxA8oEfSRrbsPsnrr3qrtDywrhYmjoXQ/XPsmRMR5PT4REa/odx90Hgn/\n+5uzzF2q6gOgqbW2MzADeL3yncaYVOAN4GZr7Ukrkhhjxhhjco0xuUVFRV4PWE7OGMPInpnMuH8g\n/Vom87fpK7nqha9YvXWP26GJSD2nRNBHjrSNqNL+wM8echozD3sOGrb1cmQiIl5kDAx9BjJ6Oe1v\n8r9zOyJ/kA9UnuFL9xw7ylq73Vp7yHPzJaDHkfuMMXHAR8AfrLVfn+pFrLVjrbXZ1trslBStHnVb\n4/gIxt2YzTOjurFpx34ueyaHpz/Po7RMlUVFxB1eTQSrUBXtyUoV0VYbY3Z6jte5qmg5eUUkx4TR\ntvEZmr4vmQDfvAi974KOV/kmOBERbwoJh2vfguhkp3jM7gK3I3Lbt0ArY0wzY0wYMBKYVvkEz4zf\nEUOBlZ7jYcBk4L/W2kk+ildqiTGGoV2aMOO+AVzSMZUnP1/N0OfmsGTzTrdDE5F6yGuJYFWqollr\n7ztSEQ14Fnjfc1edqopWUWGZu6aYfi2TCQo6zV6/Lctg2j2Q1ddpESEiUlfEpMCod5wl7+vnuh2N\nq6y1ZcBdwKc4Cd4Ea+1yY8xfjDFDPafd4/kwdDFwDzDac3wEMAAYXemD1K4+/hHkLDWICeeZUd14\n6cZsSvaXMvz5uTw2fSUHD5e7HZqI1CMhXnzuo1XRAIwxR6qirTjF+aOAR8CpinbkoLW2wBhzpCpa\nQH5ktnLLbor3lp5+WeiBnTDhBoiIh5+8CsGhvgtQRMQXGneEXy6CqCpWTq7DrLXTgeknHHu40vcP\nAg+e5HFvAm96PUDxiSHtG3FOsyQem76SF2ev47MVW3ni6s5Vry4uInIWvLk0tKpV0TDGZAHNgFkn\nuS/gq6Id2R/Y71SFYioqYPLtsHMjjHgdYhv5MDoRER9SEihynPjIUB6/ujNv3dqLsooKRrw4jz9O\nWcbeQ2VuhyYidZy/FIsZCUyy1h63JuJMVdECpSLanLxi2jSKpVFcxClO+Bes/hgu+jtknuvb4ERE\nRMR1fVsm8+m9A/hZ32a8OX8DFz05my9X++97GxEJfN5MBM9YFa2SkcD4ygeqUhUtECqiHSgt55v1\nO07dNmLNTJj1KHS6BnqO8W1wIiIi4jeiwkJ4+Ir2TLq9D5Fhwdz0yjf8asJidu4vdTs0EamDvJkI\nnrEqGoAxpi2QCMyrdKzOVEX7Zv0OSssqTr4sdOdGeO9WaNgOrnhaTeNFRESEHlmJfHh3P+46vyVT\nFuUz5F+z+XhpodthiUgd47VEsIpV0cBJEN+x1tpKx+pMVbQ5eUWEBQfRq1mD4+84fBAm3AgVZU7T\n+LBodwIUERERvxMRGsyvL2rDtLv60igunDve+o473lzAtj0H3Q5NROoIb1YNPWNVNM/tP53kcXWm\nKlpOXjHnNEskMiz4+Ds++R0ULISRb0ODFu4EJyIiIn6tQ5N4pvyiL+Ny1vHU53l8tXY7D1/enqu6\np2G0kkhEzoK/FIupk7btPsiqLXt+3DbiuzdgwWvQ735oe5krsYmIiEhgCA0O4s7zWjL9nv60ahjD\nryYuZvSr35K/84DboYlIAFMi6EVH20a0rLQ/sGARfPQraH4eDHrIlbhEREQk8LRsGMOEn/fmT1e0\n59v1O7jwX1/yxrz1VFTYMz5WRORESgS9aM6aYhpEh9E+Nc45sH+H0zQ+OgWufhmCgk//BCIiIiKV\nBAUZRvdtxqf3DqBbZiJ/nLqckeO+5ofifW6HJiIBRomgl1RUWHLyiunXKpmgIAMV5U6F0D1bYMR/\nIfoU7SREREREziAjKYo3bunJP67uzMrC3Vz81Gxe/HItZeU/arssInJSSgS9ZNWWPRTvPXRsWeiX\nT8DamXDJPyC9h7vBiYiISMAzxjDinAw+v38gA1un8NjHq7jqha9YtWW326GJSABQIuglc9YUATiF\nYr7/xEkEu14PPUa7G5iIiIjUKY3iInjxhh48d1038ksOcPkzc/jXjNWUlml2UEROTYmgl+TkFdO6\nUQyNywtg8hho3Bku+6eaxouIiEitM8ZweecmzLh/IJd3TuWZmXlc/mwOizbtdDs0EfFTSgS94ODh\ncub/sIPzmsfCuzcCBq59A0Ij3Q5NRERE6rCk6DCeGtmNV0Zns/tAGVf9ey5/+2gFB0rL3Q5NRPyM\nEkEv+Hb9DkrLyrm55BnYugyufgkSm7odloiIiNQTg9o24rP7BzCyZybjcn7gkqdn8/W67W6HJSJ+\nRImgF+TkFXNj6CxS10+G8x6EVhe4HZKIiIjUM3ERofz9yk68fVsvLDBy7Nf8YfJS9hw87HZoIuIH\nQtwOoC7aumIO/xf8OrS6EAb8xu1wREREpB7r0yKZT345gP/77HtemfsDM1duY1C7hrRtHEvbxnG0\naRxLfGSo22GKiI8pEaxlRVs38bs9f2dfZCPirxoLQZp0FREREXdFhgXz0OXtuaxzKv/87Hs+XFzA\n2/PLjt6fGh9BG09i2LZxLG0ax9IiJYawEL2PEamrlAjWpvIymPgzktjD5kveJD4y0e2IRERERI7q\nlpnIW7eei7WWwl0H+X7LHlZt2cP3W3azasse5q4p5nC5BSAkyNA8JfrorOGRBDEtIRKjKugiAU+J\nYG2a9VdSir/h4aC7+FOnPm5HIyIiInJSxhiaJETSJCGS89s2PHr8cHkF64r2sWrL7qNJ4oINJUxb\nXHD0nNjwENp4kkInOdTyUpFApESwtqyYBnOfYlLQhZS0+glBQfqkTERERAJLaHDQ0SSvst0HD7Pa\nkxgeSRKnLS7grUrLS5t4lpe2aRxHu1TnOZona3mpiL9SIlgbivNgyp0caNiV32/8KY+2SnY7IhER\nEZFaExcRSnbTJLKbJh09Vnl56UpPcvj9lj3MOWF5aYuUGGf2MPXYDGKT+AgtLxVxmRLBs3VoL7x7\nPYSEMbnlY5RuLKG/EkERERGp4061vLS0rIIfip3lpas8yeGPlpdGhBzdc9imUoGauAgtLxXxFSWC\nZ8NamHY3FK+GGybzyRchtGwYQ2p8pNuRiYiIiLgiLOTY8tJhlY7vOnCY1VuPFaf5fssepi4sYM+h\njUfP6ZQWzy39mnFZ51RCg7WkVMSblAieja9fgOXvw5A/cTCjP/PXfcZ1vTLdjkpERETE78RHhnJO\n0yTOOWF5acGug3y/ZTcrC/cweWE+9767iCc+WcXoPk0Z1StTs4QiXqJEsKY2fAUz/ghtL4e+95K7\nZjuHyiq0LFRERESkiowxpCVEkpYQyaC2jbhjYAu+WL2NcbN/4LGPV/HsrDVce04GN/dtSnpilNvh\nitQpSgRrYs8WmDgaEpvC8H+DMeSsKSI02NCrWQO3oxMREREJSEFBhkFtGzGobSOW5e9iXM46Xvtq\nPa99tZ5LO6VyW/9mdE5PcDtMkTpBi6+rq/ywkwQe2gPXvgkR8QDkrC6mR1Yi0eHKrUVE5PSMMRcb\nY743xqwxxjxwkvtHG2OKjDGLPF+3VrrvJmNMnufrJt9GLuI7HdPieXpkN3J+ez639GvGF6u2MfS5\nuYx4cR6fr9hKRYV1O0SRgKZEsLpmPAwb58HQZ6FhOwCK9hxiReFu+rdKcTk4ERHxd8aYYOB54BKg\nPTDKGNP+JKe+a63t6vl6yfPYJOARoBfQE3jEGJPoo9BFXNEkIZLfX9qOrx4cxEOXtSO/5AC3/jeX\nIU9+yVvzN3DwcLnbIYoEJCWC1bF0Enz9b+h1B3T6ydHDX60tBtD+QBERqYqewBpr7TprbSnwDhxX\nXPF0LgJmWGt3WGtLgBnAxV6KU8SvxEaEcmv/5nzxm/N4emRXosNC+MPkZfR5fBZPzlhN8d5Dboco\nElCUCFbVtpVOq4iMc+HCvx531+zVxSRGhdKhSbxLwYmISABJAzZVur3Zc+xEVxtjlhhjJhljMqr5\nWJE6KzQ4iGFd05h2V1/eGXMu3TMTeHpmHn0en8WD7y9hzba9bocoEhC0oa0qDu5ymsaHx8KI1yH4\nWBljay05eUX0bZlMcJBxMUgREalDPgDGW2sPGWN+DrwODKrOExhjxgBjADIz1dpI6h5jDOc2b8C5\nzRuwZtteXp7zA+9/t5nx32xicNuG3DagOb2aJWGM3p+JnIxmBM/EWphyJ+z4Aa55DWIbH3d33ra9\nbNtzSMtCRUSkqvKBjEq30z3HjrLWbrfWHlnn9hLQo6qPrfQcY6212dba7JQU7WGXuq1lwxgeu6oT\nXz0wiHuHtGLhpp2MHPs1Q5+by9RF+Rwur3A7RBG/o0TwTOY+Bas+hAsfhaw+P7p79uoiAPqpUIyI\niFTNt0ArY0wzY0wYMBKYVvkEY0xqpZtDgZWe7z8FLjTGJHqKxFzoOSYiQIOYcO4d0pqvHhjE36/s\nxL5DZfzynUUM/Mf/eClnHXsOHnY7RBG/oaWhp7PuC5j5F+hwFZx7x0lPyckrpkVKNGkJkb6NTURE\nApK1tswYcxdOAhcMvGKtXW6M+QuQa62dBtxjjBkKlAE7gNGex+4wxvwVJ5kE+Iu1dofPfwgRPxcR\nGsx1vTIZeU4Gs1ZtY1zOOh79aCVPf57HyJ4Z3Ny3GU303k3qOWNt3ejBkp2dbXNzc2vvCXdthhcH\nQnQy3DoTwmN+dMqhsnK6/PkzRp6TyZ+Gdqi91xYRkdMyxiyw1ma7HUegqPUxUiQALdm8k3E5PzB9\naSEAl3dO5bb+zemYpmJ/UndUZ3zUjODJlB2CCTc5/1775kmTQIAF60s4eLhC+wNFRERE/Fzn9ASe\nHdWN313chtfmruedbzcxdVEB5zZPYsyA5pzXuiFBKvwn9Yj2CJ7MJw9Cfi5c+QIktzrlabPzigkN\ndipWiYiIiIj/S0+M4qHL2/PVg4P4/aVt2bB9Pz97LZcLnvyS8d9sVIN6qTeUCJ5o0duQ+zL0vRfa\nXXHaU+esKaJbZiLR4ZpYFREREQkkcRGhjBnQgtm/PZ+nR3YlIjSYB99fSr8nZvH053ns2Ffqdogi\nXqVEsLLCJfDhfdBsAAz642lP3b73EMvydzNAy0JFREREAtaRBvUf3t2Pt2/rRef0BJ78fDW9H5vJ\nHyYvZV2RGtRL3aSprCMOlDhN46MawNWvQPDpL82cNcUA9FfbCBEREZGAZ4yhT4tk+rRIZs22PbyU\n8wMTF2zm7W82wtywtwAAFVJJREFUMrhtQ37SI4Pz26YQHhLsdqgitUKJ4BEhEdB8IHS/CWLOnNzN\nySsmPjJUlaZERERE6piWDWN5/OrO/OrCNrzx9Qbenr+Bz1duIzYihEs7pjKsWxPObdZAxWUkoCkR\nPCI0EoY+W6VTrbXk5BXTr2UywfoDICIiIlInpcSGc/8FrblnUEvmrt3O1IX5fLikgHdzN9E4LoIr\nuqQyrGsaHZrEYYzeE0pgUSJYA2u27WXL7oNqGyEiIiJSD4QEBzGwdQoDW6dwoLScz1duZeqifF6d\nu55xOT/QsmEMw7s2YWiXNDIbRLkdrkiVKBGsgZw8Z39gPyWCIiIiIvVKZFgwV3RpwhVdmlCyr5Tp\nywqZurCAf362mn9+tprumQkM75bGZZ1SaRAT7na4IqekRLAGcvKKaJ4cTXqiPvERERERqa8So8P4\naa8sftori80l+/lgcSFTF+Xz8NTl/PmDFfRvlczwrmlc0L6R2o2J39FvZDUdKivn63U7GJGd7nYo\nIiIiIuIn0hOjuOO8FtxxXgtWbdnN1EUFTFtUwL3vLiIyNJgL2jdiWNcmDGidQmiwOriJ+5QIVtN3\nG3Zy4HA5/dQ2QkREREROom3jONpeHMdvLmxD7oYSpi7K56OlhUxbXEBiVCiXdU5leNc0umcmqvKo\nuEaJYDXl5BUREmQ4t3mS26GIiIiIiB8LCjL0bJZEz2ZJPHJFB2avLmLq4gImLdjMm19vJC0hkmFd\nmzC8WxqtG8W6Ha7UM0oEqyknr5jumYnERoS6HYqIiIiIBIiwkCCGtG/EkPaN2HuojM+Wb2HqogJe\nnL2Of3+xlraNYxneLY2hXZrQJCHS7XClHlAiWA079pWyrGAX9w1p7XYoIiIiIhKgYsJDuKp7Old1\nT6dozyE+WlLAlEUFPP7xKh7/eBU9myUxvGsal3ZqTEJUmNvhSh2lRLAa5q4pxlrUP1BEREREakVK\nbDij+zZjdN9mbNi+j6mLCpiyKJ/fT17KI9OWMbB1Q4Z3a8Lgto2IDAt2O1ypQ5QIVkNOXhFxESF0\nTk9wOxQRERERqWOyGkRzz+BW3D2oJcsLdjNlYT7TFhfw+cqtRIcFc1HHxgzvmkafFg0IUeVROUtK\nBKvIWktOXjH9WiUTrOpOIiIiIuIlxhg6psXTMS2eBy9tx/x125myKJ+Pl27h/e/ySY4J5/LOqQzv\nlkaX9HiM0XtTqT4lglW0tmgfhbsOcndLtY0QEREREd8IDjL0aZlMn5bJ/GVYR774fhtTFhbw9vyN\nvPbVepJjwumRlUCPrER6ZCXRMS2O8BAtIZUzUyJYRTl5RYD2B4qIiIiIOyJCg7m4YyoXd0xl14HD\nfLp8C1+v3U7uhhI+Xb4VcKqTdk6Lp0fTRHpkJtIjK5EGMeEuRy7+yKuJoDHmYuBpIBh4yVr7+An3\nPwmc77kZBTS01iZ47rsJeMhz36PW2te9GeuZ5OQV0yw5moykKDfDEBEREREhPjKUEdkZjMjOAGDb\nnoN8t6GEBRtKyN1QwitzfuDF8nUANE+OpntWItlZiWQ3TaR5cowa2Yv3EkFjTDDwPHABsBn41hgz\nzVq74sg51tr7Kp1/N9DN830S8AiQDVhggeexJd6K93RKyyr4et12ru6e7sbLi4iIiIicVsPYiKOz\nhQAHD5ezNH8Xueud5HDmyq1MWrAZgISoULp7Zgt7ZCXSJT1BFUnrIW/OCPYE1lhr1wEYY94BhgEr\nTnH+KJzkD+AiYIa1dofnsTOAi4HxXoz3lL7bWML+0nItCxURERGRgBARGsw5TZM4p2kS4BQ+XFe8\njwUbSliwvoTcDTuYtWobACFBhg5N4uiRlUR2Uyc5bBQX4Wb44gPeTATTgE2Vbm8Gep3sRGNMFtAM\nmHWax6Z5IcYqyckrIjjI0LtFA7dCEBERERGpMWMMLVJiaJESc3Q5acm+Ur7beGw56VvzN/DK3B8A\nSE+MJDsr8WgRmjaNY1U5v47xl2IxI4FJ1try6jzIGDMGGAOQmZnpjbgAmJNXTLeMBGIjQr32GiIi\nIiIivpQYHcbgdo0Y3K4R4GyHWlG4m9z1O1iwoYS5a7czZVEBADHhIXTLTDi6nLRbZiIx4f6SSkhN\nePO/Xj6QUel2uufYyYwEfnHCY8874bFfnPgga+1YYCxAdna2rXmop1ayr5Ql+bu4d3Brbzy9iIiI\niIhfCAsJomtGAl0zEri1v7OcdHPJAXI3OIlh7voSnp6Zh7UQZKBt4zh6eArQ9MhKJC0hUj0NA4g3\nE8FvgVbGmGY4id1I4LoTTzLGtAUSgXmVDn8K/N0Yk+i5fSHwoBdjPaW5a4uxFvq31v5AEREREak/\njDFkJEWRkRTFld2cool7Dh5m4cad5G4o4bsNJbz/3Wbe+HoDAI3iwsnOSnIK0GQk0Cw5msSoUCWH\nfspriaC1tswYcxdOUhcMvGKtXW6M+QuQa62d5jl1JPCOtdZWeuwOY8xfcZJJgL8cKRzja3PyiomN\nCKFzWrwbLy8iInXUmVosVTrvamAScI61NtcYEwq8BHTHGcf/a619zEdhi0g9FxsRyoDWKQxonQJA\nWXkFq7bs4buNJUcrlH60tPDY+eEhZDaIIqtBFJlJ0WQ1iCIrKYrMBlGkxkdq36GLvLqw11o7HZh+\nwrGHT7j9p1M89hXgFa8FVwXWWnLyiunbIpmQ4CA3QxERkTqkKi2WPOfFAr8E5lc6fA0Qbq3tZIyJ\nAlYYY8Zba9f7JnoRkWNCgoPomBZPx7R4buzdFIDCXQdYlr+bDdv3sXHHfjZs38/Kwj3MWLGVw+XH\ndnOFBQeRnhjpJIpJUWQ2iCYryUkaM5KiiAhVSwtv0g7P01hXvI/8nQe48/wWbociIiJ1S1VbLP0V\neAL4TaVjFog2xoQAkUApsNvrEYuIVFFqfCSp8ZE/Ol5eYSnYeYCNO/YfTRA37tjHhu37WbC+hD2H\nyo47v3FcxNEkMavB8YliQlSYr36cOkuJ4GnMySsGoH/LFJcjERGROuaMLZaMMd2BDGvtR8aYyong\nJJyksRCIAu5za/uEiEh1BAcd23PY94T7rLWU7D983CzikUTxy9VFbNtz6Ljz4yJCyGoQfXyi6Fl6\n2jgugiAtOT0jJYKnkZNX5Pn0IcrtUEREpB4xxgQB/wJGn+TunkA50ASn2FqOMebzI7OLlZ7DJy2W\nRERqgzGGpOgwkqLD6JaZ+KP7D5SWexLESonijv0sz9/Fp8u2UFZRaclpSBAZiZFOouhJEo8kihlJ\nkYSHaMkpKBE8pcPlFcxbu50ru7vWx15EROquM7VYigU6Al94qu01BqYZY4biVOD+xFp7GNhmjJkL\nZAPHJYK+aLEkIuIrkWHBtGkcS5vGsT+6r6y8gsJdBz3J4T42bj+WKM5ft519pcdalRsD5zZrwIhz\n0rm4QyqRYfU3KVQieAoLN+5kX2k5/bQsVEREat9pWyxZa3cBR/sWGWO+AH7tqRo6GBgEvGGMiQbO\nBZ7yYewiIn4lJDjo6JLTfhzf8s1ay/Z9pUeXmeZt3cuHSwq5793FPBy+nMu7NGFEdjpdMxLqXZsL\nJYKnkJNXRHCQoXeLBm6HIiIidUw1WiydzPPAq8aY5YABXrXWLvF+1CIigccYQ3JMOMkx4fTIcpac\n/vrCNnyzfgcTcjcxeeFmxn+zkVYNY7gmO50ru6WTEhvuctS+YSq17wto2dnZNjc3t9aeb9jzcwkJ\nMrx3R59ae04REakdxpgF1tpst+MIFLU9RoqI1BV7Dh7mwyWFTMjdxMKNOwkJMpzftiEjsjM4r00K\noQHWQq4646NmBE9i5/5Slm7eyd2DWrkdioiIiIiIeElsRCijemYyqmcma7btYWLuZt77Lp8ZK7aS\nHBPOVd3TGJGdTsuGP96bGOiUCJ7EV2u3U2FhQOvkM58sIiIiIiIBr2XDWB68tB2/vqgNX3xfxMTc\nTbwy5wfGzl5Ht8wERmRncHnnVGIjQt0OtVYoETyJnLwiYsND6JKe4HYoIiIiIiLiQ6HBQVzQvhEX\ntG9E0Z5DTFmYz4TcTTz4/lL+/MFyLu2YyjXZGfRqlhTQ/QqVCJ7AWsvs1cX0btGAkABbEywiIiIi\nIrUnJTac2wY059b+zVi0aScTF2zmg0UFvL8wn8ykKK7pkc7VPdJpkhDpdqjVpkTwBOu37yd/5wFu\nP6+F26GIiIiIiIgfMMbQLTORbpmJ/PGy9nyyvJAJ327m/2as5l+fr6Zfy2RGZGdwQftGRIQGRm9C\nJYInyMkrAmBAK+0PFBERERGR40WGBXNlN6fVxKYd+5m4YDOTcjdx9/iFxEeGMrxrE67JzqBjWrzb\noZ6WEsET5OQVk5kURVaDaLdDERERERERP5aRFMX9F7Tml4Nb8dXaYibkbmb8t5t4fd4G2qXGMSI7\nneFd00iMDnM71B9RIljJ4fIK5q3dztCuTdwORUREREREAkRwkKF/qxT6t0ph1/7DTFucz4Tczfz5\ngxU8Nn0VQ9o35JrsDAa0SiHYTwrMKBGsZNGmnew9VKZloSIiIiIiUiPxUaHc0LspN/RuysrC3UzM\n3czkhZuZvnQLjeMiuLpHGtf0yKBpsrsrEJUIVpKTV0yQgd4tlAiKiIiIiMjZaZcax8NXtOeBS9oy\nc+VWJuRu4oUv1vL8/9bSs2kS12Snc2mnVKLDfZ+WKRGsJCeviC4ZCcRH1o0mkSIiIiIi4r6wkCAu\n6ZTKJZ1S2bLrIO8v3MzE3M38ZtIS/jRtOZd3bsI12en0yErEGN8sHVWjPI9dBw6zeNNO+rdKcTsU\nERERERGpoxrHR3DneS2Z9auBTLy9N5d1TuWDJQWMeWMBh8utz+LQjKBHXEQIn903kOjwwOj7ISIi\nIiIigcsYwzlNkzinaRKPXNGBvG17CQvx3TydEkEPYwwtG8a4HYaIiIiIiNQz0eEhdM1I8Olrammo\niIiIiIhIPaNEUEREREREpJ5RIigiIiIiIlLPKBEUERERERGpZ5QIioiIiIiI1DNKBEVEREREROoZ\nJYIiIiIiIiL1jBJBERERERGRekaJoIiIiIiISD2jRFBERERERKSeMdZat2OoFcaYImBDLTxVPLCr\nFp6ntmUCG90O4hR0zarHX68X6JrVhK5Z9dXGNcuy1qbURjD1QS2NkXX9d8obdM2qz1+vmb9eL9A1\nq4m6fM2qPD7WmUSwthhjxlprx7gdx4mMMUX++qZH16x6/PV6ga5ZTeiaVZ+/XjM5Pf1OVZ+uWfX5\n6zXz1+sFumY1oWvm0NLQH/vA7QBOYafbAZyGrln1+Ov1Al2zmtA1qz5/vWZyevqdqj5ds+rz12vm\nr9cLdM1qQtcMJYI/Yq31118Mf5y+BnTNqsuPrxfomtWErln1+eU1k9PT71T16ZpVnx9fM7+8XqBr\nVhO6Zg4lgoFjrNsBBCBds+rTNas+XbPq0zWT2qbfqerTNaseXa/q0zWrPp9eM+0RFBERERERqWc0\nIygiIiIiIlLPKBH0c8aYDGPM/4wxK4wxy40xv3Q7pkBgjAk2xiw0xnzodiyBwBiTYIyZZIxZZYxZ\naYzp7XZM/s4Yc5/n/8llxpjxxpgIt2PyN8aYV4wx24wxyyodSzLGzDDG5Hn+TXQzRglcGh9rTmNk\n9WiMrD6NkWfmD2OkEkH/Vwb8ylrbHjgX+IUxpr3LMQWCXwIr3Q4igDwNfGKtbQt0QdfutIwxacA9\nQLa1tiMQDIx0Nyq/9Bpw8QnHHgBmWmtbATM9t0VqQuNjzWmMrB6NkdWgMbLKXsPlMVKJoJ+z1hZa\na7/zfL8H549PmrtR+TdjTDpwGfCS27EEAmNMPDAAeBnAWltqrfXnks/+IgSINMaEAFFAgcvx+B1r\n7WxgxwmHhwGve75/HRju06CkztD4WDMaI6tHY2SNaYw8A38YI5UIBhBjTFOgGzDf3Uj83lPAb4EK\ntwMJEM2AIuBVz1Khl4wx0W4H5c+stfnAP4GNQCGwy1r7mbtRBYxG1tpCz/dbgEZuBiN1g8bHatEY\nWT0aI6tJY+RZ8ekYqUQwQBhjYoD3gHuttbvdjsdfGWMuB7ZZaxe4HUsACQG6Ay9Ya7sB+9ByvdPy\nrNkfhvMGoQkQbYy53t2oAo91ylardLWcFY2PVacxskY0RlaTxsja4YsxUolgADDGhOIMcm9Za993\nOx4/1xcYaoxZD7wDDDLGvOluSH5vM7DZWnvkk/RJOIOenNoQ4AdrbZG19jDwPtDH5ZgCxVZjTCqA\n599tLscjAUzjY7VpjKw+jZHVpzGy5nw6RioR9HPGGIOzLn2ltfZfbsfj76y1D1pr0621TXE2Js+y\n1upTqNOw1m4BNhlj2ngODQZWuBhSINgInGuMifL8PzoYFQ+oqmnATZ7vbwKmuhiLBDCNj9WnMbL6\nNEbWiMbImvPpGKlE0P/1BW7A+dRukefrUreDkjrnbuAtY8wSoCvwd5fj8WueT4YnAd8BS3H+lo51\nNSg/ZIwZD8wD2hhjNhtjbgEeBy4wxuThfGr8uJsxSkDT+Ci+ojGyGjRGVo0/jJHGWX4qIiIiIiIi\n9YVmBEVEREREROoZJYIiIiIiIiL1jBJBERERERGRekaJoIiIiIiISD2jRFBERERERKSeUSIo4geM\nMeWVyp8vMsY8UIvP3dQYs6y2nk9ERMSXNEaKeEeI2wGICAAHrLVd3Q5CRETED2mMFPECzQiK+DFj\nzHpjzD+MMUuNMd8YY1p6jjc1xswyxiwxxsw0xmR6jjcyxkw2xiz2fPXxPFWwMWacMWa5MeYzY0yk\naz+UiIhILdAYKXJ2lAiK+IfIE5a9XFvpvl3W2k7Ac8BTnmPPAq9bazsDbwHPeI4/A3xpre0CdAeW\ne463Ap631nYAdgJXe/nnERERqS0aI0W8wFhr3Y5BpN4zxuy11sac5Ph6YJC1dp0xJhTYYq1tYIwp\nBlKttYc9xwuttcnGmCIg3Vp7qNJzNAVmWGtbeW7/Dgi11j7q/Z9MRETk7GiMFPEOzQiK+D97iu+r\n41Cl78vR/mAREakbNEaK1JASQRH/d22lf+d5vv8KGOn5/qdAjuf7mcAdAMaYYGNMvK+CFBERcYHG\nSJEa0iceIv4h0hizqNLtT6y1R8pjJxpjluB8YjnKc+xu4FVjzG+AIuBmz/FfAmONMbfgfKp5B1Do\n9ehFRES8R2OkiBdoj6CIH/Psf8i21ha7HYuIiIg/0Rgpcna0NFRERERERKSe0YygiIiIiIhIPaMZ\nQRERERERkXpGiaCIiIiIiEg9o0RQRERERESknlEiKCIiIiIiUs8oERQREREREalnlAiKiIiIiIjU\nM/8f7CRLsmjE22kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-HUrxCm3c3x"
   },
   "source": [
    "**Accuracy seems to grow fast in the first 4 epochs, then it does not improve regularly. With too many epochs we would be in overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "asEwjZuY3c3x",
    "outputId": "a0712f6c-1c96-4f53-85c1-f4e413793e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3609/3609 [==============================] - 0s 46us/step\n",
      "loss function and train accuracy are:  [0.4767459826180069, 0.7722360762938069]\n",
      "903/903 [==============================] - 0s 43us/step\n",
      "loss function and validation accuracy are:  [0.5544142845857183, 0.7209302326241469]\n",
      "1504/1504 [==============================] - 0s 45us/step\n",
      "loss function and test accuracy are:  [0.6047350542342409, 0.7240691489361702]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss function and train accuracy are: \", history.model.evaluate(train_setNN, y_trainNN))\n",
    "print(\"loss function and validation accuracy are: \", history.model.evaluate(val_setNN, y_valNN))\n",
    "print(\"loss function and test accuracy are: \", history.model.evaluate(scaled_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPf3YSF03c3z"
   },
   "source": [
    "**I obtained 74% of accuracy on the test set with this simple Dense Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIRKJEN53c30"
   },
   "source": [
    "# Basic Gridsearch for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F65pkLwK3c31"
   },
   "outputs": [],
   "source": [
    "\"\"\"lr=[1e-2, 1e-3, 1e-4]\n",
    "decay=[1e-6,1e-9,0]\"\"\"\n",
    "\n",
    "def binary_classifier(nl1=1, nl2=1,  nl3=1, \n",
    "                 nn1=1000, nn2=500, nn3 = 200, lr=0.01, decay=0., l1=0.01, l2=0.01, dropout=0.2):\n",
    "    # create model\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999,  decay=decay)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(nn1, input_dim=len(X_train_scaled.columns), activation='relu', kernel_initializer=keras.initializers.he_normal(seed=1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(nn2, input_dim=len(X_train_scaled.columns), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(nn3, input_dim=len(X_train_scaled.columns), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))   \n",
    "    #anche softmax va bene, ma sigmoid dà molta più accuratezza. Penso perchè sigmoid è caso binario, softmax più classi\n",
    "    # NB: Se volevo usare softmax dovevo mettere 2 layer Dense nell'output invece di 1\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])  #se piu categorie metto categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIV3ShWV3c3-"
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=binary_classifier, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nh8JHjkQ3c3_",
    "outputId": "f5cff3c6-ab56-47a9-f656-5e8ee0c2257b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 40.0943 - acc: 0.4960 - val_loss: 13.3527 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 12.2520 - acc: 0.4997 - val_loss: 11.2255 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 11.8806 - acc: 0.4977 - val_loss: 11.8980 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 123us/step - loss: 11.9358 - acc: 0.5066 - val_loss: 12.4889 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0399 - acc: 0.5030 - val_loss: 12.1495 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 126us/step - loss: 12.0569 - acc: 0.5090 - val_loss: 12.1188 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0570 - acc: 0.5050 - val_loss: 12.0394 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0573 - acc: 0.5100 - val_loss: 11.9872 - val_acc: 0.4950\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 12.0753 - acc: 0.5100 - val_loss: 11.4578 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0646 - acc: 0.5023 - val_loss: 12.3845 - val_acc: 0.4950\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0566 - acc: 0.5100 - val_loss: 12.4316 - val_acc: 0.4950\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 123us/step - loss: 12.0788 - acc: 0.5100 - val_loss: 11.8714 - val_acc: 0.4950\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.0791 - acc: 0.5100 - val_loss: 11.7690 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.0605 - acc: 0.5100 - val_loss: 12.2882 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0651 - acc: 0.4987 - val_loss: 12.2078 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 51us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 40.3819 - acc: 0.4917 - val_loss: 13.2898 - val_acc: 0.5050\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 126us/step - loss: 12.2029 - acc: 0.5103 - val_loss: 11.2501 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 123us/step - loss: 11.8177 - acc: 0.5013 - val_loss: 11.7912 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 11.8734 - acc: 0.4970 - val_loss: 12.3823 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 11.9851 - acc: 0.5103 - val_loss: 12.1033 - val_acc: 0.5050\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 123us/step - loss: 12.0040 - acc: 0.4917 - val_loss: 12.0820 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0070 - acc: 0.5093 - val_loss: 12.0141 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.0068 - acc: 0.5093 - val_loss: 11.9104 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.0228 - acc: 0.5096 - val_loss: 11.4273 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0137 - acc: 0.5027 - val_loss: 12.2738 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 12.0056 - acc: 0.5053 - val_loss: 12.3736 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0270 - acc: 0.5096 - val_loss: 11.8907 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.0282 - acc: 0.5096 - val_loss: 11.7683 - val_acc: 0.5050\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0098 - acc: 0.5096 - val_loss: 12.1921 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0142 - acc: 0.5096 - val_loss: 12.1022 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 50us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 40.0804 - acc: 0.5096 - val_loss: 13.2895 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.2050 - acc: 0.5023 - val_loss: 11.2112 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.8246 - acc: 0.5053 - val_loss: 11.8192 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 11.8708 - acc: 0.4920 - val_loss: 12.3999 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9712 - acc: 0.4904 - val_loss: 12.0644 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9883 - acc: 0.4987 - val_loss: 12.0521 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 11.9907 - acc: 0.4914 - val_loss: 11.9782 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.9923 - acc: 0.5023 - val_loss: 11.9103 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 127us/step - loss: 12.0081 - acc: 0.4993 - val_loss: 11.4097 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 11.9989 - acc: 0.4850 - val_loss: 12.2913 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 123us/step - loss: 11.9917 - acc: 0.4960 - val_loss: 12.3527 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 126us/step - loss: 12.0128 - acc: 0.4894 - val_loss: 11.8365 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0137 - acc: 0.5023 - val_loss: 11.7354 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.9954 - acc: 0.4844 - val_loss: 12.1900 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 125us/step - loss: 11.9999 - acc: 0.4977 - val_loss: 12.1305 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 49us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 29.0281 - acc: 0.4877 - val_loss: 12.1307 - val_acc: 0.5050\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.8390 - acc: 0.5073 - val_loss: 12.0026 - val_acc: 0.4950\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9166 - acc: 0.4990 - val_loss: 11.7798 - val_acc: 0.4950\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9529 - acc: 0.4960 - val_loss: 12.0427 - val_acc: 0.4950\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.9698 - acc: 0.5010 - val_loss: 11.9711 - val_acc: 0.4950\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9683 - acc: 0.5106 - val_loss: 11.9757 - val_acc: 0.4950\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9728 - acc: 0.5027 - val_loss: 12.1224 - val_acc: 0.4950\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9775 - acc: 0.4934 - val_loss: 11.7364 - val_acc: 0.4950\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9760 - acc: 0.4987 - val_loss: 11.9866 - val_acc: 0.4950\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9829 - acc: 0.5100 - val_loss: 11.7361 - val_acc: 0.4950\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9741 - acc: 0.5100 - val_loss: 12.0554 - val_acc: 0.4950\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.9810 - acc: 0.5100 - val_loss: 11.9921 - val_acc: 0.4950\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9747 - acc: 0.5100 - val_loss: 12.0498 - val_acc: 0.4950\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 1s 194us/step - loss: 11.9812 - acc: 0.5100 - val_loss: 12.3982 - val_acc: 0.4950\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9841 - acc: 0.5100 - val_loss: 11.6079 - val_acc: 0.4950\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9778 - acc: 0.5100 - val_loss: 12.0693 - val_acc: 0.4950\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9845 - acc: 0.5093 - val_loss: 11.6141 - val_acc: 0.4950\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9719 - acc: 0.5100 - val_loss: 12.1958 - val_acc: 0.4950\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9822 - acc: 0.5100 - val_loss: 12.3139 - val_acc: 0.4950\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 1s 191us/step - loss: 11.9777 - acc: 0.5027 - val_loss: 12.1175 - val_acc: 0.4950\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.9840 - acc: 0.4980 - val_loss: 11.9833 - val_acc: 0.4950\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9854 - acc: 0.5100 - val_loss: 11.3322 - val_acc: 0.4950\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9746 - acc: 0.5100 - val_loss: 12.1626 - val_acc: 0.4950\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9842 - acc: 0.5100 - val_loss: 11.9919 - val_acc: 0.4950\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9725 - acc: 0.5100 - val_loss: 12.2389 - val_acc: 0.4950\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9837 - acc: 0.5020 - val_loss: 12.1803 - val_acc: 0.4950\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 1s 195us/step - loss: 11.9831 - acc: 0.5100 - val_loss: 11.6158 - val_acc: 0.4950\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9812 - acc: 0.5100 - val_loss: 12.0160 - val_acc: 0.4950\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.9850 - acc: 0.5100 - val_loss: 11.5247 - val_acc: 0.4950\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9741 - acc: 0.5100 - val_loss: 12.3562 - val_acc: 0.4950\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9826 - acc: 0.4967 - val_loss: 12.0166 - val_acc: 0.4950\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9774 - acc: 0.5100 - val_loss: 12.0631 - val_acc: 0.4950\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9842 - acc: 0.5100 - val_loss: 11.8914 - val_acc: 0.4950\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 1s 194us/step - loss: 11.9831 - acc: 0.5100 - val_loss: 11.7919 - val_acc: 0.4950\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 1s 193us/step - loss: 11.9817 - acc: 0.5100 - val_loss: 12.2484 - val_acc: 0.4950\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9807 - acc: 0.5047 - val_loss: 11.6850 - val_acc: 0.4950\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9763 - acc: 0.5100 - val_loss: 12.1831 - val_acc: 0.4950\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 1s 194us/step - loss: 11.9815 - acc: 0.5100 - val_loss: 12.1068 - val_acc: 0.4950\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 1s 192us/step - loss: 11.9801 - acc: 0.4953 - val_loss: 12.0703 - val_acc: 0.4950\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9868 - acc: 0.5100 - val_loss: 12.0506 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 80us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 29.1432 - acc: 0.5096 - val_loss: 12.1314 - val_acc: 0.4950\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 11.8436 - acc: 0.4987 - val_loss: 12.0176 - val_acc: 0.5050\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.9361 - acc: 0.5083 - val_loss: 11.7880 - val_acc: 0.4950\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9791 - acc: 0.5003 - val_loss: 12.0676 - val_acc: 0.5050\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9962 - acc: 0.5027 - val_loss: 11.9943 - val_acc: 0.5050\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 11.9938 - acc: 0.5143 - val_loss: 12.0026 - val_acc: 0.5050\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9977 - acc: 0.5003 - val_loss: 12.1715 - val_acc: 0.5050\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0016 - acc: 0.5096 - val_loss: 11.7501 - val_acc: 0.5050\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0015 - acc: 0.4993 - val_loss: 12.0022 - val_acc: 0.5050\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0079 - acc: 0.5083 - val_loss: 11.7498 - val_acc: 0.4950\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9985 - acc: 0.5080 - val_loss: 12.0875 - val_acc: 0.5050\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0061 - acc: 0.5000 - val_loss: 12.0389 - val_acc: 0.5050\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.9994 - acc: 0.5096 - val_loss: 12.0833 - val_acc: 0.5050\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 12.0069 - acc: 0.4970 - val_loss: 12.4051 - val_acc: 0.5050\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 12.0087 - acc: 0.5096 - val_loss: 11.6220 - val_acc: 0.5050\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0028 - acc: 0.5096 - val_loss: 12.0847 - val_acc: 0.5050\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0097 - acc: 0.5096 - val_loss: 11.6468 - val_acc: 0.5050\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9962 - acc: 0.5096 - val_loss: 12.2285 - val_acc: 0.5050\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 12.0075 - acc: 0.5096 - val_loss: 12.3453 - val_acc: 0.5050\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0026 - acc: 0.5076 - val_loss: 12.1312 - val_acc: 0.5050\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 12.0090 - acc: 0.4930 - val_loss: 11.9956 - val_acc: 0.5050\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 12.0109 - acc: 0.5096 - val_loss: 11.3552 - val_acc: 0.5050\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9988 - acc: 0.5096 - val_loss: 12.2078 - val_acc: 0.5050\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0093 - acc: 0.5096 - val_loss: 12.0160 - val_acc: 0.5050\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9970 - acc: 0.5017 - val_loss: 12.2734 - val_acc: 0.5050\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0089 - acc: 0.4937 - val_loss: 12.1848 - val_acc: 0.5050\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0085 - acc: 0.5023 - val_loss: 11.6483 - val_acc: 0.5050\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0058 - acc: 0.5096 - val_loss: 12.0438 - val_acc: 0.5050\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0101 - acc: 0.5096 - val_loss: 11.5360 - val_acc: 0.5050\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.9987 - acc: 0.5096 - val_loss: 12.3971 - val_acc: 0.5050\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 12.0068 - acc: 0.5096 - val_loss: 12.0327 - val_acc: 0.5050\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0025 - acc: 0.5096 - val_loss: 12.0945 - val_acc: 0.5050\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0088 - acc: 0.4983 - val_loss: 11.9100 - val_acc: 0.5050\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 12.0086 - acc: 0.5096 - val_loss: 11.7865 - val_acc: 0.5050\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0065 - acc: 0.5096 - val_loss: 12.2872 - val_acc: 0.5050\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 12.0052 - acc: 0.5096 - val_loss: 11.6910 - val_acc: 0.5050\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0016 - acc: 0.4990 - val_loss: 12.2406 - val_acc: 0.5050\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0062 - acc: 0.5096 - val_loss: 12.1254 - val_acc: 0.5050\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 12.0054 - acc: 0.5096 - val_loss: 12.0840 - val_acc: 0.5050\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0120 - acc: 0.5030 - val_loss: 12.0769 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 80us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 5s 2ms/step - loss: 29.0488 - acc: 0.4940 - val_loss: 12.1381 - val_acc: 0.4950\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.8198 - acc: 0.5186 - val_loss: 11.9704 - val_acc: 0.5050\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.9082 - acc: 0.4894 - val_loss: 11.7648 - val_acc: 0.5050\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9483 - acc: 0.5047 - val_loss: 12.0167 - val_acc: 0.4950\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9645 - acc: 0.4960 - val_loss: 11.9680 - val_acc: 0.4950\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9623 - acc: 0.4937 - val_loss: 11.9975 - val_acc: 0.5050\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9671 - acc: 0.4860 - val_loss: 12.1256 - val_acc: 0.5050\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9711 - acc: 0.4884 - val_loss: 11.7322 - val_acc: 0.5050\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9704 - acc: 0.5047 - val_loss: 11.9460 - val_acc: 0.5050\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9769 - acc: 0.4930 - val_loss: 11.7139 - val_acc: 0.5050\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9672 - acc: 0.5023 - val_loss: 12.0646 - val_acc: 0.5050\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9744 - acc: 0.4953 - val_loss: 12.0182 - val_acc: 0.4950\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9687 - acc: 0.4910 - val_loss: 12.0538 - val_acc: 0.4950\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9752 - acc: 0.4910 - val_loss: 12.3509 - val_acc: 0.5050\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9779 - acc: 0.5023 - val_loss: 11.5856 - val_acc: 0.5050\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9719 - acc: 0.4910 - val_loss: 12.0548 - val_acc: 0.4950\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.9778 - acc: 0.5090 - val_loss: 11.6265 - val_acc: 0.5050\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9654 - acc: 0.4990 - val_loss: 12.2128 - val_acc: 0.5050\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9759 - acc: 0.4917 - val_loss: 12.3058 - val_acc: 0.5050\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9720 - acc: 0.4904 - val_loss: 12.0977 - val_acc: 0.4950\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9779 - acc: 0.5020 - val_loss: 11.9613 - val_acc: 0.4950\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9791 - acc: 0.4934 - val_loss: 11.3268 - val_acc: 0.4950\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9684 - acc: 0.4830 - val_loss: 12.1757 - val_acc: 0.5050\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9773 - acc: 0.4963 - val_loss: 12.0056 - val_acc: 0.5050\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9660 - acc: 0.4897 - val_loss: 12.2368 - val_acc: 0.5050\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9778 - acc: 0.4937 - val_loss: 12.1409 - val_acc: 0.5050\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.9770 - acc: 0.5043 - val_loss: 11.6108 - val_acc: 0.4950\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9751 - acc: 0.5003 - val_loss: 11.9926 - val_acc: 0.5050\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9779 - acc: 0.4850 - val_loss: 11.5231 - val_acc: 0.5050\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9677 - acc: 0.4924 - val_loss: 12.3688 - val_acc: 0.5050\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.9757 - acc: 0.5010 - val_loss: 12.0102 - val_acc: 0.4950\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 11.9712 - acc: 0.4924 - val_loss: 12.0626 - val_acc: 0.4950\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.9782 - acc: 0.5050 - val_loss: 11.8521 - val_acc: 0.4950\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9768 - acc: 0.4983 - val_loss: 11.7599 - val_acc: 0.5050\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9751 - acc: 0.4930 - val_loss: 12.2454 - val_acc: 0.5050\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 11.9739 - acc: 0.4957 - val_loss: 11.7080 - val_acc: 0.5050\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 11.9701 - acc: 0.4784 - val_loss: 12.2069 - val_acc: 0.5050\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 11.9755 - acc: 0.5030 - val_loss: 12.0772 - val_acc: 0.4950\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.9742 - acc: 0.4963 - val_loss: 12.0475 - val_acc: 0.5050\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 11.9806 - acc: 0.5050 - val_loss: 12.0219 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 79us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 29.8580 - acc: 0.5013 - val_loss: 12.0619 - val_acc: 0.5050\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 11.7364 - acc: 0.4937 - val_loss: 11.8670 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 1s 207us/step - loss: 11.8342 - acc: 0.4927 - val_loss: 11.6997 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.8831 - acc: 0.4900 - val_loss: 11.9511 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 11.9008 - acc: 0.5106 - val_loss: 11.8844 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9017 - acc: 0.5083 - val_loss: 11.9467 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9049 - acc: 0.5100 - val_loss: 12.0758 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9115 - acc: 0.5080 - val_loss: 11.7007 - val_acc: 0.4950\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9105 - acc: 0.5070 - val_loss: 11.8875 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 1s 200us/step - loss: 11.9157 - acc: 0.5100 - val_loss: 11.6717 - val_acc: 0.4950\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9095 - acc: 0.4973 - val_loss: 11.9960 - val_acc: 0.4950\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 11.9139 - acc: 0.5000 - val_loss: 11.9628 - val_acc: 0.4950\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 1s 197us/step - loss: 11.9102 - acc: 0.5043 - val_loss: 11.9886 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 1s 194us/step - loss: 11.9163 - acc: 0.5100 - val_loss: 12.2311 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 11.9180 - acc: 0.4973 - val_loss: 11.5837 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 81us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 29.0457 - acc: 0.5010 - val_loss: 12.1488 - val_acc: 0.5050\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.8663 - acc: 0.5047 - val_loss: 12.0555 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 11.9561 - acc: 0.5023 - val_loss: 11.8338 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9932 - acc: 0.5060 - val_loss: 12.0824 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 12.0116 - acc: 0.5060 - val_loss: 12.0119 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 12.0091 - acc: 0.5040 - val_loss: 12.0088 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0133 - acc: 0.5096 - val_loss: 12.1522 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0171 - acc: 0.5093 - val_loss: 11.7691 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 12.0166 - acc: 0.5070 - val_loss: 12.0445 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0240 - acc: 0.5096 - val_loss: 11.7820 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 1s 208us/step - loss: 12.0140 - acc: 0.5047 - val_loss: 12.0904 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 12.0220 - acc: 0.5073 - val_loss: 12.0062 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0148 - acc: 0.5096 - val_loss: 12.0833 - val_acc: 0.5050\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0211 - acc: 0.5003 - val_loss: 12.4577 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 1s 201us/step - loss: 12.0244 - acc: 0.5096 - val_loss: 11.6638 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 84us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 29.0163 - acc: 0.4987 - val_loss: 12.1713 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 1s 209us/step - loss: 11.8824 - acc: 0.4827 - val_loss: 12.0625 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 1s 205us/step - loss: 11.9934 - acc: 0.5023 - val_loss: 11.8448 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0375 - acc: 0.5060 - val_loss: 12.1440 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 1s 206us/step - loss: 12.0536 - acc: 0.5013 - val_loss: 12.0673 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0496 - acc: 0.4973 - val_loss: 12.0489 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0533 - acc: 0.4963 - val_loss: 12.2126 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 1s 204us/step - loss: 12.0559 - acc: 0.4890 - val_loss: 11.7789 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0553 - acc: 0.4930 - val_loss: 12.0642 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 1s 207us/step - loss: 12.0623 - acc: 0.4877 - val_loss: 11.8217 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 1s 202us/step - loss: 12.0519 - acc: 0.4804 - val_loss: 12.1485 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 1s 199us/step - loss: 12.0606 - acc: 0.4937 - val_loss: 12.0660 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 1s 198us/step - loss: 12.0529 - acc: 0.4917 - val_loss: 12.1232 - val_acc: 0.5050\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 1s 196us/step - loss: 12.0604 - acc: 0.4990 - val_loss: 12.5054 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 1s 203us/step - loss: 12.0632 - acc: 0.4897 - val_loss: 11.6709 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 82us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 34.5796 - acc: 0.5080 - val_loss: 12.6500 - val_acc: 0.4950\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0981 - acc: 0.5070 - val_loss: 11.7209 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 151us/step - loss: 11.9345 - acc: 0.4997 - val_loss: 11.9676 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0400 - acc: 0.4993 - val_loss: 11.8605 - val_acc: 0.4950\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 151us/step - loss: 12.0682 - acc: 0.4983 - val_loss: 11.8176 - val_acc: 0.4950\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0738 - acc: 0.4910 - val_loss: 12.2571 - val_acc: 0.4950\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 150us/step - loss: 12.0680 - acc: 0.4817 - val_loss: 12.2704 - val_acc: 0.4950\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0786 - acc: 0.5100 - val_loss: 12.2045 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0830 - acc: 0.5096 - val_loss: 12.2929 - val_acc: 0.4950\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 149us/step - loss: 12.0839 - acc: 0.5100 - val_loss: 12.2319 - val_acc: 0.4950\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0937 - acc: 0.5060 - val_loss: 11.6505 - val_acc: 0.4950\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0870 - acc: 0.5100 - val_loss: 11.8079 - val_acc: 0.4950\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0826 - acc: 0.4890 - val_loss: 12.2778 - val_acc: 0.4950\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0802 - acc: 0.5100 - val_loss: 12.2302 - val_acc: 0.4950\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0814 - acc: 0.5100 - val_loss: 12.4601 - val_acc: 0.4950\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0871 - acc: 0.5100 - val_loss: 12.3856 - val_acc: 0.4950\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 12.0984 - acc: 0.5100 - val_loss: 11.7169 - val_acc: 0.4950\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0904 - acc: 0.5120 - val_loss: 11.6713 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0854 - acc: 0.4967 - val_loss: 12.1927 - val_acc: 0.4950\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0902 - acc: 0.4993 - val_loss: 12.1935 - val_acc: 0.4950\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0801 - acc: 0.5100 - val_loss: 12.3984 - val_acc: 0.4950\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0800 - acc: 0.5100 - val_loss: 12.2239 - val_acc: 0.4950\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0911 - acc: 0.5100 - val_loss: 12.0651 - val_acc: 0.4950\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0931 - acc: 0.5100 - val_loss: 12.1650 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0910 - acc: 0.4987 - val_loss: 11.8906 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0975 - acc: 0.5100 - val_loss: 11.8493 - val_acc: 0.4950\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0848 - acc: 0.4973 - val_loss: 11.9808 - val_acc: 0.4950\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0758 - acc: 0.5100 - val_loss: 12.5123 - val_acc: 0.4950\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0869 - acc: 0.5100 - val_loss: 12.3250 - val_acc: 0.4950\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0905 - acc: 0.5100 - val_loss: 12.3474 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 61us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 34.6578 - acc: 0.5060 - val_loss: 12.4921 - val_acc: 0.4950\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.9976 - acc: 0.4980 - val_loss: 11.6105 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 11.8331 - acc: 0.5003 - val_loss: 11.9126 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9502 - acc: 0.4880 - val_loss: 11.7878 - val_acc: 0.5050\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9809 - acc: 0.5103 - val_loss: 11.7075 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 11.9818 - acc: 0.5037 - val_loss: 12.1795 - val_acc: 0.5050\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 150us/step - loss: 11.9824 - acc: 0.5010 - val_loss: 12.1781 - val_acc: 0.5050\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9946 - acc: 0.5113 - val_loss: 12.0808 - val_acc: 0.5050\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 11.9948 - acc: 0.5096 - val_loss: 12.2070 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9964 - acc: 0.5096 - val_loss: 12.1191 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0063 - acc: 0.5096 - val_loss: 11.5772 - val_acc: 0.5050\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 11.9984 - acc: 0.5096 - val_loss: 11.7843 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9943 - acc: 0.5096 - val_loss: 12.1753 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9934 - acc: 0.5096 - val_loss: 12.0832 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9930 - acc: 0.5096 - val_loss: 12.3502 - val_acc: 0.5050\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 11.9988 - acc: 0.5096 - val_loss: 12.2795 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0099 - acc: 0.5003 - val_loss: 11.6501 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0008 - acc: 0.4970 - val_loss: 11.6485 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9974 - acc: 0.5096 - val_loss: 12.0883 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0028 - acc: 0.5096 - val_loss: 12.0512 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9913 - acc: 0.5023 - val_loss: 12.3104 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9940 - acc: 0.5063 - val_loss: 12.1223 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0030 - acc: 0.5096 - val_loss: 11.9561 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0029 - acc: 0.4990 - val_loss: 12.1005 - val_acc: 0.5050\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0028 - acc: 0.5096 - val_loss: 11.8092 - val_acc: 0.5050\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0097 - acc: 0.5096 - val_loss: 11.7569 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 11.9956 - acc: 0.5096 - val_loss: 11.9332 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9888 - acc: 0.5096 - val_loss: 12.3930 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0005 - acc: 0.5096 - val_loss: 12.1685 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0013 - acc: 0.4997 - val_loss: 12.2589 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 62us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 34.6297 - acc: 0.4943 - val_loss: 12.5060 - val_acc: 0.4950\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0162 - acc: 0.4963 - val_loss: 11.6217 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 11.8469 - acc: 0.5106 - val_loss: 11.9427 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9658 - acc: 0.5013 - val_loss: 11.7917 - val_acc: 0.5050\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 11.9983 - acc: 0.4801 - val_loss: 11.7127 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9991 - acc: 0.4877 - val_loss: 12.2103 - val_acc: 0.4950\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 11.9979 - acc: 0.4897 - val_loss: 12.1805 - val_acc: 0.5050\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0101 - acc: 0.4953 - val_loss: 12.0883 - val_acc: 0.5050\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0105 - acc: 0.4894 - val_loss: 12.2280 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 12.0103 - acc: 0.4983 - val_loss: 12.1363 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0215 - acc: 0.5023 - val_loss: 11.5911 - val_acc: 0.4950\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 152us/step - loss: 12.0138 - acc: 0.4897 - val_loss: 11.7983 - val_acc: 0.4950\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0093 - acc: 0.5010 - val_loss: 12.1769 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 153us/step - loss: 12.0079 - acc: 0.4910 - val_loss: 12.0948 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0080 - acc: 0.5030 - val_loss: 12.3821 - val_acc: 0.4950\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0141 - acc: 0.4784 - val_loss: 12.2878 - val_acc: 0.4950\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0247 - acc: 0.4930 - val_loss: 11.6563 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 151us/step - loss: 12.0152 - acc: 0.5063 - val_loss: 11.6589 - val_acc: 0.4950\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0125 - acc: 0.4983 - val_loss: 12.0975 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 151us/step - loss: 12.0179 - acc: 0.4970 - val_loss: 12.0749 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0066 - acc: 0.4897 - val_loss: 12.3355 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0085 - acc: 0.4850 - val_loss: 12.1179 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 154us/step - loss: 12.0182 - acc: 0.4904 - val_loss: 11.9641 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 12.0178 - acc: 0.5023 - val_loss: 12.1358 - val_acc: 0.5050\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 12.0173 - acc: 0.4983 - val_loss: 11.8172 - val_acc: 0.5050\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0254 - acc: 0.4870 - val_loss: 11.7578 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 152us/step - loss: 12.0107 - acc: 0.4937 - val_loss: 11.9443 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0035 - acc: 0.5023 - val_loss: 12.4031 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 152us/step - loss: 12.0155 - acc: 0.4917 - val_loss: 12.1908 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 12.0162 - acc: 0.5010 - val_loss: 12.2879 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 63us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 45.5507 - acc: 0.5199 - val_loss: 13.7851 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 113us/step - loss: 12.4550 - acc: 0.5070 - val_loss: 11.8836 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 106us/step - loss: 11.9103 - acc: 0.5076 - val_loss: 11.8092 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 11.8517 - acc: 0.4924 - val_loss: 12.1784 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 11.9490 - acc: 0.4950 - val_loss: 11.6214 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 11.9989 - acc: 0.5070 - val_loss: 12.1827 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0225 - acc: 0.5033 - val_loss: 11.8282 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 12.0340 - acc: 0.5096 - val_loss: 11.9750 - val_acc: 0.4950\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 12.0278 - acc: 0.5100 - val_loss: 11.9074 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 12.0311 - acc: 0.5076 - val_loss: 12.4295 - val_acc: 0.4950\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 106us/step - loss: 12.0399 - acc: 0.4950 - val_loss: 11.9700 - val_acc: 0.4950\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 12.0422 - acc: 0.5100 - val_loss: 11.4745 - val_acc: 0.4950\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0294 - acc: 0.5080 - val_loss: 12.1039 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0449 - acc: 0.5100 - val_loss: 12.3999 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 12.0450 - acc: 0.5100 - val_loss: 11.7502 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 46us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 45.6008 - acc: 0.4907 - val_loss: 13.7863 - val_acc: 0.5050\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 112us/step - loss: 12.4540 - acc: 0.5020 - val_loss: 11.9030 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 11.9111 - acc: 0.5083 - val_loss: 11.8011 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 11.8499 - acc: 0.4920 - val_loss: 12.1915 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 11.9428 - acc: 0.5007 - val_loss: 11.6161 - val_acc: 0.5050\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 11.9913 - acc: 0.5096 - val_loss: 12.1850 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 12.0171 - acc: 0.4983 - val_loss: 11.8021 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 111us/step - loss: 12.0275 - acc: 0.5060 - val_loss: 11.9851 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 106us/step - loss: 12.0233 - acc: 0.5096 - val_loss: 11.9028 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0255 - acc: 0.4993 - val_loss: 12.4407 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 111us/step - loss: 12.0338 - acc: 0.5096 - val_loss: 11.9561 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0355 - acc: 0.5096 - val_loss: 11.4729 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0238 - acc: 0.5096 - val_loss: 12.0921 - val_acc: 0.5050\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 12.0387 - acc: 0.5096 - val_loss: 12.4049 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 12.0391 - acc: 0.5096 - val_loss: 11.7362 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 46us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 6s 2ms/step - loss: 45.5025 - acc: 0.5080 - val_loss: 13.7553 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.4501 - acc: 0.4973 - val_loss: 11.8881 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 11.9155 - acc: 0.4917 - val_loss: 11.8063 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 111us/step - loss: 11.8543 - acc: 0.5030 - val_loss: 12.1761 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 11.9465 - acc: 0.4993 - val_loss: 11.6334 - val_acc: 0.5050\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 11.9937 - acc: 0.4947 - val_loss: 12.1759 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 12.0163 - acc: 0.4900 - val_loss: 11.8124 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 109us/step - loss: 12.0282 - acc: 0.4924 - val_loss: 11.9667 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 111us/step - loss: 12.0225 - acc: 0.5050 - val_loss: 11.9011 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 113us/step - loss: 12.0260 - acc: 0.4983 - val_loss: 12.4161 - val_acc: 0.4950\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 110us/step - loss: 12.0346 - acc: 0.4963 - val_loss: 11.9702 - val_acc: 0.4950\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0352 - acc: 0.4934 - val_loss: 11.4839 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 111us/step - loss: 12.0241 - acc: 0.4930 - val_loss: 12.0895 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 107us/step - loss: 12.0397 - acc: 0.4910 - val_loss: 12.3855 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 108us/step - loss: 12.0394 - acc: 0.4757 - val_loss: 11.7436 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 46us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.7358 - acc: 0.5093 - val_loss: 12.4985 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 1s 166us/step - loss: 11.9975 - acc: 0.4997 - val_loss: 11.6101 - val_acc: 0.4950\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.8326 - acc: 0.5043 - val_loss: 11.9240 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 11.9522 - acc: 0.4904 - val_loss: 11.7974 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9838 - acc: 0.5003 - val_loss: 11.7127 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9846 - acc: 0.5066 - val_loss: 12.1760 - val_acc: 0.4950\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 11.9843 - acc: 0.5047 - val_loss: 12.1715 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9965 - acc: 0.5100 - val_loss: 12.0813 - val_acc: 0.4950\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9963 - acc: 0.5017 - val_loss: 12.2079 - val_acc: 0.4950\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9971 - acc: 0.4987 - val_loss: 12.1166 - val_acc: 0.4950\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 12.0074 - acc: 0.5100 - val_loss: 11.5824 - val_acc: 0.4950\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 11.9995 - acc: 0.5100 - val_loss: 11.7937 - val_acc: 0.4950\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9959 - acc: 0.4987 - val_loss: 12.1692 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9949 - acc: 0.5100 - val_loss: 12.0881 - val_acc: 0.4950\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 11.9942 - acc: 0.5100 - val_loss: 12.3531 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 64us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.9191 - acc: 0.5043 - val_loss: 12.4248 - val_acc: 0.4950\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 11.9721 - acc: 0.5007 - val_loss: 11.5837 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 11.7997 - acc: 0.5027 - val_loss: 11.9269 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 11.9232 - acc: 0.5063 - val_loss: 11.7627 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 11.9600 - acc: 0.4860 - val_loss: 11.6530 - val_acc: 0.5050\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 11.9585 - acc: 0.5096 - val_loss: 12.1630 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9595 - acc: 0.5007 - val_loss: 12.1244 - val_acc: 0.5050\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9716 - acc: 0.5020 - val_loss: 12.0172 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 11.9706 - acc: 0.5096 - val_loss: 12.1979 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 11.9721 - acc: 0.5017 - val_loss: 12.0703 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 11.9830 - acc: 0.5096 - val_loss: 11.5610 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 11.9737 - acc: 0.4930 - val_loss: 11.8123 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 11.9716 - acc: 0.5096 - val_loss: 12.1157 - val_acc: 0.5050\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 166us/step - loss: 11.9706 - acc: 0.5096 - val_loss: 12.0170 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 11.9689 - acc: 0.5096 - val_loss: 12.3257 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 66us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.6357 - acc: 0.4874 - val_loss: 12.5251 - val_acc: 0.5050\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 12.0318 - acc: 0.5096 - val_loss: 11.6467 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 11.8663 - acc: 0.4977 - val_loss: 11.9456 - val_acc: 0.5050\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 11.9724 - acc: 0.4917 - val_loss: 11.8124 - val_acc: 0.5050\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0056 - acc: 0.5040 - val_loss: 11.7373 - val_acc: 0.4950\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0093 - acc: 0.5010 - val_loss: 12.2088 - val_acc: 0.5050\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0078 - acc: 0.4997 - val_loss: 12.1949 - val_acc: 0.4950\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0188 - acc: 0.4864 - val_loss: 12.1197 - val_acc: 0.5050\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0208 - acc: 0.4947 - val_loss: 12.2402 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0209 - acc: 0.4811 - val_loss: 12.1419 - val_acc: 0.5050\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0310 - acc: 0.4940 - val_loss: 11.5918 - val_acc: 0.5050\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0232 - acc: 0.4937 - val_loss: 11.8064 - val_acc: 0.5050\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0194 - acc: 0.4957 - val_loss: 12.2127 - val_acc: 0.4950\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0185 - acc: 0.5030 - val_loss: 12.1222 - val_acc: 0.5050\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0181 - acc: 0.4924 - val_loss: 12.3838 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 64us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/20\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.5533 - acc: 0.5050 - val_loss: 12.5427 - val_acc: 0.5050\n",
      "Epoch 2/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0290 - acc: 0.5076 - val_loss: 11.6289 - val_acc: 0.5050\n",
      "Epoch 3/20\n",
      "3008/3008 [==============================] - 0s 166us/step - loss: 11.8640 - acc: 0.5086 - val_loss: 11.9564 - val_acc: 0.4950\n",
      "Epoch 4/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9865 - acc: 0.4993 - val_loss: 11.7953 - val_acc: 0.4950\n",
      "Epoch 5/20\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 12.0186 - acc: 0.5003 - val_loss: 11.7366 - val_acc: 0.4950\n",
      "Epoch 6/20\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 12.0204 - acc: 0.5093 - val_loss: 12.2434 - val_acc: 0.4950\n",
      "Epoch 7/20\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 12.0199 - acc: 0.5010 - val_loss: 12.2047 - val_acc: 0.4950\n",
      "Epoch 8/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0305 - acc: 0.5083 - val_loss: 12.1193 - val_acc: 0.4950\n",
      "Epoch 9/20\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0317 - acc: 0.5100 - val_loss: 12.2597 - val_acc: 0.4950\n",
      "Epoch 10/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0319 - acc: 0.5100 - val_loss: 12.1524 - val_acc: 0.4950\n",
      "Epoch 11/20\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0430 - acc: 0.5100 - val_loss: 11.5968 - val_acc: 0.4950\n",
      "Epoch 12/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0350 - acc: 0.4950 - val_loss: 11.8066 - val_acc: 0.4950\n",
      "Epoch 13/20\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 12.0304 - acc: 0.4953 - val_loss: 12.1969 - val_acc: 0.4950\n",
      "Epoch 14/20\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0290 - acc: 0.5100 - val_loss: 12.1360 - val_acc: 0.4950\n",
      "Epoch 15/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0286 - acc: 0.5100 - val_loss: 12.4271 - val_acc: 0.4950\n",
      "Epoch 16/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0352 - acc: 0.5013 - val_loss: 12.3190 - val_acc: 0.4950\n",
      "Epoch 17/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0464 - acc: 0.5100 - val_loss: 11.6610 - val_acc: 0.4950\n",
      "Epoch 18/20\n",
      "3008/3008 [==============================] - 1s 166us/step - loss: 12.0366 - acc: 0.5100 - val_loss: 11.6518 - val_acc: 0.4950\n",
      "Epoch 19/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0330 - acc: 0.5100 - val_loss: 12.1122 - val_acc: 0.4950\n",
      "Epoch 20/20\n",
      "3008/3008 [==============================] - 1s 167us/step - loss: 12.0391 - acc: 0.5100 - val_loss: 12.1090 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 63us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/20\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.6572 - acc: 0.5050 - val_loss: 12.4769 - val_acc: 0.5050\n",
      "Epoch 2/20\n",
      "3008/3008 [==============================] - 0s 166us/step - loss: 11.9891 - acc: 0.5160 - val_loss: 11.6037 - val_acc: 0.4950\n",
      "Epoch 3/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.8166 - acc: 0.4950 - val_loss: 11.9282 - val_acc: 0.4950\n",
      "Epoch 4/20\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 11.9378 - acc: 0.5027 - val_loss: 11.7643 - val_acc: 0.5050\n",
      "Epoch 5/20\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9733 - acc: 0.5030 - val_loss: 11.6902 - val_acc: 0.4950\n",
      "Epoch 6/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 11.9739 - acc: 0.5116 - val_loss: 12.1818 - val_acc: 0.5050\n",
      "Epoch 7/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.9735 - acc: 0.5096 - val_loss: 12.1575 - val_acc: 0.5050\n",
      "Epoch 8/20\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9851 - acc: 0.5033 - val_loss: 12.0745 - val_acc: 0.5050\n",
      "Epoch 9/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.9853 - acc: 0.5096 - val_loss: 12.2112 - val_acc: 0.5050\n",
      "Epoch 10/20\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9873 - acc: 0.5066 - val_loss: 12.0937 - val_acc: 0.4950\n",
      "Epoch 11/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.9979 - acc: 0.4993 - val_loss: 11.5679 - val_acc: 0.5050\n",
      "Epoch 12/20\n",
      "3008/3008 [==============================] - 0s 156us/step - loss: 11.9887 - acc: 0.5066 - val_loss: 11.8041 - val_acc: 0.5050\n",
      "Epoch 13/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 11.9859 - acc: 0.5096 - val_loss: 12.1546 - val_acc: 0.5050\n",
      "Epoch 14/20\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9848 - acc: 0.5096 - val_loss: 12.0598 - val_acc: 0.5050\n",
      "Epoch 15/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 11.9828 - acc: 0.5096 - val_loss: 12.3550 - val_acc: 0.5050\n",
      "Epoch 16/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9892 - acc: 0.4997 - val_loss: 12.2542 - val_acc: 0.5050\n",
      "Epoch 17/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0004 - acc: 0.5096 - val_loss: 11.6475 - val_acc: 0.5050\n",
      "Epoch 18/20\n",
      "3008/3008 [==============================] - 0s 158us/step - loss: 11.9910 - acc: 0.4890 - val_loss: 11.6663 - val_acc: 0.5050\n",
      "Epoch 19/20\n",
      "3008/3008 [==============================] - 0s 157us/step - loss: 11.9882 - acc: 0.5096 - val_loss: 12.0575 - val_acc: 0.5050\n",
      "Epoch 20/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 11.9935 - acc: 0.5096 - val_loss: 12.0287 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 67us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/20\n",
      "3008/3008 [==============================] - 7s 2ms/step - loss: 34.6327 - acc: 0.4820 - val_loss: 12.5416 - val_acc: 0.5050\n",
      "Epoch 2/20\n",
      "3008/3008 [==============================] - 1s 166us/step - loss: 12.0371 - acc: 0.4904 - val_loss: 11.6226 - val_acc: 0.4950\n",
      "Epoch 3/20\n",
      "3008/3008 [==============================] - 1s 169us/step - loss: 11.8717 - acc: 0.4957 - val_loss: 11.9680 - val_acc: 0.5050\n",
      "Epoch 4/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 11.9972 - acc: 0.4870 - val_loss: 11.8220 - val_acc: 0.5050\n",
      "Epoch 5/20\n",
      "3008/3008 [==============================] - 0s 166us/step - loss: 12.0288 - acc: 0.4943 - val_loss: 11.7442 - val_acc: 0.5050\n",
      "Epoch 6/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0296 - acc: 0.5020 - val_loss: 12.2535 - val_acc: 0.4950\n",
      "Epoch 7/20\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0284 - acc: 0.5023 - val_loss: 12.2236 - val_acc: 0.5050\n",
      "Epoch 8/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0401 - acc: 0.5003 - val_loss: 12.1172 - val_acc: 0.4950\n",
      "Epoch 9/20\n",
      "3008/3008 [==============================] - 1s 169us/step - loss: 12.0403 - acc: 0.5010 - val_loss: 12.2578 - val_acc: 0.5050\n",
      "Epoch 10/20\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0406 - acc: 0.4943 - val_loss: 12.1732 - val_acc: 0.5050\n",
      "Epoch 11/20\n",
      "3008/3008 [==============================] - 0s 162us/step - loss: 12.0516 - acc: 0.4857 - val_loss: 11.6036 - val_acc: 0.5050\n",
      "Epoch 12/20\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0434 - acc: 0.4817 - val_loss: 11.8206 - val_acc: 0.4950\n",
      "Epoch 13/20\n",
      "3008/3008 [==============================] - 0s 163us/step - loss: 12.0394 - acc: 0.5010 - val_loss: 12.2226 - val_acc: 0.5050\n",
      "Epoch 14/20\n",
      "3008/3008 [==============================] - 0s 160us/step - loss: 12.0376 - acc: 0.4990 - val_loss: 12.1351 - val_acc: 0.4950\n",
      "Epoch 15/20\n",
      "3008/3008 [==============================] - 1s 167us/step - loss: 12.0378 - acc: 0.4910 - val_loss: 12.4232 - val_acc: 0.5050\n",
      "Epoch 16/20\n",
      "3008/3008 [==============================] - 0s 159us/step - loss: 12.0442 - acc: 0.4977 - val_loss: 12.3298 - val_acc: 0.5050\n",
      "Epoch 17/20\n",
      "3008/3008 [==============================] - 1s 166us/step - loss: 12.0548 - acc: 0.4830 - val_loss: 11.6742 - val_acc: 0.4950\n",
      "Epoch 18/20\n",
      "3008/3008 [==============================] - 0s 164us/step - loss: 12.0452 - acc: 0.4877 - val_loss: 11.6750 - val_acc: 0.5050\n",
      "Epoch 19/20\n",
      "3008/3008 [==============================] - 0s 165us/step - loss: 12.0425 - acc: 0.5023 - val_loss: 12.1301 - val_acc: 0.5050\n",
      "Epoch 20/20\n",
      "3008/3008 [==============================] - 0s 161us/step - loss: 12.0480 - acc: 0.4897 - val_loss: 12.1012 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 66us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 29.1654 - acc: 0.5020 - val_loss: 12.1295 - val_acc: 0.4950\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.8030 - acc: 0.5086 - val_loss: 11.9554 - val_acc: 0.4950\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 1s 229us/step - loss: 11.8805 - acc: 0.5020 - val_loss: 11.7192 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9253 - acc: 0.4980 - val_loss: 11.9668 - val_acc: 0.4950\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9395 - acc: 0.5096 - val_loss: 11.9462 - val_acc: 0.4950\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 1s 232us/step - loss: 11.9389 - acc: 0.5100 - val_loss: 11.9699 - val_acc: 0.4950\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 1s 234us/step - loss: 11.9424 - acc: 0.5073 - val_loss: 12.1144 - val_acc: 0.4950\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9477 - acc: 0.5100 - val_loss: 11.7284 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9473 - acc: 0.4934 - val_loss: 11.9209 - val_acc: 0.4950\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 1s 230us/step - loss: 11.9515 - acc: 0.4953 - val_loss: 11.7062 - val_acc: 0.4950\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9441 - acc: 0.5000 - val_loss: 12.0277 - val_acc: 0.4950\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9500 - acc: 0.5080 - val_loss: 11.9921 - val_acc: 0.4950\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 1s 231us/step - loss: 11.9444 - acc: 0.5100 - val_loss: 12.0456 - val_acc: 0.4950\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9513 - acc: 0.5100 - val_loss: 12.3100 - val_acc: 0.4950\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9530 - acc: 0.5033 - val_loss: 11.5854 - val_acc: 0.4950\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 1s 229us/step - loss: 11.9483 - acc: 0.5100 - val_loss: 11.9995 - val_acc: 0.4950\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9535 - acc: 0.5100 - val_loss: 11.6438 - val_acc: 0.4950\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9415 - acc: 0.4993 - val_loss: 12.1736 - val_acc: 0.4950\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 1s 230us/step - loss: 11.9518 - acc: 0.5100 - val_loss: 12.2602 - val_acc: 0.4950\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9478 - acc: 0.4960 - val_loss: 12.0741 - val_acc: 0.4950\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9540 - acc: 0.5007 - val_loss: 11.9192 - val_acc: 0.4950\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9553 - acc: 0.4927 - val_loss: 11.3529 - val_acc: 0.4950\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9444 - acc: 0.5100 - val_loss: 12.1258 - val_acc: 0.4950\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 1s 230us/step - loss: 11.9532 - acc: 0.5100 - val_loss: 11.9625 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9422 - acc: 0.5100 - val_loss: 12.2203 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 1s 232us/step - loss: 11.9536 - acc: 0.5027 - val_loss: 12.1104 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9533 - acc: 0.5040 - val_loss: 11.6167 - val_acc: 0.4950\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9508 - acc: 0.5020 - val_loss: 11.9493 - val_acc: 0.4950\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9544 - acc: 0.4914 - val_loss: 11.5353 - val_acc: 0.4950\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9439 - acc: 0.5033 - val_loss: 12.3214 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 93us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 28.9963 - acc: 0.5057 - val_loss: 12.1163 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.8421 - acc: 0.4960 - val_loss: 12.0097 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9172 - acc: 0.4910 - val_loss: 11.7626 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9555 - acc: 0.4900 - val_loss: 12.0552 - val_acc: 0.5050\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9720 - acc: 0.5096 - val_loss: 11.9714 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9714 - acc: 0.5096 - val_loss: 11.9853 - val_acc: 0.5050\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 1s 228us/step - loss: 11.9755 - acc: 0.4927 - val_loss: 12.1314 - val_acc: 0.5050\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9795 - acc: 0.5096 - val_loss: 11.7307 - val_acc: 0.5050\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9789 - acc: 0.5003 - val_loss: 11.9874 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.9853 - acc: 0.4943 - val_loss: 11.7257 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.9760 - acc: 0.5096 - val_loss: 12.0665 - val_acc: 0.5050\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9833 - acc: 0.5017 - val_loss: 11.9999 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 1s 217us/step - loss: 11.9769 - acc: 0.5037 - val_loss: 12.0455 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 1s 222us/step - loss: 11.9837 - acc: 0.4943 - val_loss: 12.3983 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9863 - acc: 0.5096 - val_loss: 11.5992 - val_acc: 0.5050\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 1s 219us/step - loss: 11.9805 - acc: 0.5043 - val_loss: 12.0759 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9870 - acc: 0.4910 - val_loss: 11.6090 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9738 - acc: 0.5096 - val_loss: 12.2080 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 1s 219us/step - loss: 11.9846 - acc: 0.5003 - val_loss: 12.3165 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.9803 - acc: 0.4957 - val_loss: 12.1274 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9863 - acc: 0.4963 - val_loss: 11.9854 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9877 - acc: 0.5063 - val_loss: 11.3206 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9766 - acc: 0.5096 - val_loss: 12.1722 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9863 - acc: 0.5096 - val_loss: 11.9979 - val_acc: 0.5050\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9742 - acc: 0.5076 - val_loss: 12.2461 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9862 - acc: 0.4970 - val_loss: 12.1770 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9855 - acc: 0.5063 - val_loss: 11.6085 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9835 - acc: 0.4977 - val_loss: 12.0207 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9870 - acc: 0.4817 - val_loss: 11.5161 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 1s 229us/step - loss: 11.9765 - acc: 0.5063 - val_loss: 12.3696 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 93us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 29.8930 - acc: 0.4970 - val_loss: 12.0631 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.7508 - acc: 0.5020 - val_loss: 11.8725 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.8342 - acc: 0.5130 - val_loss: 11.7006 - val_acc: 0.5050\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.8836 - acc: 0.4904 - val_loss: 11.9636 - val_acc: 0.4950\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9026 - acc: 0.4827 - val_loss: 11.8954 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9042 - acc: 0.4847 - val_loss: 11.9404 - val_acc: 0.5050\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 1s 219us/step - loss: 11.9082 - acc: 0.4953 - val_loss: 12.0610 - val_acc: 0.5050\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 1s 219us/step - loss: 11.9137 - acc: 0.4967 - val_loss: 11.7005 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 1s 218us/step - loss: 11.9126 - acc: 0.4840 - val_loss: 11.9017 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9185 - acc: 0.5013 - val_loss: 11.6698 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 1s 231us/step - loss: 11.9126 - acc: 0.4837 - val_loss: 11.9943 - val_acc: 0.4950\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 1s 222us/step - loss: 11.9167 - acc: 0.4960 - val_loss: 11.9692 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 1s 224us/step - loss: 11.9138 - acc: 0.4914 - val_loss: 11.9960 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9186 - acc: 0.4930 - val_loss: 12.2370 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 1s 218us/step - loss: 11.9207 - acc: 0.4897 - val_loss: 11.5572 - val_acc: 0.5050\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 1s 222us/step - loss: 11.9168 - acc: 0.4937 - val_loss: 11.9769 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 1s 222us/step - loss: 11.9201 - acc: 0.4973 - val_loss: 11.6571 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9114 - acc: 0.4917 - val_loss: 12.1198 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9199 - acc: 0.4937 - val_loss: 12.2125 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9162 - acc: 0.4963 - val_loss: 11.9984 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9227 - acc: 0.4897 - val_loss: 11.8890 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9218 - acc: 0.4977 - val_loss: 11.3898 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 1s 221us/step - loss: 11.9139 - acc: 0.4983 - val_loss: 12.0909 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9216 - acc: 0.5070 - val_loss: 11.9564 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 1s 220us/step - loss: 11.9115 - acc: 0.4957 - val_loss: 12.1519 - val_acc: 0.5050\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 1s 226us/step - loss: 11.9230 - acc: 0.4890 - val_loss: 12.0284 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 1s 223us/step - loss: 11.9205 - acc: 0.4924 - val_loss: 11.6143 - val_acc: 0.4950\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 1s 225us/step - loss: 11.9198 - acc: 0.4983 - val_loss: 11.9390 - val_acc: 0.4950\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 1s 229us/step - loss: 11.9224 - acc: 0.4924 - val_loss: 11.5517 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 1s 227us/step - loss: 11.9124 - acc: 0.4957 - val_loss: 12.2542 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 93us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 40.3059 - acc: 0.4980 - val_loss: 13.3174 - val_acc: 0.5050\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.2268 - acc: 0.5066 - val_loss: 11.2457 - val_acc: 0.4950\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.8513 - acc: 0.5017 - val_loss: 11.8453 - val_acc: 0.5050\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 11.9086 - acc: 0.5043 - val_loss: 12.4373 - val_acc: 0.4950\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0183 - acc: 0.5066 - val_loss: 12.1298 - val_acc: 0.4950\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 0s 145us/step - loss: 12.0371 - acc: 0.5106 - val_loss: 12.1056 - val_acc: 0.4950\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0371 - acc: 0.5080 - val_loss: 12.0307 - val_acc: 0.4950\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0373 - acc: 0.5100 - val_loss: 11.9681 - val_acc: 0.4950\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0542 - acc: 0.5100 - val_loss: 11.4538 - val_acc: 0.4950\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0437 - acc: 0.5100 - val_loss: 12.3315 - val_acc: 0.4950\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0372 - acc: 0.5086 - val_loss: 12.4088 - val_acc: 0.4950\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0589 - acc: 0.5083 - val_loss: 11.8893 - val_acc: 0.4950\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0590 - acc: 0.5100 - val_loss: 11.7833 - val_acc: 0.4950\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0412 - acc: 0.5100 - val_loss: 12.2406 - val_acc: 0.4950\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0449 - acc: 0.5100 - val_loss: 12.1561 - val_acc: 0.4950\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0573 - acc: 0.5100 - val_loss: 11.7868 - val_acc: 0.4950\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0539 - acc: 0.5027 - val_loss: 12.1793 - val_acc: 0.4950\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0466 - acc: 0.5100 - val_loss: 12.3044 - val_acc: 0.4950\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 0s 147us/step - loss: 12.0545 - acc: 0.5100 - val_loss: 12.0797 - val_acc: 0.4950\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0605 - acc: 0.5066 - val_loss: 11.8571 - val_acc: 0.4950\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0415 - acc: 0.5100 - val_loss: 12.0458 - val_acc: 0.4950\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0512 - acc: 0.5047 - val_loss: 12.2140 - val_acc: 0.4950\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0659 - acc: 0.5100 - val_loss: 11.8269 - val_acc: 0.4950\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0495 - acc: 0.5100 - val_loss: 12.3310 - val_acc: 0.4950\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0477 - acc: 0.5100 - val_loss: 12.1213 - val_acc: 0.4950\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0550 - acc: 0.5100 - val_loss: 11.7204 - val_acc: 0.4950\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0601 - acc: 0.5100 - val_loss: 11.6388 - val_acc: 0.4950\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 0s 146us/step - loss: 12.0453 - acc: 0.5100 - val_loss: 12.4348 - val_acc: 0.4950\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0488 - acc: 0.5033 - val_loss: 12.3705 - val_acc: 0.4950\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0683 - acc: 0.5100 - val_loss: 11.7535 - val_acc: 0.4950\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 0s 144us/step - loss: 12.0495 - acc: 0.4967 - val_loss: 12.0730 - val_acc: 0.4950\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0424 - acc: 0.5100 - val_loss: 12.0199 - val_acc: 0.4950\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 0s 144us/step - loss: 12.0581 - acc: 0.5100 - val_loss: 11.8123 - val_acc: 0.4950\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0598 - acc: 0.5100 - val_loss: 12.2343 - val_acc: 0.4950\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 0s 143us/step - loss: 12.0520 - acc: 0.5100 - val_loss: 12.5274 - val_acc: 0.4950\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0507 - acc: 0.5100 - val_loss: 12.2108 - val_acc: 0.4950\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0584 - acc: 0.5100 - val_loss: 11.3990 - val_acc: 0.4950\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0519 - acc: 0.5100 - val_loss: 11.9720 - val_acc: 0.4950\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0449 - acc: 0.4947 - val_loss: 12.3103 - val_acc: 0.4950\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0645 - acc: 0.5100 - val_loss: 12.1915 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 60us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 40.4146 - acc: 0.5057 - val_loss: 13.2668 - val_acc: 0.5050\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.1733 - acc: 0.5130 - val_loss: 11.2415 - val_acc: 0.5050\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 11.7931 - acc: 0.5043 - val_loss: 11.7777 - val_acc: 0.5050\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.8474 - acc: 0.5063 - val_loss: 12.3299 - val_acc: 0.5050\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 11.9543 - acc: 0.5096 - val_loss: 12.0710 - val_acc: 0.5050\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9746 - acc: 0.5096 - val_loss: 12.0681 - val_acc: 0.5050\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.9777 - acc: 0.5096 - val_loss: 11.9947 - val_acc: 0.5050\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 0s 143us/step - loss: 11.9782 - acc: 0.5040 - val_loss: 11.8762 - val_acc: 0.5050\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 11.9927 - acc: 0.5096 - val_loss: 11.3935 - val_acc: 0.5050\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 11.9845 - acc: 0.4940 - val_loss: 12.2238 - val_acc: 0.5050\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 0s 135us/step - loss: 11.9768 - acc: 0.5096 - val_loss: 12.3474 - val_acc: 0.5050\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 11.9976 - acc: 0.5096 - val_loss: 11.8797 - val_acc: 0.5050\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 11.9985 - acc: 0.5096 - val_loss: 11.7435 - val_acc: 0.5050\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.9810 - acc: 0.5096 - val_loss: 12.1357 - val_acc: 0.5050\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 0s 144us/step - loss: 11.9851 - acc: 0.5096 - val_loss: 12.0805 - val_acc: 0.5050\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 11.9963 - acc: 0.5096 - val_loss: 11.7393 - val_acc: 0.5050\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9927 - acc: 0.5096 - val_loss: 12.1380 - val_acc: 0.5050\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9870 - acc: 0.5096 - val_loss: 12.2761 - val_acc: 0.5050\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9935 - acc: 0.5050 - val_loss: 12.0225 - val_acc: 0.5050\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9998 - acc: 0.5096 - val_loss: 11.7676 - val_acc: 0.5050\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9824 - acc: 0.4963 - val_loss: 11.9330 - val_acc: 0.5050\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.9896 - acc: 0.5096 - val_loss: 12.1415 - val_acc: 0.5050\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0047 - acc: 0.5096 - val_loss: 11.8313 - val_acc: 0.5050\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 11.9892 - acc: 0.4930 - val_loss: 12.2525 - val_acc: 0.4950\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 11.9876 - acc: 0.5037 - val_loss: 12.0489 - val_acc: 0.5050\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 11.9951 - acc: 0.5096 - val_loss: 11.6642 - val_acc: 0.5050\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 11.9982 - acc: 0.5096 - val_loss: 11.6076 - val_acc: 0.5050\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.9855 - acc: 0.5096 - val_loss: 12.3538 - val_acc: 0.5050\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 11.9884 - acc: 0.5096 - val_loss: 12.2910 - val_acc: 0.5050\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0063 - acc: 0.5096 - val_loss: 11.7427 - val_acc: 0.5050\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 11.9900 - acc: 0.5096 - val_loss: 12.0206 - val_acc: 0.5050\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 0s 155us/step - loss: 11.9820 - acc: 0.5096 - val_loss: 11.9226 - val_acc: 0.5050\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 0s 147us/step - loss: 11.9971 - acc: 0.5096 - val_loss: 11.7454 - val_acc: 0.5050\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 0s 148us/step - loss: 11.9986 - acc: 0.4930 - val_loss: 12.1383 - val_acc: 0.5050\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 0s 145us/step - loss: 11.9913 - acc: 0.5096 - val_loss: 12.4550 - val_acc: 0.5050\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 0s 149us/step - loss: 11.9913 - acc: 0.5096 - val_loss: 12.1682 - val_acc: 0.5050\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 0s 146us/step - loss: 11.9969 - acc: 0.5096 - val_loss: 11.3949 - val_acc: 0.5050\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 11.9922 - acc: 0.5096 - val_loss: 11.9080 - val_acc: 0.5050\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 0s 148us/step - loss: 11.9853 - acc: 0.5096 - val_loss: 12.2040 - val_acc: 0.5050\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 0s 147us/step - loss: 12.0021 - acc: 0.4943 - val_loss: 12.1191 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 59us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/40\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 40.0492 - acc: 0.4870 - val_loss: 13.3626 - val_acc: 0.5050\n",
      "Epoch 2/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.2689 - acc: 0.4963 - val_loss: 11.2556 - val_acc: 0.4950\n",
      "Epoch 3/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 11.8950 - acc: 0.4983 - val_loss: 11.8979 - val_acc: 0.4950\n",
      "Epoch 4/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 11.9472 - acc: 0.5073 - val_loss: 12.4974 - val_acc: 0.4950\n",
      "Epoch 5/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.0538 - acc: 0.4850 - val_loss: 12.1638 - val_acc: 0.4950\n",
      "Epoch 6/40\n",
      "3008/3008 [==============================] - 0s 135us/step - loss: 12.0716 - acc: 0.4987 - val_loss: 12.1238 - val_acc: 0.4950\n",
      "Epoch 7/40\n",
      "3008/3008 [==============================] - 0s 135us/step - loss: 12.0703 - acc: 0.5120 - val_loss: 12.0575 - val_acc: 0.5050\n",
      "Epoch 8/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0712 - acc: 0.4950 - val_loss: 12.0013 - val_acc: 0.5050\n",
      "Epoch 9/40\n",
      "3008/3008 [==============================] - 0s 136us/step - loss: 12.0885 - acc: 0.5007 - val_loss: 11.4683 - val_acc: 0.4950\n",
      "Epoch 10/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0786 - acc: 0.4957 - val_loss: 12.4068 - val_acc: 0.5050\n",
      "Epoch 11/40\n",
      "3008/3008 [==============================] - 0s 135us/step - loss: 12.0706 - acc: 0.5020 - val_loss: 12.4475 - val_acc: 0.4950\n",
      "Epoch 12/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0926 - acc: 0.4963 - val_loss: 11.8901 - val_acc: 0.4950\n",
      "Epoch 13/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0939 - acc: 0.4930 - val_loss: 11.7975 - val_acc: 0.5050\n",
      "Epoch 14/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0752 - acc: 0.5080 - val_loss: 12.2852 - val_acc: 0.5050\n",
      "Epoch 15/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0792 - acc: 0.4997 - val_loss: 12.2226 - val_acc: 0.5050\n",
      "Epoch 16/40\n",
      "3008/3008 [==============================] - 0s 136us/step - loss: 12.0926 - acc: 0.4847 - val_loss: 11.8090 - val_acc: 0.5050\n",
      "Epoch 17/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0877 - acc: 0.4983 - val_loss: 12.2025 - val_acc: 0.5050\n",
      "Epoch 18/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0813 - acc: 0.5017 - val_loss: 12.3383 - val_acc: 0.5050\n",
      "Epoch 19/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0889 - acc: 0.4924 - val_loss: 12.0969 - val_acc: 0.5050\n",
      "Epoch 20/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0948 - acc: 0.5023 - val_loss: 11.9037 - val_acc: 0.5050\n",
      "Epoch 21/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0758 - acc: 0.5023 - val_loss: 12.0935 - val_acc: 0.5050\n",
      "Epoch 22/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0846 - acc: 0.5023 - val_loss: 12.2604 - val_acc: 0.5050\n",
      "Epoch 23/40\n",
      "3008/3008 [==============================] - 0s 140us/step - loss: 12.1014 - acc: 0.5023 - val_loss: 11.8482 - val_acc: 0.5050\n",
      "Epoch 24/40\n",
      "3008/3008 [==============================] - 0s 136us/step - loss: 12.0832 - acc: 0.4924 - val_loss: 12.3609 - val_acc: 0.5050\n",
      "Epoch 25/40\n",
      "3008/3008 [==============================] - 0s 142us/step - loss: 12.0813 - acc: 0.4977 - val_loss: 12.1689 - val_acc: 0.4950\n",
      "Epoch 26/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0901 - acc: 0.4870 - val_loss: 11.7500 - val_acc: 0.5050\n",
      "Epoch 27/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0940 - acc: 0.5023 - val_loss: 11.6523 - val_acc: 0.5050\n",
      "Epoch 28/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0795 - acc: 0.5037 - val_loss: 12.4937 - val_acc: 0.4950\n",
      "Epoch 29/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0828 - acc: 0.4943 - val_loss: 12.3984 - val_acc: 0.5050\n",
      "Epoch 30/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.1022 - acc: 0.4917 - val_loss: 11.7661 - val_acc: 0.5050\n",
      "Epoch 31/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0844 - acc: 0.4897 - val_loss: 12.1016 - val_acc: 0.5050\n",
      "Epoch 32/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0753 - acc: 0.4990 - val_loss: 12.0357 - val_acc: 0.5050\n",
      "Epoch 33/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0924 - acc: 0.4884 - val_loss: 11.8564 - val_acc: 0.5050\n",
      "Epoch 34/40\n",
      "3008/3008 [==============================] - 0s 137us/step - loss: 12.0945 - acc: 0.4930 - val_loss: 12.2869 - val_acc: 0.5050\n",
      "Epoch 35/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0851 - acc: 0.5023 - val_loss: 12.5922 - val_acc: 0.5050\n",
      "Epoch 36/40\n",
      "3008/3008 [==============================] - 0s 134us/step - loss: 12.0857 - acc: 0.4970 - val_loss: 12.2443 - val_acc: 0.5050\n",
      "Epoch 37/40\n",
      "3008/3008 [==============================] - 0s 141us/step - loss: 12.0925 - acc: 0.5057 - val_loss: 11.3687 - val_acc: 0.4950\n",
      "Epoch 38/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0855 - acc: 0.4977 - val_loss: 12.0264 - val_acc: 0.4950\n",
      "Epoch 39/40\n",
      "3008/3008 [==============================] - 0s 139us/step - loss: 12.0793 - acc: 0.4990 - val_loss: 12.3652 - val_acc: 0.5050\n",
      "Epoch 40/40\n",
      "3008/3008 [==============================] - 0s 138us/step - loss: 12.0983 - acc: 0.4983 - val_loss: 12.2362 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 63us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 45.4966 - acc: 0.5146 - val_loss: 13.8443 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.4857 - acc: 0.4980 - val_loss: 11.8821 - val_acc: 0.4950\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 11.9494 - acc: 0.5100 - val_loss: 11.8723 - val_acc: 0.5050\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 11.8897 - acc: 0.4960 - val_loss: 12.1826 - val_acc: 0.4950\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9800 - acc: 0.5100 - val_loss: 11.6692 - val_acc: 0.4950\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0281 - acc: 0.5027 - val_loss: 12.1980 - val_acc: 0.4950\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0502 - acc: 0.5047 - val_loss: 11.8854 - val_acc: 0.4950\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0624 - acc: 0.5100 - val_loss: 11.9697 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0558 - acc: 0.5100 - val_loss: 11.9403 - val_acc: 0.4950\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0602 - acc: 0.5100 - val_loss: 12.4502 - val_acc: 0.4950\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0677 - acc: 0.4947 - val_loss: 12.0420 - val_acc: 0.4950\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0708 - acc: 0.5100 - val_loss: 11.4399 - val_acc: 0.4950\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0557 - acc: 0.5100 - val_loss: 12.1604 - val_acc: 0.4950\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0736 - acc: 0.5100 - val_loss: 12.4243 - val_acc: 0.4950\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0731 - acc: 0.5100 - val_loss: 11.7939 - val_acc: 0.4950\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0613 - acc: 0.5100 - val_loss: 12.0712 - val_acc: 0.4950\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0618 - acc: 0.5100 - val_loss: 12.0844 - val_acc: 0.4950\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 115us/step - loss: 12.0795 - acc: 0.5100 - val_loss: 12.1951 - val_acc: 0.4950\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0729 - acc: 0.5100 - val_loss: 11.5186 - val_acc: 0.4950\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0612 - acc: 0.5100 - val_loss: 12.2931 - val_acc: 0.4950\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0687 - acc: 0.5000 - val_loss: 12.2449 - val_acc: 0.4950\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0722 - acc: 0.5100 - val_loss: 11.8798 - val_acc: 0.4950\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0640 - acc: 0.5100 - val_loss: 11.8308 - val_acc: 0.4950\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 12.0737 - acc: 0.4907 - val_loss: 12.3803 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0777 - acc: 0.5100 - val_loss: 12.1590 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0744 - acc: 0.5100 - val_loss: 11.6879 - val_acc: 0.4950\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0581 - acc: 0.5100 - val_loss: 11.8172 - val_acc: 0.4950\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0710 - acc: 0.5100 - val_loss: 12.3689 - val_acc: 0.4950\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0722 - acc: 0.5100 - val_loss: 11.9546 - val_acc: 0.4950\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0736 - acc: 0.5100 - val_loss: 12.0392 - val_acc: 0.4950\n",
      "1504/1504 [==============================] - 0s 51us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 45.4982 - acc: 0.5100 - val_loss: 13.7490 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.4199 - acc: 0.4970 - val_loss: 11.8664 - val_acc: 0.4950\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 11.8865 - acc: 0.5033 - val_loss: 11.7812 - val_acc: 0.5050\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.8255 - acc: 0.5066 - val_loss: 12.1547 - val_acc: 0.5050\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9160 - acc: 0.4970 - val_loss: 11.5749 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9583 - acc: 0.5090 - val_loss: 12.1540 - val_acc: 0.5050\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 11.9816 - acc: 0.4977 - val_loss: 11.7872 - val_acc: 0.5050\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 11.9889 - acc: 0.5030 - val_loss: 11.9560 - val_acc: 0.5050\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 11.9847 - acc: 0.5043 - val_loss: 11.8613 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 11.9892 - acc: 0.5090 - val_loss: 12.3802 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 11.9990 - acc: 0.4980 - val_loss: 11.9199 - val_acc: 0.5050\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.9999 - acc: 0.5100 - val_loss: 11.4650 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 11.9888 - acc: 0.4973 - val_loss: 12.0695 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0039 - acc: 0.5096 - val_loss: 12.3634 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0057 - acc: 0.5096 - val_loss: 11.7025 - val_acc: 0.5050\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 11.9931 - acc: 0.5063 - val_loss: 12.0519 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 11.9963 - acc: 0.4980 - val_loss: 11.9787 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0102 - acc: 0.5096 - val_loss: 12.1639 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 115us/step - loss: 12.0060 - acc: 0.5096 - val_loss: 11.4574 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.9931 - acc: 0.5096 - val_loss: 12.2478 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 115us/step - loss: 12.0022 - acc: 0.5096 - val_loss: 12.1284 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 113us/step - loss: 12.0035 - acc: 0.5096 - val_loss: 11.8600 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 11.9973 - acc: 0.5096 - val_loss: 11.7486 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 12.0046 - acc: 0.5096 - val_loss: 12.3389 - val_acc: 0.5050\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0105 - acc: 0.4977 - val_loss: 12.0470 - val_acc: 0.5050\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 12.0051 - acc: 0.5096 - val_loss: 11.6786 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 11.9922 - acc: 0.4917 - val_loss: 11.7346 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0019 - acc: 0.5096 - val_loss: 12.3363 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 114us/step - loss: 12.0052 - acc: 0.5096 - val_loss: 11.8607 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 116us/step - loss: 12.0046 - acc: 0.5096 - val_loss: 12.0120 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 50us/step\n",
      "Train on 3008 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "3008/3008 [==============================] - 8s 3ms/step - loss: 45.4896 - acc: 0.5110 - val_loss: 13.8713 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.5138 - acc: 0.4900 - val_loss: 11.9091 - val_acc: 0.4950\n",
      "Epoch 3/30\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 11.9786 - acc: 0.4860 - val_loss: 11.8989 - val_acc: 0.4950\n",
      "Epoch 4/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 11.9226 - acc: 0.4890 - val_loss: 12.2478 - val_acc: 0.4950\n",
      "Epoch 5/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0096 - acc: 0.5040 - val_loss: 11.6860 - val_acc: 0.5050\n",
      "Epoch 6/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0545 - acc: 0.4980 - val_loss: 12.2379 - val_acc: 0.5050\n",
      "Epoch 7/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0787 - acc: 0.4830 - val_loss: 11.8915 - val_acc: 0.4950\n",
      "Epoch 8/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0910 - acc: 0.4820 - val_loss: 11.9791 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.0833 - acc: 0.4963 - val_loss: 11.9675 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0872 - acc: 0.4997 - val_loss: 12.5153 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0952 - acc: 0.4983 - val_loss: 12.0653 - val_acc: 0.5050\n",
      "Epoch 12/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0970 - acc: 0.5020 - val_loss: 11.4483 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "3008/3008 [==============================] - 0s 118us/step - loss: 12.0821 - acc: 0.5013 - val_loss: 12.1680 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.1006 - acc: 0.4884 - val_loss: 12.4645 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.1002 - acc: 0.5023 - val_loss: 11.8170 - val_acc: 0.5050\n",
      "Epoch 16/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0869 - acc: 0.5023 - val_loss: 12.1130 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0871 - acc: 0.5023 - val_loss: 12.1005 - val_acc: 0.5050\n",
      "Epoch 18/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.1043 - acc: 0.4817 - val_loss: 12.2260 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0996 - acc: 0.4850 - val_loss: 11.5240 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "3008/3008 [==============================] - 0s 124us/step - loss: 12.0877 - acc: 0.5023 - val_loss: 12.3300 - val_acc: 0.5050\n",
      "Epoch 21/30\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.0947 - acc: 0.5023 - val_loss: 12.2702 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0972 - acc: 0.5023 - val_loss: 11.9118 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "3008/3008 [==============================] - 0s 121us/step - loss: 12.0892 - acc: 0.4943 - val_loss: 11.8369 - val_acc: 0.5050\n",
      "Epoch 24/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0993 - acc: 0.4864 - val_loss: 12.4205 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.1045 - acc: 0.4973 - val_loss: 12.1825 - val_acc: 0.5050\n",
      "Epoch 26/30\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.1005 - acc: 0.5023 - val_loss: 11.7011 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "3008/3008 [==============================] - 0s 122us/step - loss: 12.0840 - acc: 0.4904 - val_loss: 11.8223 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "3008/3008 [==============================] - 0s 120us/step - loss: 12.0960 - acc: 0.5023 - val_loss: 12.4133 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "3008/3008 [==============================] - 0s 117us/step - loss: 12.0982 - acc: 0.4777 - val_loss: 11.9773 - val_acc: 0.5050\n",
      "Epoch 30/30\n",
      "3008/3008 [==============================] - 0s 119us/step - loss: 12.0992 - acc: 0.4897 - val_loss: 12.0748 - val_acc: 0.5050\n",
      "1504/1504 [==============================] - 0s 51us/step\n",
      "Train on 4512 samples, validate on 903 samples\n",
      "Epoch 1/30\n",
      "4512/4512 [==============================] - 9s 2ms/step - loss: 23.2913 - acc: 0.4940 - val_loss: 11.7774 - val_acc: 0.4950\n",
      "Epoch 2/30\n",
      "4512/4512 [==============================] - 1s 226us/step - loss: 11.8937 - acc: 0.4905 - val_loss: 11.7204 - val_acc: 0.5050\n",
      "Epoch 3/30\n",
      "4512/4512 [==============================] - 1s 220us/step - loss: 11.9760 - acc: 0.4942 - val_loss: 11.9474 - val_acc: 0.5050\n",
      "Epoch 4/30\n",
      "4512/4512 [==============================] - 1s 222us/step - loss: 11.9953 - acc: 0.4984 - val_loss: 12.1201 - val_acc: 0.5050\n",
      "Epoch 5/30\n",
      "4512/4512 [==============================] - 1s 222us/step - loss: 11.9935 - acc: 0.5122 - val_loss: 12.3849 - val_acc: 0.4950\n",
      "Epoch 6/30\n",
      "4512/4512 [==============================] - 1s 222us/step - loss: 12.0021 - acc: 0.5007 - val_loss: 11.7375 - val_acc: 0.4950\n",
      "Epoch 7/30\n",
      "4512/4512 [==============================] - 1s 219us/step - loss: 11.9985 - acc: 0.4909 - val_loss: 11.8444 - val_acc: 0.4950\n",
      "Epoch 8/30\n",
      "4512/4512 [==============================] - 1s 219us/step - loss: 11.9997 - acc: 0.5024 - val_loss: 11.7415 - val_acc: 0.4950\n",
      "Epoch 9/30\n",
      "4512/4512 [==============================] - 1s 219us/step - loss: 12.0012 - acc: 0.5002 - val_loss: 12.0191 - val_acc: 0.5050\n",
      "Epoch 10/30\n",
      "4512/4512 [==============================] - 1s 220us/step - loss: 11.9972 - acc: 0.4860 - val_loss: 12.3388 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "4512/4512 [==============================] - 1s 221us/step - loss: 12.0050 - acc: 0.5007 - val_loss: 11.9828 - val_acc: 0.5050\n",
      "Epoch 12/30\n",
      "4512/4512 [==============================] - 1s 220us/step - loss: 11.9973 - acc: 0.4998 - val_loss: 12.0850 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "4512/4512 [==============================] - 1s 223us/step - loss: 12.0040 - acc: 0.4971 - val_loss: 11.5943 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      "4512/4512 [==============================] - 1s 221us/step - loss: 11.9995 - acc: 0.4918 - val_loss: 12.1402 - val_acc: 0.5050\n",
      "Epoch 15/30\n",
      "4512/4512 [==============================] - 1s 223us/step - loss: 12.0016 - acc: 0.4927 - val_loss: 11.7693 - val_acc: 0.4950\n",
      "Epoch 16/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0034 - acc: 0.4993 - val_loss: 12.2320 - val_acc: 0.5050\n",
      "Epoch 17/30\n",
      "4512/4512 [==============================] - 1s 219us/step - loss: 11.9978 - acc: 0.5002 - val_loss: 12.1989 - val_acc: 0.4950\n",
      "Epoch 18/30\n",
      "4512/4512 [==============================] - 1s 226us/step - loss: 12.0052 - acc: 0.5029 - val_loss: 11.9828 - val_acc: 0.4950\n",
      "Epoch 19/30\n",
      "4512/4512 [==============================] - 1s 224us/step - loss: 11.9980 - acc: 0.5047 - val_loss: 12.1927 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0044 - acc: 0.4896 - val_loss: 11.5530 - val_acc: 0.4950\n",
      "Epoch 21/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0002 - acc: 0.4945 - val_loss: 12.1408 - val_acc: 0.5050\n",
      "Epoch 22/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0017 - acc: 0.5002 - val_loss: 11.9161 - val_acc: 0.5050\n",
      "Epoch 23/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0046 - acc: 0.4993 - val_loss: 12.1980 - val_acc: 0.4950\n",
      "Epoch 24/30\n",
      "4512/4512 [==============================] - 1s 224us/step - loss: 11.9990 - acc: 0.5029 - val_loss: 12.3119 - val_acc: 0.4950\n",
      "Epoch 25/30\n",
      "4512/4512 [==============================] - 1s 226us/step - loss: 12.0057 - acc: 0.4887 - val_loss: 11.8942 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "4512/4512 [==============================] - 1s 222us/step - loss: 11.9986 - acc: 0.4865 - val_loss: 12.0332 - val_acc: 0.5050\n",
      "Epoch 27/30\n",
      "4512/4512 [==============================] - 1s 225us/step - loss: 12.0041 - acc: 0.4967 - val_loss: 11.1970 - val_acc: 0.5050\n",
      "Epoch 28/30\n",
      "4512/4512 [==============================] - 1s 221us/step - loss: 11.9993 - acc: 0.4945 - val_loss: 12.3122 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "4512/4512 [==============================] - 1s 226us/step - loss: 12.0024 - acc: 0.4931 - val_loss: 12.0914 - val_acc: 0.4950\n",
      "Epoch 30/30\n",
      "4512/4512 [==============================] - 1s 221us/step - loss: 12.0040 - acc: 0.5007 - val_loss: 12.2059 - val_acc: 0.5050\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(\n",
    "        epochs= [15,20,30,40],\n",
    "        batch_size= [30, 40, 50, 60] ) \n",
    "\n",
    "grid_NN = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid ) \n",
    "grid_NN = grid_NN.fit(scaled_train, y_train, validation_data=(scaled_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "uCLYJszj5Rh_",
    "outputId": "edd47850-d9dc-40dc-c445-90b82c525cc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f615927b198>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'batch_size': [10, 20, 30, 50, 80],\n",
       "                                        'epochs': [10, 20]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"check_point = ModelCheckpoint(file_path, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAtsf2O53c4B"
   },
   "outputs": [],
   "source": [
    "def summary_grid_NN(grid_fit):\n",
    "    print('best score on validation: ', grid_fit.best_score_)   #data combinazione parametri, è accuratezza media valutata sulle K cross validation. quindi è sempre una validation accuracy\n",
    "    print('best param combination: ', grid_fit.best_params_)   #'C': 0.357\n",
    "    print('predictions', grid_fit.predict(scaled_test))\n",
    "    #posso predire scaled_test, quello con tutte le 369 feauters: lui da solo ne toglierà alcune con lasso, è gia incorporato\n",
    "    print('train accuracy TP+TN/tot is {}'.format(accuracy_score(Y_train, grid_fit.predict(scaled_train))))\n",
    "    print('test accuracy TP+TN/tot is {}'.format(accuracy_score(Y_test, grid_fit.predict(scaled_test))))\n",
    "    #print(\"cross validation accuracy sul train è: \", cross_val_score(estimator = grid_fit, X = scaled_train, y = Y_train, cv = 5, verbose=0).mean())\n",
    "    print(classification_report(Y_test, grid_fit.best_estimator_.predict(scaled_test)))\n",
    "    print(confusion_matrix(Y_test, grid_fit.predict(scaled_test)))\n",
    "    \"\"\"ROC CURVE SCORES\"\"\"\n",
    "    print('roc curve train is {a}, roc test is {b}'.format(a= roc_auc_score(Y_train, grid_fit.predict(scaled_train)), b= roc_auc_score(Y_test, grid_fit.predict(scaled_test))))\n",
    "    print('F1 SCORE train is {a}, F1 SCORE test is {b}'.format(a= f1_score(Y_train, grid_fit.predict(scaled_train)), b= f1_score(Y_test, grid_fit.predict(scaled_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "6nxmHYOFOnRe",
    "outputId": "b3a6ea1c-5494-4d3f-922c-9c624570e0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score on validation:  0.5006649018139809\n",
      "best param combination:  {'epochs': 30, 'batch_size': 30}\n",
      "1504/1504 [==============================] - 3s 2ms/step\n",
      "predictions [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "4512/4512 [==============================] - 0s 75us/step\n",
      "train accuracy TP+TN/tot is 0.5006648936170213\n",
      "1504/1504 [==============================] - 0s 77us/step\n",
      "test accuracy TP+TN/tot is 0.49800531914893614\n",
      "1504/1504 [==============================] - 0s 75us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.66       749\n",
      "           1       0.00      0.00      0.00       755\n",
      "\n",
      "    accuracy                           0.50      1504\n",
      "   macro avg       0.25      0.50      0.33      1504\n",
      "weighted avg       0.25      0.50      0.33      1504\n",
      "\n",
      "1504/1504 [==============================] - 0s 77us/step\n",
      "[[749   0]\n",
      " [755   0]]\n",
      " 750/4512 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/4512 [==============================] - 0s 73us/step\n",
      "1504/1504 [==============================] - 0s 75us/step\n",
      "roc curve train is 0.5, roc test is 0.5\n",
      "4512/4512 [==============================] - 0s 76us/step\n",
      "1504/1504 [==============================] - 0s 76us/step\n",
      "F1 SCORE train is 0.0, F1 SCORE test is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "summary_grid_NN(grid_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "id": "cf-AGxrs3c4D",
    "outputId": "004b3f77-1b99-4e3b-c0ed-76e5e790a54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.714761, using {'epochs': 20, 'batch_size': 60}\n",
      "0.702349 (0.005217) with: {'epochs': 40, 'batch_size': 60}\n",
      "0.699468 (0.014084) with: {'epochs': 40, 'batch_size': 50}\n",
      "0.681073 (0.028085) with: {'epochs': 15, 'batch_size': 50}\n",
      "0.712544 (0.006545) with: {'epochs': 15, 'batch_size': 40}\n",
      "0.552527 (0.088590) with: {'epochs': 40, 'batch_size': 30}\n",
      "0.707890 (0.003087) with: {'epochs': 20, 'batch_size': 40}\n",
      "0.711658 (0.013123) with: {'epochs': 20, 'batch_size': 50}\n",
      "0.673316 (0.024844) with: {'epochs': 30, 'batch_size': 40}\n",
      "0.652704 (0.056690) with: {'epochs': 20, 'batch_size': 30}\n",
      "0.714761 (0.007463) with: {'epochs': 20, 'batch_size': 60}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %f, using %s\" % (grid_NN.best_score_, grid_NN.best_params_))\n",
    "means = grid_NN.cv_results_['mean_test_score']\n",
    "stds = grid_NN.cv_results_['std_test_score']\n",
    "params = grid_NN.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mF5yM2-IJn52"
   },
   "source": [
    "### FINALLY, I CREATE A FUNCTION AND IMPLEMENT A GRIDSEARCH (RANDOMIZED TO SPEED UP EFFICIENCY) WHICH CHOOSE NOT ONLY THE BEST PARAMETERS BUT EVEN THE BEST ARCHITECTURE (NUMBER OF LAYERS, NUMBER OF NODES FOR EACH LAYER..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tued3Rc63c4E"
   },
   "outputs": [],
   "source": [
    "def create_model( nl1=1, nl2=1,  nl3=1, \n",
    "                 nn1=1000, nn2=500, nn3 = 200, lr=0.01, decay=0., l1=0.01, l2=0.01,\n",
    "                act = 'relu', dropout=0.2, input_shape= len(X_train_scaled.columns), output_shape=1):   \n",
    "  \n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999,  decay=decay)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    # for the firt layer we need to specify the input dimensions\n",
    "    first=True\n",
    "    \n",
    "    for i in range(nl1):\n",
    "        if first:\n",
    "            model.add(Dense(nn1, input_dim= len(X_train_scaled.columns), activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    for i in range(nl2):\n",
    "        if first:\n",
    "            model.add(Dense(nn2, input_dim= len(X_train_scaled.columns), activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    for i in range(nl3):\n",
    "        if first:\n",
    "            model.add(Dense(nn3, input_dim= len(X_train_scaled.columns), activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    model.add(Dense(1, activation='sigmoid'))   #anche softmax va bene, ma sigmoid dà molta più accuratezza. Penso perchè sigmoid è caso binario, softmax più classi\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRsTVFcCCEYv"
   },
   "outputs": [],
   "source": [
    "# learning algorithm parameters\n",
    "lr=[1e-2, 1e-3, 1e-4]  #sarebbero 0.01, 0.001 e 0.0001\n",
    "decay=[1e-6,1e-9,0]\n",
    "\n",
    "# activation\n",
    "activation=['relu', 'sigmoid']\n",
    "\n",
    "# numbers of layers\n",
    "nl1 = [0,1,2,3]\n",
    "nl2 = [0,1,2,3]\n",
    "nl3 = [0,1,2]\n",
    "\n",
    "# neurons in each layer\n",
    "nn1=[100,200,400]\n",
    "nn2=[50,100,200]\n",
    "nn3=[20,50,100]\n",
    "\n",
    "# dropout and regularisation\n",
    "dropout = [0.1, 0.2]\n",
    "l1 = [0, 0.01, 0.001, 0.0001]\n",
    "l2 = [0, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# dictionary summary\n",
    "param_grid_fin = dict(\n",
    "                    nl1=nl1, nl2= nl2, nl3= nl3, nn1= nn1, nn2= nn2, nn3=nn3,\n",
    "                    act=activation, l1=l1, l2=l2, lr=lr, decay=decay, dropout=dropout,\n",
    "                    epochs= [7,11,15], batch_size= [20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N2BZGXQYFy6H",
    "outputId": "2f90888e-ec16-4420-9adb-02ece07efb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.1594 - acc: 0.6240 - val_loss: 2.1285 - val_acc: 0.6915\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 2.0744 - acc: 0.6825 - val_loss: 2.0484 - val_acc: 0.7021\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 1.9984 - acc: 0.6941 - val_loss: 1.9972 - val_acc: 0.7108\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 318us/step - loss: 1.9463 - acc: 0.7141 - val_loss: 1.9450 - val_acc: 0.7214\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 324us/step - loss: 1.8978 - acc: 0.7291 - val_loss: 1.9134 - val_acc: 0.7287\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 1.8525 - acc: 0.7374 - val_loss: 1.8781 - val_acc: 0.7387\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 1.8173 - acc: 0.7477 - val_loss: 1.8457 - val_acc: 0.7367\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 324us/step - loss: 1.7890 - acc: 0.7473 - val_loss: 1.8305 - val_acc: 0.7407\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 323us/step - loss: 1.7612 - acc: 0.7583 - val_loss: 1.8059 - val_acc: 0.7367\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 1.7246 - acc: 0.7606 - val_loss: 1.7760 - val_acc: 0.7434\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 1.6994 - acc: 0.7590 - val_loss: 1.7585 - val_acc: 0.7394\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.1691 - acc: 0.5924 - val_loss: 2.1377 - val_acc: 0.7035\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 2.0859 - acc: 0.6732 - val_loss: 2.0519 - val_acc: 0.7101\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 315us/step - loss: 2.0123 - acc: 0.6895 - val_loss: 1.9963 - val_acc: 0.7148\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 1.9490 - acc: 0.7068 - val_loss: 1.9566 - val_acc: 0.7234\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 318us/step - loss: 1.8979 - acc: 0.7324 - val_loss: 1.9255 - val_acc: 0.7281\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 317us/step - loss: 1.8534 - acc: 0.7410 - val_loss: 1.9118 - val_acc: 0.7227\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 318us/step - loss: 1.8218 - acc: 0.7493 - val_loss: 1.8784 - val_acc: 0.7314\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 1.7972 - acc: 0.7487 - val_loss: 1.8460 - val_acc: 0.7340\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 324us/step - loss: 1.7597 - acc: 0.7550 - val_loss: 1.8350 - val_acc: 0.7301\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 1.7336 - acc: 0.7623 - val_loss: 1.7932 - val_acc: 0.7427\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 1.7079 - acc: 0.7583 - val_loss: 1.7778 - val_acc: 0.7467\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.1640 - acc: 0.6094 - val_loss: 2.1259 - val_acc: 0.6902\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 2.0711 - acc: 0.6925 - val_loss: 2.0494 - val_acc: 0.6995\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 315us/step - loss: 1.9969 - acc: 0.6938 - val_loss: 1.9970 - val_acc: 0.7061\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 316us/step - loss: 1.9345 - acc: 0.7171 - val_loss: 1.9631 - val_acc: 0.7088\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 316us/step - loss: 1.8939 - acc: 0.7297 - val_loss: 1.9236 - val_acc: 0.7168\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 319us/step - loss: 1.8531 - acc: 0.7314 - val_loss: 1.8892 - val_acc: 0.7427\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 320us/step - loss: 1.8212 - acc: 0.7350 - val_loss: 1.8618 - val_acc: 0.7387\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 320us/step - loss: 1.7850 - acc: 0.7463 - val_loss: 1.8336 - val_acc: 0.7440\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 1.7602 - acc: 0.7463 - val_loss: 1.8035 - val_acc: 0.7520\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 318us/step - loss: 1.7241 - acc: 0.7553 - val_loss: 1.7985 - val_acc: 0.7414\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 324us/step - loss: 1.7001 - acc: 0.7623 - val_loss: 1.7673 - val_acc: 0.7473\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.4953 - acc: 0.5096 - val_loss: 0.9442 - val_acc: 0.5020\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 272us/step - loss: 0.9394 - acc: 0.5070 - val_loss: 0.9251 - val_acc: 0.5020\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 271us/step - loss: 0.9387 - acc: 0.4934 - val_loss: 0.9356 - val_acc: 0.5020\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 270us/step - loss: 0.9295 - acc: 0.5043 - val_loss: 0.9130 - val_acc: 0.5020\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 268us/step - loss: 0.9211 - acc: 0.5000 - val_loss: 0.9106 - val_acc: 0.5020\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 272us/step - loss: 0.9209 - acc: 0.5000 - val_loss: 0.9132 - val_acc: 0.5020\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 271us/step - loss: 0.9233 - acc: 0.5043 - val_loss: 0.9201 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.4816 - acc: 0.5100 - val_loss: 0.9422 - val_acc: 0.5020\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 265us/step - loss: 0.9395 - acc: 0.4963 - val_loss: 0.9416 - val_acc: 0.4980\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 267us/step - loss: 0.9354 - acc: 0.4953 - val_loss: 0.9179 - val_acc: 0.5020\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 269us/step - loss: 0.9323 - acc: 0.4940 - val_loss: 0.9201 - val_acc: 0.5020\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 266us/step - loss: 0.9236 - acc: 0.4907 - val_loss: 0.9146 - val_acc: 0.4980\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 271us/step - loss: 0.9217 - acc: 0.4884 - val_loss: 0.9166 - val_acc: 0.5020\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 268us/step - loss: 0.9290 - acc: 0.5007 - val_loss: 0.9175 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 10s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 27s 9ms/step - loss: 2.4936 - acc: 0.4747 - val_loss: 0.9422 - val_acc: 0.4980\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 276us/step - loss: 0.9442 - acc: 0.4980 - val_loss: 0.9348 - val_acc: 0.5020\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 274us/step - loss: 0.9394 - acc: 0.4977 - val_loss: 0.9289 - val_acc: 0.5020\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 271us/step - loss: 0.9342 - acc: 0.5106 - val_loss: 0.9167 - val_acc: 0.4980\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 278us/step - loss: 0.9246 - acc: 0.4947 - val_loss: 0.9146 - val_acc: 0.4980\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 269us/step - loss: 0.9251 - acc: 0.5023 - val_loss: 0.9182 - val_acc: 0.5020\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 270us/step - loss: 0.9184 - acc: 0.5073 - val_loss: 0.9158 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 28s 9ms/step - loss: 3.8825 - acc: 0.4887 - val_loss: 1.3924 - val_acc: 0.4980\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 259us/step - loss: 1.3540 - acc: 0.4970 - val_loss: 1.3579 - val_acc: 0.5020\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3595 - acc: 0.4894 - val_loss: 1.3422 - val_acc: 0.5020\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3445 - acc: 0.4914 - val_loss: 1.3382 - val_acc: 0.5020\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 256us/step - loss: 1.3413 - acc: 0.4993 - val_loss: 1.3376 - val_acc: 0.5020\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 255us/step - loss: 1.3314 - acc: 0.5116 - val_loss: 1.3447 - val_acc: 0.5020\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 258us/step - loss: 1.3305 - acc: 0.5083 - val_loss: 1.3221 - val_acc: 0.5020\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 260us/step - loss: 1.3377 - acc: 0.4960 - val_loss: 1.3164 - val_acc: 0.5020\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 258us/step - loss: 1.3365 - acc: 0.4894 - val_loss: 1.3095 - val_acc: 0.5020\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 255us/step - loss: 1.3311 - acc: 0.4877 - val_loss: 1.3467 - val_acc: 0.5020\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 259us/step - loss: 1.3334 - acc: 0.4904 - val_loss: 1.3412 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 10s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 28s 9ms/step - loss: 3.8261 - acc: 0.4990 - val_loss: 1.3882 - val_acc: 0.5020\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3787 - acc: 0.4914 - val_loss: 1.3468 - val_acc: 0.5020\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 260us/step - loss: 1.3495 - acc: 0.4834 - val_loss: 1.3453 - val_acc: 0.4980\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 261us/step - loss: 1.3344 - acc: 0.5126 - val_loss: 1.3425 - val_acc: 0.4980\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 259us/step - loss: 1.3421 - acc: 0.4877 - val_loss: 1.3266 - val_acc: 0.4980\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 264us/step - loss: 1.3329 - acc: 0.5047 - val_loss: 1.3373 - val_acc: 0.4980\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3356 - acc: 0.4934 - val_loss: 1.3228 - val_acc: 0.5020\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 269us/step - loss: 1.3306 - acc: 0.5040 - val_loss: 1.3357 - val_acc: 0.5020\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 267us/step - loss: 1.3479 - acc: 0.5023 - val_loss: 1.3140 - val_acc: 0.5020\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 261us/step - loss: 1.3353 - acc: 0.5073 - val_loss: 1.3598 - val_acc: 0.4980\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3320 - acc: 0.5007 - val_loss: 1.3429 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 28s 9ms/step - loss: 3.8719 - acc: 0.4950 - val_loss: 1.4213 - val_acc: 0.4980\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 259us/step - loss: 1.3590 - acc: 0.4983 - val_loss: 1.3387 - val_acc: 0.4980\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 260us/step - loss: 1.3428 - acc: 0.4917 - val_loss: 1.3482 - val_acc: 0.5020\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 257us/step - loss: 1.3449 - acc: 0.4884 - val_loss: 1.3439 - val_acc: 0.5020\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 254us/step - loss: 1.3523 - acc: 0.5063 - val_loss: 1.3267 - val_acc: 0.5020\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 260us/step - loss: 1.3352 - acc: 0.4957 - val_loss: 1.3401 - val_acc: 0.5020\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 255us/step - loss: 1.3346 - acc: 0.4977 - val_loss: 1.3490 - val_acc: 0.5020\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 260us/step - loss: 1.3385 - acc: 0.5073 - val_loss: 1.3161 - val_acc: 0.4980\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 255us/step - loss: 1.3348 - acc: 0.5030 - val_loss: 1.3080 - val_acc: 0.4980\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 256us/step - loss: 1.3334 - acc: 0.5066 - val_loss: 1.3498 - val_acc: 0.4980\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 259us/step - loss: 1.3358 - acc: 0.4890 - val_loss: 1.3418 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 28s 9ms/step - loss: 0.7058 - acc: 0.6832 - val_loss: 0.6110 - val_acc: 0.7227\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 354us/step - loss: 0.5588 - acc: 0.7267 - val_loss: 0.5896 - val_acc: 0.7201\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 354us/step - loss: 0.5500 - acc: 0.7204 - val_loss: 0.5910 - val_acc: 0.7360\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 357us/step - loss: 0.5433 - acc: 0.7344 - val_loss: 0.6058 - val_acc: 0.7334\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 358us/step - loss: 0.5383 - acc: 0.7473 - val_loss: 0.5698 - val_acc: 0.7301\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.5345 - acc: 0.7407 - val_loss: 0.5923 - val_acc: 0.7434\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 353us/step - loss: 0.5268 - acc: 0.7437 - val_loss: 0.5683 - val_acc: 0.7354\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 28s 9ms/step - loss: 0.6863 - acc: 0.6922 - val_loss: 0.5540 - val_acc: 0.7327\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 355us/step - loss: 0.5688 - acc: 0.7294 - val_loss: 0.6026 - val_acc: 0.7254\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 351us/step - loss: 0.5551 - acc: 0.7274 - val_loss: 0.5575 - val_acc: 0.7081\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 356us/step - loss: 0.5595 - acc: 0.7317 - val_loss: 0.5959 - val_acc: 0.7394\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 353us/step - loss: 0.5469 - acc: 0.7267 - val_loss: 0.6103 - val_acc: 0.7374\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 351us/step - loss: 0.5556 - acc: 0.7314 - val_loss: 0.5868 - val_acc: 0.7247\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 355us/step - loss: 0.5374 - acc: 0.7340 - val_loss: 0.5618 - val_acc: 0.7227\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 29s 10ms/step - loss: 0.6632 - acc: 0.6855 - val_loss: 0.6190 - val_acc: 0.7207\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 350us/step - loss: 0.5767 - acc: 0.7094 - val_loss: 0.6002 - val_acc: 0.7227\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.5610 - acc: 0.7168 - val_loss: 0.6167 - val_acc: 0.7314\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.5503 - acc: 0.7374 - val_loss: 0.5796 - val_acc: 0.7128\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 349us/step - loss: 0.5385 - acc: 0.7334 - val_loss: 0.5851 - val_acc: 0.7181\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 356us/step - loss: 0.5697 - acc: 0.7257 - val_loss: 0.6005 - val_acc: 0.7154\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.5380 - acc: 0.7377 - val_loss: 0.5938 - val_acc: 0.7394\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 30s 10ms/step - loss: 0.8876 - acc: 0.4927 - val_loss: 0.7447 - val_acc: 0.4980\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 649us/step - loss: 0.7539 - acc: 0.5120 - val_loss: 0.7704 - val_acc: 0.5020\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7445 - acc: 0.5063 - val_loss: 0.7385 - val_acc: 0.5020\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 637us/step - loss: 0.7376 - acc: 0.5047 - val_loss: 0.7288 - val_acc: 0.4980\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 642us/step - loss: 0.7318 - acc: 0.5027 - val_loss: 0.7567 - val_acc: 0.5020\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 645us/step - loss: 0.7282 - acc: 0.5146 - val_loss: 0.7234 - val_acc: 0.4980\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 648us/step - loss: 0.7278 - acc: 0.4731 - val_loss: 0.7409 - val_acc: 0.5020\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 651us/step - loss: 0.7295 - acc: 0.4914 - val_loss: 0.7232 - val_acc: 0.4980\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 652us/step - loss: 0.7267 - acc: 0.4950 - val_loss: 0.7250 - val_acc: 0.5020\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 648us/step - loss: 0.7240 - acc: 0.4977 - val_loss: 0.7318 - val_acc: 0.5020\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 656us/step - loss: 0.7251 - acc: 0.4877 - val_loss: 0.7221 - val_acc: 0.4980\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 650us/step - loss: 0.7234 - acc: 0.5000 - val_loss: 0.7239 - val_acc: 0.5020\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 646us/step - loss: 0.7245 - acc: 0.4914 - val_loss: 0.7239 - val_acc: 0.5020\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 642us/step - loss: 0.7231 - acc: 0.5030 - val_loss: 0.7233 - val_acc: 0.5020\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 648us/step - loss: 0.7248 - acc: 0.4980 - val_loss: 0.7205 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 30s 10ms/step - loss: 0.8684 - acc: 0.5166 - val_loss: 0.7579 - val_acc: 0.4980\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 644us/step - loss: 0.7603 - acc: 0.4927 - val_loss: 0.7361 - val_acc: 0.4980\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 637us/step - loss: 0.7396 - acc: 0.5043 - val_loss: 0.7399 - val_acc: 0.4980\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 629us/step - loss: 0.7302 - acc: 0.4950 - val_loss: 0.7399 - val_acc: 0.4980\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 636us/step - loss: 0.7318 - acc: 0.4934 - val_loss: 0.7294 - val_acc: 0.4980\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 636us/step - loss: 0.7289 - acc: 0.4860 - val_loss: 0.7318 - val_acc: 0.4980\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 640us/step - loss: 0.7262 - acc: 0.4827 - val_loss: 0.7222 - val_acc: 0.4980\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 640us/step - loss: 0.7277 - acc: 0.4990 - val_loss: 0.7254 - val_acc: 0.5020\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 632us/step - loss: 0.7255 - acc: 0.4857 - val_loss: 0.7218 - val_acc: 0.5020\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7240 - acc: 0.5066 - val_loss: 0.7251 - val_acc: 0.4980\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 637us/step - loss: 0.7264 - acc: 0.5103 - val_loss: 0.7277 - val_acc: 0.4980\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 629us/step - loss: 0.7236 - acc: 0.5030 - val_loss: 0.7266 - val_acc: 0.4980\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 642us/step - loss: 0.7252 - acc: 0.4943 - val_loss: 0.7267 - val_acc: 0.4980\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7262 - acc: 0.4820 - val_loss: 0.7216 - val_acc: 0.4980\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7258 - acc: 0.4977 - val_loss: 0.7205 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 30s 10ms/step - loss: 0.8737 - acc: 0.5043 - val_loss: 0.7454 - val_acc: 0.5020\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.7451 - acc: 0.4950 - val_loss: 0.7373 - val_acc: 0.4980\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 634us/step - loss: 0.7367 - acc: 0.4894 - val_loss: 0.7327 - val_acc: 0.4980\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 644us/step - loss: 0.7327 - acc: 0.5133 - val_loss: 0.7308 - val_acc: 0.4980\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 635us/step - loss: 0.7263 - acc: 0.5143 - val_loss: 0.7263 - val_acc: 0.5020\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 637us/step - loss: 0.7288 - acc: 0.4967 - val_loss: 0.7270 - val_acc: 0.4980\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 634us/step - loss: 0.7286 - acc: 0.4947 - val_loss: 0.7230 - val_acc: 0.5020\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7252 - acc: 0.4894 - val_loss: 0.7245 - val_acc: 0.5020\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 641us/step - loss: 0.7321 - acc: 0.5037 - val_loss: 0.7273 - val_acc: 0.4980\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 645us/step - loss: 0.7261 - acc: 0.5113 - val_loss: 0.7297 - val_acc: 0.5020\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 643us/step - loss: 0.7256 - acc: 0.4950 - val_loss: 0.7200 - val_acc: 0.5020\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 643us/step - loss: 0.7247 - acc: 0.4887 - val_loss: 0.7208 - val_acc: 0.4980\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 646us/step - loss: 0.7230 - acc: 0.4884 - val_loss: 0.7238 - val_acc: 0.4980\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 647us/step - loss: 0.7254 - acc: 0.4957 - val_loss: 0.7223 - val_acc: 0.4980\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 648us/step - loss: 0.7257 - acc: 0.4963 - val_loss: 0.7211 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 29s 10ms/step - loss: 0.7185 - acc: 0.5352 - val_loss: 0.6770 - val_acc: 0.5113\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 343us/step - loss: 0.6738 - acc: 0.5868 - val_loss: 0.6424 - val_acc: 0.6602\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 344us/step - loss: 0.6531 - acc: 0.6267 - val_loss: 0.6215 - val_acc: 0.7041\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 344us/step - loss: 0.6347 - acc: 0.6486 - val_loss: 0.6081 - val_acc: 0.7021\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 347us/step - loss: 0.6242 - acc: 0.6602 - val_loss: 0.5989 - val_acc: 0.6995\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 347us/step - loss: 0.6135 - acc: 0.6842 - val_loss: 0.5923 - val_acc: 0.7015\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 343us/step - loss: 0.6100 - acc: 0.6898 - val_loss: 0.5876 - val_acc: 0.7061\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 29s 10ms/step - loss: 0.6963 - acc: 0.5372 - val_loss: 0.6690 - val_acc: 0.6815\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.6705 - acc: 0.5811 - val_loss: 0.6421 - val_acc: 0.7008\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 353us/step - loss: 0.6489 - acc: 0.6277 - val_loss: 0.6241 - val_acc: 0.7015\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 345us/step - loss: 0.6294 - acc: 0.6579 - val_loss: 0.6108 - val_acc: 0.6995\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 351us/step - loss: 0.6225 - acc: 0.6686 - val_loss: 0.6017 - val_acc: 0.7015\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 352us/step - loss: 0.6086 - acc: 0.6795 - val_loss: 0.5945 - val_acc: 0.7015\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 348us/step - loss: 0.6023 - acc: 0.6912 - val_loss: 0.5890 - val_acc: 0.7035\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 29s 10ms/step - loss: 1.0689 - acc: 0.4977 - val_loss: 0.9866 - val_acc: 0.5020\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 354us/step - loss: 0.9409 - acc: 0.4977 - val_loss: 0.8735 - val_acc: 0.5020\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 354us/step - loss: 0.8350 - acc: 0.4990 - val_loss: 0.7884 - val_acc: 0.5020\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 356us/step - loss: 0.7685 - acc: 0.5040 - val_loss: 0.7278 - val_acc: 0.5027\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 350us/step - loss: 0.7193 - acc: 0.5316 - val_loss: 0.6846 - val_acc: 0.5206\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 350us/step - loss: 0.6782 - acc: 0.6017 - val_loss: 0.6552 - val_acc: 0.6370\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 345us/step - loss: 0.6535 - acc: 0.6336 - val_loss: 0.6350 - val_acc: 0.6875\n",
      "1504/1504 [==============================] - 11s 7ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 0.7610 - acc: 0.5110 - val_loss: 0.7140 - val_acc: 0.5020\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.7161 - acc: 0.5153 - val_loss: 0.7079 - val_acc: 0.4980\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 632us/step - loss: 0.7056 - acc: 0.5073 - val_loss: 0.7039 - val_acc: 0.4980\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 625us/step - loss: 0.7068 - acc: 0.5010 - val_loss: 0.6990 - val_acc: 0.4980\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 627us/step - loss: 0.7031 - acc: 0.5213 - val_loss: 0.7038 - val_acc: 0.4980\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.7023 - acc: 0.5070 - val_loss: 0.7009 - val_acc: 0.5020\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.7005 - acc: 0.5007 - val_loss: 0.7117 - val_acc: 0.5020\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 630us/step - loss: 0.7014 - acc: 0.4904 - val_loss: 0.7016 - val_acc: 0.4980\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 633us/step - loss: 0.7032 - acc: 0.4807 - val_loss: 0.6982 - val_acc: 0.4980\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 627us/step - loss: 0.7040 - acc: 0.4877 - val_loss: 0.6961 - val_acc: 0.5020\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 622us/step - loss: 0.7013 - acc: 0.5173 - val_loss: 0.6971 - val_acc: 0.5020\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 630us/step - loss: 0.6990 - acc: 0.5189 - val_loss: 0.6970 - val_acc: 0.4980\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 629us/step - loss: 0.7000 - acc: 0.4927 - val_loss: 0.6981 - val_acc: 0.5020\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 632us/step - loss: 0.6992 - acc: 0.4983 - val_loss: 0.6996 - val_acc: 0.5020\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.7011 - acc: 0.4963 - val_loss: 0.6976 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 11s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 0.7509 - acc: 0.5037 - val_loss: 0.7215 - val_acc: 0.5020\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 645us/step - loss: 0.7149 - acc: 0.5047 - val_loss: 0.7158 - val_acc: 0.4980\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 646us/step - loss: 0.7103 - acc: 0.5010 - val_loss: 0.7045 - val_acc: 0.4980\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 644us/step - loss: 0.7085 - acc: 0.4927 - val_loss: 0.7029 - val_acc: 0.5020\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 629us/step - loss: 0.7028 - acc: 0.5146 - val_loss: 0.6984 - val_acc: 0.4980\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 634us/step - loss: 0.7021 - acc: 0.5047 - val_loss: 0.6977 - val_acc: 0.5020\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.6999 - acc: 0.4864 - val_loss: 0.7024 - val_acc: 0.4980\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 638us/step - loss: 0.7015 - acc: 0.4830 - val_loss: 0.6986 - val_acc: 0.5020\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.6990 - acc: 0.5007 - val_loss: 0.6967 - val_acc: 0.4980\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 633us/step - loss: 0.7015 - acc: 0.5199 - val_loss: 0.6991 - val_acc: 0.4980\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 632us/step - loss: 0.7047 - acc: 0.4973 - val_loss: 0.6985 - val_acc: 0.5020\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 635us/step - loss: 0.7014 - acc: 0.5013 - val_loss: 0.6979 - val_acc: 0.4980\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 630us/step - loss: 0.7000 - acc: 0.4983 - val_loss: 0.6970 - val_acc: 0.4980\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 631us/step - loss: 0.6974 - acc: 0.5136 - val_loss: 0.6961 - val_acc: 0.5020\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 639us/step - loss: 0.6991 - acc: 0.5080 - val_loss: 0.6991 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 11s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/15\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 0.7577 - acc: 0.5070 - val_loss: 0.7388 - val_acc: 0.5020\n",
      "Epoch 2/15\n",
      "3008/3008 [==============================] - 2s 645us/step - loss: 0.7192 - acc: 0.4904 - val_loss: 0.7217 - val_acc: 0.5020\n",
      "Epoch 3/15\n",
      "3008/3008 [==============================] - 2s 646us/step - loss: 0.7083 - acc: 0.4970 - val_loss: 0.7000 - val_acc: 0.5020\n",
      "Epoch 4/15\n",
      "3008/3008 [==============================] - 2s 649us/step - loss: 0.7075 - acc: 0.5017 - val_loss: 0.7067 - val_acc: 0.5020\n",
      "Epoch 5/15\n",
      "3008/3008 [==============================] - 2s 654us/step - loss: 0.7067 - acc: 0.5153 - val_loss: 0.7216 - val_acc: 0.4980\n",
      "Epoch 6/15\n",
      "3008/3008 [==============================] - 2s 662us/step - loss: 0.7052 - acc: 0.5033 - val_loss: 0.7061 - val_acc: 0.4980\n",
      "Epoch 7/15\n",
      "3008/3008 [==============================] - 2s 671us/step - loss: 0.6998 - acc: 0.5037 - val_loss: 0.6984 - val_acc: 0.4980\n",
      "Epoch 8/15\n",
      "3008/3008 [==============================] - 2s 656us/step - loss: 0.7023 - acc: 0.5033 - val_loss: 0.6966 - val_acc: 0.4980\n",
      "Epoch 9/15\n",
      "3008/3008 [==============================] - 2s 656us/step - loss: 0.7017 - acc: 0.5103 - val_loss: 0.7113 - val_acc: 0.4980\n",
      "Epoch 10/15\n",
      "3008/3008 [==============================] - 2s 651us/step - loss: 0.6999 - acc: 0.5066 - val_loss: 0.6995 - val_acc: 0.5020\n",
      "Epoch 11/15\n",
      "3008/3008 [==============================] - 2s 649us/step - loss: 0.7026 - acc: 0.4980 - val_loss: 0.6993 - val_acc: 0.4980\n",
      "Epoch 12/15\n",
      "3008/3008 [==============================] - 2s 649us/step - loss: 0.7012 - acc: 0.5023 - val_loss: 0.7017 - val_acc: 0.4980\n",
      "Epoch 13/15\n",
      "3008/3008 [==============================] - 2s 644us/step - loss: 0.7029 - acc: 0.4927 - val_loss: 0.6998 - val_acc: 0.4980\n",
      "Epoch 14/15\n",
      "3008/3008 [==============================] - 2s 649us/step - loss: 0.7014 - acc: 0.4950 - val_loss: 0.6997 - val_acc: 0.4980\n",
      "Epoch 15/15\n",
      "3008/3008 [==============================] - 2s 648us/step - loss: 0.6988 - acc: 0.4963 - val_loss: 0.6967 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 30s 10ms/step - loss: 0.7581 - acc: 0.5997 - val_loss: 0.7486 - val_acc: 0.7068\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 329us/step - loss: 0.7183 - acc: 0.6825 - val_loss: 0.7055 - val_acc: 0.7094\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 331us/step - loss: 0.6858 - acc: 0.6918 - val_loss: 0.6840 - val_acc: 0.7141\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 325us/step - loss: 0.6653 - acc: 0.7031 - val_loss: 0.6712 - val_acc: 0.7108\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 0.6514 - acc: 0.7111 - val_loss: 0.6644 - val_acc: 0.7174\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 326us/step - loss: 0.6355 - acc: 0.7274 - val_loss: 0.6633 - val_acc: 0.7168\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 325us/step - loss: 0.6268 - acc: 0.7314 - val_loss: 0.6517 - val_acc: 0.7214\n",
      "1504/1504 [==============================] - 11s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 30s 10ms/step - loss: 0.7601 - acc: 0.5828 - val_loss: 0.7447 - val_acc: 0.6935\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 340us/step - loss: 0.7285 - acc: 0.6679 - val_loss: 0.7051 - val_acc: 0.7008\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 341us/step - loss: 0.6938 - acc: 0.6882 - val_loss: 0.6769 - val_acc: 0.7061\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 345us/step - loss: 0.6611 - acc: 0.7031 - val_loss: 0.6679 - val_acc: 0.7128\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 339us/step - loss: 0.6558 - acc: 0.7068 - val_loss: 0.6594 - val_acc: 0.7128\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 339us/step - loss: 0.6387 - acc: 0.7271 - val_loss: 0.6523 - val_acc: 0.7234\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 342us/step - loss: 0.6226 - acc: 0.7367 - val_loss: 0.6575 - val_acc: 0.7234\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/7\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 0.7595 - acc: 0.5967 - val_loss: 0.7509 - val_acc: 0.7015\n",
      "Epoch 2/7\n",
      "3008/3008 [==============================] - 1s 335us/step - loss: 0.7180 - acc: 0.6789 - val_loss: 0.6998 - val_acc: 0.7048\n",
      "Epoch 3/7\n",
      "3008/3008 [==============================] - 1s 333us/step - loss: 0.6784 - acc: 0.6945 - val_loss: 0.6866 - val_acc: 0.7055\n",
      "Epoch 4/7\n",
      "3008/3008 [==============================] - 1s 337us/step - loss: 0.6640 - acc: 0.7071 - val_loss: 0.6755 - val_acc: 0.7048\n",
      "Epoch 5/7\n",
      "3008/3008 [==============================] - 1s 338us/step - loss: 0.6438 - acc: 0.7204 - val_loss: 0.6668 - val_acc: 0.7148\n",
      "Epoch 6/7\n",
      "3008/3008 [==============================] - 1s 334us/step - loss: 0.6349 - acc: 0.7221 - val_loss: 0.6607 - val_acc: 0.7247\n",
      "Epoch 7/7\n",
      "3008/3008 [==============================] - 1s 342us/step - loss: 0.6230 - acc: 0.7320 - val_loss: 0.6625 - val_acc: 0.7367\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 4.1565 - acc: 0.6439 - val_loss: 3.0854 - val_acc: 0.6922\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 323us/step - loss: 2.4858 - acc: 0.7091 - val_loss: 2.0135 - val_acc: 0.7214\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 321us/step - loss: 1.7311 - acc: 0.7387 - val_loss: 1.4695 - val_acc: 0.7327\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 1.3036 - acc: 0.7420 - val_loss: 1.1448 - val_acc: 0.7427\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 323us/step - loss: 1.0550 - acc: 0.7473 - val_loss: 0.9737 - val_acc: 0.7354\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 322us/step - loss: 0.9233 - acc: 0.7437 - val_loss: 0.8775 - val_acc: 0.7520\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 325us/step - loss: 0.8345 - acc: 0.7450 - val_loss: 0.8003 - val_acc: 0.7380\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 329us/step - loss: 0.7772 - acc: 0.7427 - val_loss: 0.7474 - val_acc: 0.7447\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 326us/step - loss: 0.7355 - acc: 0.7500 - val_loss: 0.7251 - val_acc: 0.7447\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 329us/step - loss: 0.7071 - acc: 0.7463 - val_loss: 0.6999 - val_acc: 0.7447\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 329us/step - loss: 0.6999 - acc: 0.7453 - val_loss: 0.6958 - val_acc: 0.7427\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 4.0762 - acc: 0.6230 - val_loss: 2.9102 - val_acc: 0.6995\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 333us/step - loss: 2.2598 - acc: 0.6998 - val_loss: 1.8215 - val_acc: 0.6855\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 328us/step - loss: 1.4907 - acc: 0.7254 - val_loss: 1.2524 - val_acc: 0.7214\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 331us/step - loss: 1.1118 - acc: 0.7377 - val_loss: 1.0193 - val_acc: 0.7174\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 332us/step - loss: 0.9242 - acc: 0.7390 - val_loss: 0.8787 - val_acc: 0.7434\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 330us/step - loss: 0.8175 - acc: 0.7400 - val_loss: 0.7829 - val_acc: 0.7414\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 331us/step - loss: 0.7591 - acc: 0.7394 - val_loss: 0.7420 - val_acc: 0.7407\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 334us/step - loss: 0.7177 - acc: 0.7394 - val_loss: 0.7050 - val_acc: 0.7447\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 331us/step - loss: 0.6943 - acc: 0.7487 - val_loss: 0.6999 - val_acc: 0.7394\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 329us/step - loss: 0.6801 - acc: 0.7510 - val_loss: 0.6835 - val_acc: 0.7440\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 330us/step - loss: 0.6748 - acc: 0.7513 - val_loss: 0.6860 - val_acc: 0.7487\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 4.1841 - acc: 0.6626 - val_loss: 3.1513 - val_acc: 0.7055\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 341us/step - loss: 2.5920 - acc: 0.7101 - val_loss: 2.1302 - val_acc: 0.7274\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 333us/step - loss: 1.8467 - acc: 0.7287 - val_loss: 1.5950 - val_acc: 0.7227\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 340us/step - loss: 1.4237 - acc: 0.7350 - val_loss: 1.2659 - val_acc: 0.7367\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 339us/step - loss: 1.1578 - acc: 0.7367 - val_loss: 1.0660 - val_acc: 0.7201\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 346us/step - loss: 0.9984 - acc: 0.7394 - val_loss: 0.9396 - val_acc: 0.7301\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 338us/step - loss: 0.8914 - acc: 0.7374 - val_loss: 0.8657 - val_acc: 0.7347\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 337us/step - loss: 0.8312 - acc: 0.7314 - val_loss: 0.7989 - val_acc: 0.7460\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 340us/step - loss: 0.7753 - acc: 0.7377 - val_loss: 0.7590 - val_acc: 0.7400\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 342us/step - loss: 0.7358 - acc: 0.7440 - val_loss: 0.7378 - val_acc: 0.7241\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 342us/step - loss: 0.7236 - acc: 0.7407 - val_loss: 0.7161 - val_acc: 0.7254\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 30.2236 - acc: 0.4804 - val_loss: 9.4542 - val_acc: 0.5020\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 247us/step - loss: 3.3359 - acc: 0.5033 - val_loss: 1.2903 - val_acc: 0.5020\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 1.1621 - acc: 0.5043 - val_loss: 1.0624 - val_acc: 0.5020\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 245us/step - loss: 1.0152 - acc: 0.5106 - val_loss: 0.9521 - val_acc: 0.5020\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 249us/step - loss: 0.9308 - acc: 0.4900 - val_loss: 0.8817 - val_acc: 0.5020\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 250us/step - loss: 0.8692 - acc: 0.5020 - val_loss: 0.8447 - val_acc: 0.5020\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 247us/step - loss: 0.8483 - acc: 0.4950 - val_loss: 0.8353 - val_acc: 0.5020\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 249us/step - loss: 0.8420 - acc: 0.5116 - val_loss: 0.8350 - val_acc: 0.5020\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 245us/step - loss: 0.8432 - acc: 0.5033 - val_loss: 0.8349 - val_acc: 0.5020\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 247us/step - loss: 0.8379 - acc: 0.5173 - val_loss: 0.8349 - val_acc: 0.5020\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 0.8452 - acc: 0.4914 - val_loss: 0.8355 - val_acc: 0.5020\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 30.1656 - acc: 0.5103 - val_loss: 9.4198 - val_acc: 0.4980\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 250us/step - loss: 3.3556 - acc: 0.4963 - val_loss: 1.3267 - val_acc: 0.4980\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 249us/step - loss: 1.1922 - acc: 0.5003 - val_loss: 1.0914 - val_acc: 0.4980\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 1.0407 - acc: 0.4977 - val_loss: 0.9729 - val_acc: 0.4980\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 252us/step - loss: 0.9427 - acc: 0.4980 - val_loss: 0.8893 - val_acc: 0.4980\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 256us/step - loss: 0.8763 - acc: 0.4907 - val_loss: 0.8471 - val_acc: 0.4980\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 247us/step - loss: 0.8479 - acc: 0.5170 - val_loss: 0.8348 - val_acc: 0.4980\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 251us/step - loss: 0.8471 - acc: 0.4884 - val_loss: 0.8348 - val_acc: 0.4980\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 251us/step - loss: 0.8392 - acc: 0.5020 - val_loss: 0.8347 - val_acc: 0.4980\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 247us/step - loss: 0.8437 - acc: 0.4894 - val_loss: 0.8349 - val_acc: 0.4980\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 244us/step - loss: 0.8410 - acc: 0.5063 - val_loss: 0.8352 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 3008 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "3008/3008 [==============================] - 31s 10ms/step - loss: 30.1413 - acc: 0.5033 - val_loss: 9.3585 - val_acc: 0.4980\n",
      "Epoch 2/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 3.3077 - acc: 0.5027 - val_loss: 1.2919 - val_acc: 0.4980\n",
      "Epoch 3/11\n",
      "3008/3008 [==============================] - 1s 245us/step - loss: 1.1656 - acc: 0.5090 - val_loss: 1.0657 - val_acc: 0.4980\n",
      "Epoch 4/11\n",
      "3008/3008 [==============================] - 1s 249us/step - loss: 1.0228 - acc: 0.5023 - val_loss: 0.9579 - val_acc: 0.4980\n",
      "Epoch 5/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 0.9281 - acc: 0.5080 - val_loss: 0.8853 - val_acc: 0.4980\n",
      "Epoch 6/11\n",
      "3008/3008 [==============================] - 1s 245us/step - loss: 0.8778 - acc: 0.4940 - val_loss: 0.8468 - val_acc: 0.5020\n",
      "Epoch 7/11\n",
      "3008/3008 [==============================] - 1s 248us/step - loss: 0.8522 - acc: 0.4950 - val_loss: 0.8349 - val_acc: 0.5020\n",
      "Epoch 8/11\n",
      "3008/3008 [==============================] - 1s 245us/step - loss: 0.8476 - acc: 0.4894 - val_loss: 0.8340 - val_acc: 0.5020\n",
      "Epoch 9/11\n",
      "3008/3008 [==============================] - 1s 243us/step - loss: 0.8457 - acc: 0.5023 - val_loss: 0.8341 - val_acc: 0.4980\n",
      "Epoch 10/11\n",
      "3008/3008 [==============================] - 1s 246us/step - loss: 0.8452 - acc: 0.4973 - val_loss: 0.8351 - val_acc: 0.4980\n",
      "Epoch 11/11\n",
      "3008/3008 [==============================] - 1s 243us/step - loss: 0.8458 - acc: 0.4947 - val_loss: 0.8351 - val_acc: 0.4980\n",
      "1504/1504 [==============================] - 12s 8ms/step\n",
      "Train on 4512 samples, validate on 1504 samples\n",
      "Epoch 1/11\n",
      "4512/4512 [==============================] - 32s 7ms/step - loss: 3.4894 - acc: 0.6562 - val_loss: 2.1864 - val_acc: 0.7154\n",
      "Epoch 2/11\n",
      "4512/4512 [==============================] - 1s 307us/step - loss: 1.6664 - acc: 0.7181 - val_loss: 1.2870 - val_acc: 0.7168\n",
      "Epoch 3/11\n",
      "4512/4512 [==============================] - 1s 310us/step - loss: 1.0897 - acc: 0.7270 - val_loss: 0.9338 - val_acc: 0.7467\n",
      "Epoch 4/11\n",
      "4512/4512 [==============================] - 1s 310us/step - loss: 0.8635 - acc: 0.7367 - val_loss: 0.8002 - val_acc: 0.7467\n",
      "Epoch 5/11\n",
      "4512/4512 [==============================] - 1s 314us/step - loss: 0.7675 - acc: 0.7352 - val_loss: 0.7303 - val_acc: 0.7487\n",
      "Epoch 6/11\n",
      "4512/4512 [==============================] - 1s 313us/step - loss: 0.7208 - acc: 0.7312 - val_loss: 0.7086 - val_acc: 0.7414\n",
      "Epoch 7/11\n",
      "4512/4512 [==============================] - 1s 312us/step - loss: 0.6897 - acc: 0.7414 - val_loss: 0.6739 - val_acc: 0.7487\n",
      "Epoch 8/11\n",
      "4512/4512 [==============================] - 1s 313us/step - loss: 0.6765 - acc: 0.7394 - val_loss: 0.6674 - val_acc: 0.7493\n",
      "Epoch 9/11\n",
      "4512/4512 [==============================] - 1s 310us/step - loss: 0.6679 - acc: 0.7380 - val_loss: 0.6659 - val_acc: 0.7434\n",
      "Epoch 10/11\n",
      "4512/4512 [==============================] - 1s 305us/step - loss: 0.6617 - acc: 0.7363 - val_loss: 0.6494 - val_acc: 0.7487\n",
      "Epoch 11/11\n",
      "4512/4512 [==============================] - 1s 310us/step - loss: 0.6559 - acc: 0.7420 - val_loss: 0.6526 - val_acc: 0.7427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f7cfb6fbf28>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'act': ['relu', 'sigmoid'],\n",
       "                                        'batch_size': [20, 30, 40, 50],\n",
       "                                        'decay': [1e-06, 1e-09, 0],\n",
       "                                        'dropout': [0.1, 0.2],\n",
       "                                        'epochs': [7, 11, 15],\n",
       "                                        'l1': [0, 0.01, 0.001, 0.0001],\n",
       "                                        'l2': [0, 0.01, 0.001, 0.0001],\n",
       "                                        'lr': [0.01, 0.001, 0.0001],\n",
       "                                        'nl1': [0, 1, 2, 3],\n",
       "                                        'nl2': [0, 1, 2, 3], 'nl3': [0, 1, 2],\n",
       "                                        'nn1': [100, 200, 400],\n",
       "                                        'nn2': [50, 100, 200],\n",
       "                                        'nn3': [20, 50, 100]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn= create_model, verbose=1) \n",
    "\n",
    "final_grid_NN = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid_fin, cv=KFold(3),\n",
    "                                   scoring= 'f1') # scoring= 'f1' or AUC look at the best trade off\n",
    "final_grid_NN.fit(scaled_train.values, y_train.values, validation_data=(scaled_test.values, y_test.values), ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "colab_type": "code",
    "id": "k6ePE-3eS6IG",
    "outputId": "d7eba81c-ffa8-4e22-8eea-c4622df6acdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score on validation:  0.7431357042424873\n",
      "best param combination:  {'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 3, 'nl1': 1, 'lr': 0.001, 'l2': 0.0001, 'l1': 0.001, 'epochs': 11, 'dropout': 0.2, 'decay': 1e-06, 'batch_size': 40, 'act': 'relu'}\n",
      "1504/1504 [==============================] - 0s 173us/step\n",
      "predictions [[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "4512/4512 [==============================] - 1s 152us/step\n",
      "train accuracy TP+TN/tot is 0.7570921985815603\n",
      "1504/1504 [==============================] - 0s 157us/step\n",
      "test accuracy TP+TN/tot is 0.742686170212766\n",
      "1504/1504 [==============================] - 0s 150us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74       749\n",
      "           1       0.74      0.75      0.75       755\n",
      "\n",
      "    accuracy                           0.74      1504\n",
      "   macro avg       0.74      0.74      0.74      1504\n",
      "weighted avg       0.74      0.74      0.74      1504\n",
      "\n",
      "1504/1504 [==============================] - 0s 149us/step\n",
      "[[547 202]\n",
      " [185 570]]\n",
      "4512/4512 [==============================] - 1s 151us/step\n",
      "1504/1504 [==============================] - 0s 151us/step\n",
      "roc curve train is 0.7570911795929169, roc test is 0.7426369817593436\n",
      "4512/4512 [==============================] - 1s 145us/step\n",
      "1504/1504 [==============================] - 0s 150us/step\n",
      "F1 SCORE train is 0.7566607460035525, F1 SCORE test is 0.7465618860510806\n"
     ]
    }
   ],
   "source": [
    "summary_grid_NN(final_grid_NN) #{'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 3, 'nl1': 1, 'lr': 0.001, 'l2': 0.0001, 'l1': 0.001, 'epochs': 11, 'dropout': 0.2, 'decay': 1e-06, 'batch_size': 40, 'act': 'relu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "SNfoEthW3c4J",
    "outputId": "f594a379-3b3a-4fba-8469-129365a62f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation score: 0.743136, using {'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 3, 'nl1': 1, 'lr': 0.001, 'l2': 0.0001, 'l1': 0.001, 'epochs': 11, 'dropout': 0.2, 'decay': 1e-06, 'batch_size': 40, 'act': 'relu'}\n",
      "SCORE (roc_auc in this last case), STD DEV and PARAMETER COMBINATIONS are: \n",
      "\n",
      "0.736680 (0.012044) with: {'nn3': 100, 'nn2': 200, 'nn1': 200, 'nl3': 2, 'nl2': 2, 'nl1': 3, 'lr': 0.0001, 'l2': 0, 'l1': 0.0001, 'epochs': 11, 'dropout': 0.1, 'decay': 0, 'batch_size': 40, 'act': 'relu'}\n",
      "0.215625 (0.304939) with: {'nn3': 100, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 1, 'nl1': 0, 'lr': 0.01, 'l2': 0.001, 'l1': 0.01, 'epochs': 7, 'dropout': 0.2, 'decay': 1e-09, 'batch_size': 40, 'act': 'sigmoid'}\n",
      "0.450295 (0.318450) with: {'nn3': 100, 'nn2': 100, 'nn1': 400, 'nl3': 1, 'nl2': 2, 'nl1': 3, 'lr': 0.01, 'l2': 0, 'l1': 0.001, 'epochs': 11, 'dropout': 0.1, 'decay': 1e-06, 'batch_size': 50, 'act': 'sigmoid'}\n",
      "0.730665 (0.009614) with: {'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 0, 'nl1': 1, 'lr': 0.01, 'l2': 0, 'l1': 0, 'epochs': 7, 'dropout': 0.1, 'decay': 0, 'batch_size': 30, 'act': 'relu'}\n",
      "0.438633 (0.310292) with: {'nn3': 50, 'nn2': 200, 'nn1': 200, 'nl3': 1, 'nl2': 2, 'nl1': 3, 'lr': 0.01, 'l2': 0, 'l1': 0.0001, 'epochs': 15, 'dropout': 0.1, 'decay': 0, 'batch_size': 20, 'act': 'sigmoid'}\n",
      "0.694300 (0.003786) with: {'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 0, 'nl2': 1, 'nl1': 0, 'lr': 0.0001, 'l2': 0, 'l1': 0, 'epochs': 7, 'dropout': 0.2, 'decay': 1e-09, 'batch_size': 30, 'act': 'sigmoid'}\n",
      "0.000000 (0.000000) with: {'nn3': 50, 'nn2': 50, 'nn1': 400, 'nl3': 2, 'nl2': 3, 'nl1': 0, 'lr': 0.01, 'l2': 0.0001, 'l1': 0.0001, 'epochs': 15, 'dropout': 0.1, 'decay': 0, 'batch_size': 20, 'act': 'sigmoid'}\n",
      "0.722265 (0.009087) with: {'nn3': 20, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 2, 'nl1': 3, 'lr': 0.0001, 'l2': 0.0001, 'l1': 0, 'epochs': 7, 'dropout': 0.1, 'decay': 0, 'batch_size': 40, 'act': 'relu'}\n",
      "0.743136 (0.012014) with: {'nn3': 50, 'nn2': 50, 'nn1': 200, 'nl3': 1, 'nl2': 3, 'nl1': 1, 'lr': 0.001, 'l2': 0.0001, 'l1': 0.001, 'epochs': 11, 'dropout': 0.2, 'decay': 1e-06, 'batch_size': 40, 'act': 'relu'}\n",
      "0.215625 (0.304939) with: {'nn3': 20, 'nn2': 50, 'nn1': 400, 'nl3': 2, 'nl2': 0, 'nl1': 1, 'lr': 0.001, 'l2': 0.0001, 'l1': 0.01, 'epochs': 11, 'dropout': 0.2, 'decay': 0, 'batch_size': 50, 'act': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best validation score: %f, using %s\" % (final_grid_NN.best_score_, final_grid_NN.best_params_)) #siccome come validation set ho usato il test_set (ovvero dataset splittato solo in due parti, senza validation set), in questo caso sarebbe proprio il best test_set\n",
    "means = final_grid_NN.cv_results_['mean_test_score']\n",
    "stds = final_grid_NN.cv_results_['std_test_score']\n",
    "params = final_grid_NN.cv_results_['params']\n",
    "\n",
    "print('SCORE (roc_auc in this last case), STD DEV and PARAMETER COMBINATIONS are: \\n')  #non prova tutte le combinazioni: è randomized. ha in realtà una logica iterativa in base alla quale ne prova solo alcuni..\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DawlNDPVlKy"
   },
   "source": [
    "**With the final best Grid Search (randomized) I obtained 75% of f1 score on the test set. Therefore for this small dataset and relatively simple problem, among all the model I used it is better the Gradient Boosting, which gives 77% on the test set and is much less expensive than Ensemble methods or Neural Networks**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "43gMBSol3c1P",
    "DPNaLx5D3c1y",
    "Vr6ZdAD_3c10",
    "1nuy3L0t3c1-",
    "uXJRkdvE3c2L",
    "8gOUWPWN3c2l",
    "q0CqiRKv3c20",
    "2XP4VxQy3c3C",
    "cjHlruRY3c3H",
    "F-N08poE3c3L",
    "fJSN9Fyr3c3V",
    "Vcvrq28b3c3W",
    "G1qE8piP3c3e",
    "R27Q3c_J3c3h",
    "lgCbZYIy3c3h",
    "ulF4LAYD3c3m"
   ],
   "name": "Santander Project Federico Francone final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
